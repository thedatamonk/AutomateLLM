{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict\n",
    "from examples import CODE_EXAMPLES\n",
    "from openai import OpenAI\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set OpenAI API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = \"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\"\n",
    "os.environ['APIFY_API_TOKEN'] = \"apify_api_smCekcxblzAnEwDaH94Oz3aJ2gvaqN4z0tQK\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get relevant examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_examples(examples_pool: List[Dict]) -> List[Dict]:\n",
    "    # for v1, we will use all the examples\n",
    "    # for later versions, we can apply filters -> for instance selecting relevant examples based on the query\n",
    "    return examples_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = get_examples(examples_pool=CODE_EXAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job: Send an activity summary email to users at 4 PM every Friday.\n",
      "TypeSript Code: \n",
      "import { TriggerClient, cronTrigger } from \"@trigger.dev/sdk\";\n",
      "import { SendGrid } from \"@trigger.dev/sendgrid\";\n",
      "import { Slack } from \"@trigger.dev/slack\";\n",
      "import { weeklySummaryDb } from \"./mocks/db\";\n",
      "import { weeklySummaryEmail } from \"./mocks/emails\";\n",
      "\n",
      "const sendgrid = new SendGrid({\n",
      "  id: \"sendgrid\",\n",
      "  apiKey: process.env.SENDGRID_API_KEY!,\n",
      "});\n",
      "\n",
      "const slack = new Slack({ id: \"slack\" });\n",
      "\n",
      "// This Job sends a weekly summary email to users who have\n",
      "// summariesEnabled = true, and then posts the total numbers to Slack.\n",
      "client.defineJob({\n",
      "  id: \"weekly-user-activity-summary\",\n",
      "  name: \"Weekly user activity summary\",\n",
      "  version: \"1.0.0\",\n",
      "  integrations: { sendgrid, slack },\n",
      "  trigger: cronTrigger({\n",
      "    // Send every Friday at 4pm\n",
      "    cron: \"0 16 * * 5\",\n",
      "  }),\n",
      "  run: async (payload, io, ctx) => {\n",
      "    const users = await weeklySummaryDb.getUsers();\n",
      "\n",
      "    let sentCount = 0;\n",
      "    let notSentCount = 0;\n",
      "\n",
      "    for (const user of users) {\n",
      "      if (user.summariesEnabled) {\n",
      "        await io.sendgrid.sendEmail(`Weekly summary for ${user.id}`, {\n",
      "          to: user.email,\n",
      "          // The 'from' email must be a verified domain in your SendGrid account.\n",
      "          from: \"hello@acme.inc\",\n",
      "          subject: \"Your weekly summary\",\n",
      "          html: weeklySummaryEmail(user),\n",
      "        });\n",
      "        sentCount++;\n",
      "      } else {\n",
      "        notSentCount++;\n",
      "      }\n",
      "    }\n",
      "\n",
      "    await io.slack.postMessage(\"Notify team\", {\n",
      "      text: `Weekly summary sent to ${sentCount} users and not sent to ${notSentCount} users`,\n",
      "      // This has to be a channel ID, not a channel name\n",
      "      channel: \"YOUR_CHANNEL_ID\",\n",
      "    });\n",
      "  },\n",
      "});\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for example in examples[:1]:\n",
    "    print (f\"Job: {example['job']}\")\n",
    "    print (f\"TypeSript Code: {example['automation_code']}\")\n",
    "    print (f\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(job_description, examples, prefix=\"\\n\", suffix=\"\\n\"):\n",
    "\n",
    "    examples_string = \"\"\n",
    "\n",
    "    examples_string += prefix\n",
    "\n",
    "    for example in examples:\n",
    "        examples_string += f\"Job Description: {example['job']}\\n\"\n",
    "        examples_string += f\"Automation Code: \\n```ts\\n{example['automation_code']}```\"\n",
    "        examples_string += \"\\n\"\n",
    "\n",
    "    examples_string += suffix\n",
    "\n",
    "\n",
    "    prompt_template = f\"\"\"\n",
    "    You are given a description of a job that the user wants to automate. Your goal is to convert this job description into\n",
    "     an equivalent [trigger.dev](https://trigger.dev/docs/documentation/introduction) code. Also the generated code should be\n",
    "     valid TypeScript code.\n",
    "\n",
    "     Here are the steps that you have to follow -\n",
    "    1. Breakdown the job into 1 or more tasks.\n",
    "    2. Identify the job trigger.\n",
    "    3. Generate syntactically and logically correct **trigger.dev** code.\n",
    "\n",
    "    Here are some examples that demonstrate the task you have to perform:\n",
    "\n",
    "    {examples_string}\n",
    "\n",
    "    <END OF EXAMPLES>\n",
    "\n",
    "    Job Description: {job_description}\n",
    "    \n",
    "    Automation code: \n",
    "\n",
    "\"\"\"\n",
    "    return prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_queries = [\n",
    "    \"\"\"I have a SQL table setup in a Supabase DB.\n",
    "    When I run a SQL query and it fails or returns no result, then I want to trigger a job.\n",
    "    This job should send an email to the user specifying in the email -\n",
    "    \n",
    "    1. Why the query failed or returned no results?\n",
    "    2. The parameters that are required from the user to correct the SQL query.\n",
    "\n",
    "    When the user replies to the email with the relevant parameters, trigger a webhook\n",
    "    that calls a Python script. This script parses the email response data and re runs the SQL query.\n",
    "    \"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompt = get_prompt(examples=examples, job_description=test_queries[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate number of input tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_string(string: str, encoding_name: str = \"cl100k_base\") -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input tokens: 1416\n"
     ]
    }
   ],
   "source": [
    "print (f\"Number of input tokens: {num_tokens_from_string(input_prompt)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='To automate the described job, we\\'ll break it down into two main tasks:\\n\\n1. **Task 1:** Trigger a job when a SQL query fails or returns no result, sending an email to the user with details on why it failed or returned no results, and what parameters are needed to correct the SQL query.\\n2. **Task 2:** When the user replies to the email with the relevant parameters, trigger a webhook that calls a Python script to parse the email response data and rerun the SQL query.\\n\\nFor **Task 1**, we\\'ll use an event trigger that listens for a specific event indicating a failed or empty SQL query result. For **Task 2**, we\\'ll use a webhook trigger that listens for incoming data from the user\\'s email response.\\n\\nHere\\'s how the automation code might look in TypeScript using the **trigger.dev** SDK:\\n\\n```ts\\nimport { TriggerClient, eventTrigger, webhookTrigger } from \"@trigger.dev/sdk\";\\nimport { SendGrid } from \"@trigger.dev/sendgrid\";\\nimport { Supabase } from \"@trigger.dev/supabase\";\\nimport { z } from \"zod\";\\n\\nconst sendgrid = new SendGrid({\\n  id: \"sendgrid\",\\n  apiKey: process.env.SENDGRID_API_KEY!,\\n});\\n\\nconst supabase = new Supabase({\\n  id: \"supabase\",\\n  apiKey: process.env.SUPABASE_API_KEY!,\\n  url: process.env.SUPABASE_URL!,\\n});\\n\\n// Task 1: Trigger a job when a SQL query fails or returns no result\\nclient.defineJob({\\n  id: \"sql-query-failure-notification\",\\n  name: \"SQL Query Failure Notification\",\\n  version: \"1.0.0\",\\n  trigger: eventTrigger({\\n    name: \"sql.query.failure\",\\n    schema: z.object({\\n      failureReason: z.string(),\\n      requiredParameters: z.array(z.string()),\\n    }),\\n  }),\\n  integrations: { sendgrid, supabase },\\n  run: async (payload, io, ctx) => {\\n    await io.sendgrid.sendEmail(\"sql-failure-notification\", {\\n      to: \"user@example.com\",\\n      from: \"support@yourcompany.com\",\\n      subject: \"SQL Query Failed\",\\n      html: `Your SQL query failed due to: ${payload.failureReason}. Please provide the following parameters to correct the query: ${payload.requiredParameters.join(\", \")}`,\\n    });\\n  },\\n});\\n\\n// Task 2: Trigger a webhook that calls a Python script to parse email response data and rerun the SQL query\\nclient.defineJob({\\n  id: \"rerun-sql-query-from-email\",\\n  name: \"Rerun SQL Query from Email Response\",\\n  version: \"1.0.0\",\\n  trigger: webhookTrigger({\\n    path: \"/rerun-sql-query\",\\n    schema: z.object({\\n      emailResponseData: z.string(),\\n    }),\\n  }),\\n  integrations: { supabase },\\n  run: async (payload, io, ctx) => {\\n    // Call the Python script with the email response data\\n    // This part is pseudo-code as the actual implementation depends on how the Python script is exposed (e.g., via an HTTP endpoint)\\n    const pythonScriptResponse = await callPythonScriptWithPayload(payload.emailResponseData);\\n    if (pythonScriptResponse.success) {\\n      console.log(\"SQL query rerun successfully.\");\\n    } else {\\n      console.error(\"Failed to rerun SQL query:\", pythonScriptResponse.error);\\n    }\\n  },\\n});\\n\\n// Helper function to call the Python script (pseudo-code)\\nasync function callPythonScriptWithPayload(payload: string): Promise<{ success: boolean; error?: string }> {\\n  // Implementation depends on the Python script\\'s exposure method\\n  return { success: true }; // Placeholder response\\n}\\n```\\n\\nThis code defines two jobs using the **trigger.dev** SDK. The first job sends an email to the user when a SQL query fails or returns no result, including details on the failure and what\\'s needed to correct it. The second job listens for a webhook trigger, which is expected to be called with data from the user\\'s email response, and then calls a Python script to rerun the SQL query with the new parameters. Note that the actual implementation of calling the Python script will depend on how the script is exposed (e.g., as a web service).', role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-4-0125-preview\",\n",
    "  temperature=0.0,\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are an expert software engineer who is proficient in TypeScript.\"},\n",
    "    {\"role\": \"user\", \"content\": input_prompt}\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To automate the described job, we'll break it down into two main tasks:\n",
      "\n",
      "1. **Task 1:** Trigger a job when a SQL query fails or returns no result, sending an email to the user with details on why it failed or returned no results, and what parameters are needed to correct the SQL query.\n",
      "2. **Task 2:** When the user replies to the email with the relevant parameters, trigger a webhook that calls a Python script to parse the email response data and rerun the SQL query.\n",
      "\n",
      "For **Task 1**, we'll use an event trigger that listens for a specific event indicating a failed or empty SQL query result. For **Task 2**, we'll use a webhook trigger that listens for incoming data from the user's email response.\n",
      "\n",
      "Here's how the automation code might look in TypeScript using the **trigger.dev** SDK:\n",
      "\n",
      "```ts\n",
      "import { TriggerClient, eventTrigger, webhookTrigger } from \"@trigger.dev/sdk\";\n",
      "import { SendGrid } from \"@trigger.dev/sendgrid\";\n",
      "import { Supabase } from \"@trigger.dev/supabase\";\n",
      "import { z } from \"zod\";\n",
      "\n",
      "const sendgrid = new SendGrid({\n",
      "  id: \"sendgrid\",\n",
      "  apiKey: process.env.SENDGRID_API_KEY!,\n",
      "});\n",
      "\n",
      "const supabase = new Supabase({\n",
      "  id: \"supabase\",\n",
      "  apiKey: process.env.SUPABASE_API_KEY!,\n",
      "  url: process.env.SUPABASE_URL!,\n",
      "});\n",
      "\n",
      "// Task 1: Trigger a job when a SQL query fails or returns no result\n",
      "client.defineJob({\n",
      "  id: \"sql-query-failure-notification\",\n",
      "  name: \"SQL Query Failure Notification\",\n",
      "  version: \"1.0.0\",\n",
      "  trigger: eventTrigger({\n",
      "    name: \"sql.query.failure\",\n",
      "    schema: z.object({\n",
      "      failureReason: z.string(),\n",
      "      requiredParameters: z.array(z.string()),\n",
      "    }),\n",
      "  }),\n",
      "  integrations: { sendgrid, supabase },\n",
      "  run: async (payload, io, ctx) => {\n",
      "    await io.sendgrid.sendEmail(\"sql-failure-notification\", {\n",
      "      to: \"user@example.com\",\n",
      "      from: \"support@yourcompany.com\",\n",
      "      subject: \"SQL Query Failed\",\n",
      "      html: `Your SQL query failed due to: ${payload.failureReason}. Please provide the following parameters to correct the query: ${payload.requiredParameters.join(\", \")}`,\n",
      "    });\n",
      "  },\n",
      "});\n",
      "\n",
      "// Task 2: Trigger a webhook that calls a Python script to parse email response data and rerun the SQL query\n",
      "client.defineJob({\n",
      "  id: \"rerun-sql-query-from-email\",\n",
      "  name: \"Rerun SQL Query from Email Response\",\n",
      "  version: \"1.0.0\",\n",
      "  trigger: webhookTrigger({\n",
      "    path: \"/rerun-sql-query\",\n",
      "    schema: z.object({\n",
      "      emailResponseData: z.string(),\n",
      "    }),\n",
      "  }),\n",
      "  integrations: { supabase },\n",
      "  run: async (payload, io, ctx) => {\n",
      "    // Call the Python script with the email response data\n",
      "    // This part is pseudo-code as the actual implementation depends on how the Python script is exposed (e.g., via an HTTP endpoint)\n",
      "    const pythonScriptResponse = await callPythonScriptWithPayload(payload.emailResponseData);\n",
      "    if (pythonScriptResponse.success) {\n",
      "      console.log(\"SQL query rerun successfully.\");\n",
      "    } else {\n",
      "      console.error(\"Failed to rerun SQL query:\", pythonScriptResponse.error);\n",
      "    }\n",
      "  },\n",
      "});\n",
      "\n",
      "// Helper function to call the Python script (pseudo-code)\n",
      "async function callPythonScriptWithPayload(payload: string): Promise<{ success: boolean; error?: string }> {\n",
      "  // Implementation depends on the Python script's exposure method\n",
      "  return { success: true }; // Placeholder response\n",
      "}\n",
      "```\n",
      "\n",
      "This code defines two jobs using the **trigger.dev** SDK. The first job sends an email to the user when a SQL query fails or returns no result, including details on the failure and what's needed to correct it. The second job listens for a webhook trigger, which is expected to be called with data from the user's email response, and then calls a Python script to rerun the SQL query with the new parameters. Note that the actual implementation of calling the Python script will depend on how the script is exposed (e.g., as a web service).\n"
     ]
    }
   ],
   "source": [
    "print (completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawl `trigger.dev` documentation using Apify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.utilities import ApifyWrapper\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "apify = ApifyWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = apify.call_actor(\n",
    "    actor_id=\"apify/website-content-crawler\",\n",
    "    run_input={\"startUrls\": [{\"url\": \"https://trigger.dev/docs/documentation/\"}]},\n",
    "    dataset_mapping_function=lambda item: Document(\n",
    "        page_content=item[\"text\"] or \"\", metadata={\"source\": item[\"url\"]}\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentation_data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Trigger.dev is an open source framework for creating long-running Jobs directly in your app with API Integrations, webhooks, scheduling and delays. You can reliably run Jobs that wouldn’t normally work in serverless environments (like Vercel) because of timeouts.\\nYou can use Trigger.dev Cloud or Self-host Trigger.dev on your own infrastructure.\\nGetting help\\nWe’d love to hear from you or give you a hand getting started. Here are some ways to get in touch with us. We’d also ❤️ your support.\\nGetting help', metadata={'source': 'https://trigger.dev/docs/documentation/'}),\n",
       " Document(page_content='Pre-requisites\\nThis guide assumes you already have a project setup and working with Trigger.dev. If not, head over to our Quick Start guides to get up and running in a few minutes.\\n1. Define your Job\\nA Job is a collection of Tasks that are run in a specific order. You can think of it as a function that you can run on a schedule, or when an event happens. Jobs are defined by calling the TriggerClient.defineJob function\\n//this path might be different depending on your project import { client } from \"@/trigger\"; client.defineJob({ // ... job definition }); \\n2. Choose a name and ID\\nEach job must have a unique and stable id and name. The id is used to identify the Job in the database, and the name is used to identify the Job in the UI.\\nclient.defineJob({ id: \"my-job\", name: \"My Job\", // ... job definition }); \\nWe will pass the value of the id property through a slugifier because we use it in URLs in our Dashboard. This means you can use any characters you want, but we recommend using only lowercase letters, numbers and dashes.\\n3. Set the current version\\nThe version property is used to track changes to your Job. It’s required to be a semantic version string. You can track changes to your Job by incrementing the version number, and we will display in the Dashboard which version each Job Run was created with.\\nclient.defineJob({ id: \"my-job\", name: \"My Job\", version: \"1.0.0\", // ... job definition }); \\n4. Choose a Trigger\\nThe trigger you choose determines how and when a job will run. See our Triggers guide for more information. The Trigger you choose also defines the type of the run payload argument (more in this below)\\nclient.defineJob({ id: \"my-job\", name: \"My Job\", version: \"1.0.0\", trigger: eventTrigger({ name: \"my.event\", }), // ... job definition }); \\n5. Add integrations (optional)\\nIntegrations provide a convienent way to create and run tasks against authenticated APIs inside your Job’s run function. You’ll need to pass them in the integrations option when defining your Job.\\nimport { Slack } from \"@trigger.dev/slack\"; const slack = new Slack({ id: \"slack\" }); client.defineJob({ id: \"my-job\", name: \"My Job\", version: \"1.0.0\", trigger: eventTrigger({ name: \"my.event\", }), integrations: { slack }, // ... job definition }); \\n6. Implement the run function\\nThe run function implements your custom code that will be executed when your Job is run. It’s an async function that takes three arguments:\\nThe run payload - The type of the payload argument is determined by the Trigger you choose.\\nAn instance of IO, which exposes built-in tasks and allows you to create your own, as well as interact with integrations.\\nA context object, which contains information about the current run, such as the run ID, the Job ID, and the Job version. Context reference\\nimport { Slack } from \"@trigger.dev/slack\"; const slack = new Slack({ id: \"slack\" }); client.defineJob({ id: \"my-job\", name: \"My Job\", version: \"1.0.0\", trigger: eventTrigger({ name: \"my.event\", }), integrations: { slack }, run: async (payload, io, context) => { // ... your code }, }); \\nThe run function you define is like a normal JavaScript function in all respects except one: it will be called one or more times to complete a single Job Run.\\nThis means that you can’t rely on any state that is not persisted between runs, and you must create tasks to ensure that work is not repeated.\\nTasks are so important that we’ve dedicated a whole section to them. See our Tasks guide for more information.\\nBuilt in tasks\\nWe provide some built-in tasks that you can use in your run function:\\nTaskDescriptionTask code\\nDelay\\tWait for a period of time\\tawait io.wait(\"wait\", 60);\\t\\nLog\\tLog a message\\tawait io.logger.log(\"Hello\");\\t\\nSend Event\\tSend an event (for eventTrigger)\\tawait io.sendEvent(\"my-event\", { name: \"my.event\", payload: { hello: \"world\" } });\\t\\nBackground fetch\\tFetch data from a URL that can take longer that the serverless timeout.\\tawait io.backgroundFetch(\"fetch-some-data\", { url: \"https://example.com\" });\\t\\nFor a full list of built-in Tasks, see the io SDK reference. The below example makes use a few of these built-in tasks:\\nimport { Slack } from \"@trigger.dev/slack\"; const slack = new Slack({ id: \"slack\" }); client.defineJob({ id: \"my-job\", name: \"My Job\", version: \"1.0.0\", trigger: eventTrigger({ name: \"my.event\", }), // Optional integration integrations: { slack }, run: async (payload, io, context) => { await io.logger.info(\"Received the my.event event\", { payload }); await io.sendEvent(\"send-event\", { name: \"other.event\", payload: { hello: \"world\" }, }); await io.wait(\"wait for 60 seconds\", 60); await io.backgroundFetch(\"fetch-some-data\", { url: \"https://example.com\", }); }, }); \\nCreate your own tasks\\nYou can also create your own tasks or use tasks provided by our integration packages. See Creating Tasks for more information. The example below demonstrates creating tasks in 3 different ways:\\nimport { Slack } from \"@trigger.dev/slack\"; const slack = new Slack({ id: \"slack\" }); client.defineJob({ id: \"my-job\", name: \"My Job\", version: \"1.0.0\", trigger: eventTrigger({ name: \"my.event\", }), // Optional integration integrations: { slack }, run: async (payload, io, context) => { // Use runTask with the \"get-user\" cacheKey, and return the user const user = await io.runTask(\"get-user\", async () => { return prisma.user.findUniqueOrThrow({ where: { id: payload.id, }, }); }); // Use the Slack integration to create a task using the \"post-message\" cacheKey const message = await io.slack.postMessage(\"post-message\", { channel: process.env.SLACK_CHANNEL_ID, message: `Hello ${user.name}`, }); await io.wait(\"wait for 10 seconds\", 10); // Use the Slack integration\\'s runTask method to add a reaction to the message await io.slack.runTask(\"add-reaction\", async (client) => { // client here is an authenticated instance of the Slack SDK await client.reactions.add({ channel: process.env.SLACK_CHANNEL_ID, name: \"thumbsup\", timestamp: message.ts, }); }); }, }); \\n7. Handling errors\\nIf your run function throws an error, the Job Run will fail and the error will be displayed in the Dashboard. If you’d like to retry on an error, you can do so using tasks and the retry option.\\nconst user = await io.runTask( \"get-user\", async () => { return prisma.user.findUniqueOrThrow({ where: { id: payload.id, }, }); }, { retry: { limit: 3, factor: 2, minTimeoutInMs: 1000, }, } ); \\nSee the retry options for more information.\\n8. Skip catching internal errors\\nWe will throw some errors internally to interrupt run execution so they can be resumed later. If you put a try/catch block in your run code and catch these errors, your job will not work correctly. You can check if an error is an internal error using isTriggerError():\\nclient.defineJob({ run: async (payload, io, context) => { try { // Use runTask with the \"get-user\" cacheKey, and return the user const user = await io.runTask(\"get-user\", async () => { return prisma.user.findUniqueOrThrow({ where: { id: payload.id, }, }); }); } catch (error) { if (isTriggerError(error)) throw error; // do something with your error here } }, }); \\nAlternatively, you can use the io.try() function:\\nclient.defineJob({ run: async (payload, io, context) => { const result = io.try( () => { return io.runTask(\"get-user\", async () => { return prisma.user.findUniqueOrThrow({ where: { id: payload.id, }, }); }); }, async (error) => { //you can return data from the error handler, //if you wish to elegantly deal with errors return { success: false as const, error, }; } ); }, }); \\n9. Creating tasks in a loop\\nIf you want to create tasks in a loop, you should use for const ... of to ensure that the tasks are created in the correct order.\\nclient.defineJob({ run: async (payload, io, context) => { for (const user of payload.users) { await io.runTask(`update-user-${user.id}`, async () => { return prisma.user.update({ where: { id: user.id, }, data: { name: user.name, }, }); }); } }, }); \\n10. Return data from your Job\\nAnything you return from the run function will be automatically set as the run output and displayed in the Dashboard.\\nclient.defineJob({ run: async (payload, io, context) => { return { success: true, message: \"Hello world\", }; }, }); \\nNext steps\\nWe recommend exploring all of the below sections to fully understand how to create and run Jobs using Trigger.dev.', metadata={'source': 'https://trigger.dev/docs/documentation/guides/writing-jobs-step-by-step'}),\n",
       " Document(page_content='Welcome to the Trigger.dev Task Library 📚. You may be wondering, what is a Task and why are there a library of them? Well you see, Trigger.dev works by divvying up a long-running job execution into a bunch of little tasks, each one taking less time then a single serverless function execution. 💫\\nYou can define and run your own tasks easily using io.runTask(), or you can use one of our Integrations which are tasks for specific APIs, like OpenAI or Stripe.\\nWe also have a growing library of built-in tasks that you can use in your Jobs through the io object. These tasks are designed to be generic and reusable, and are a great way to get started with Trigger.dev.\\nwait\\nThis task allows you to resume executing your job after a certain amount of time has passed:\\nawait io.wait(\"⏰\", 60); // wait 60 seconds \\nInternally this task is considered a “noop”, and noop tasks have no output.\\nreference docs\\nwaitForRequest\\nYou supply this task with a callback to receive a URL. When a POST request is made to that URL, the JSON body of the request becomes the task output.\\nThe example below uses waitForRequest to capture a Screenshot of a website using ScreenshotOne.com and passes the callback URL to the webhook URL to get notified when the screenshot is finished:\\nconst result = await io.waitForRequest<ScreenshotResponse>( \"📸\", async (url) => { await fetch(`https://api.screenshotone.com/take`, { method: \"POST\", headers: { \"Content-Type\": \"application/json\", }, body: JSON.stringify({ webhook_url: url, // this is the URL that will be called when the screenshot is ready access_key: \"my-access-key\", url: \"https://trigger.dev\", store: \"true\", storage_path: \"my-screeshots\", response_type: \"json\", async: \"true\", storage_return_location: \"true\", }), }); }, { timeoutInSeconds: 300, // wait up to 5 minutes for the screenshot to be ready } ); \\nWe actually originally built this task for our Replicate integration, which accepts a callback URL to notify you when a prediction is ready. So this allows you to write very succinct code to create a prediction and wait for it’s results:\\nconst sdPrediction = await io.replicate.predictions.createAndAwait(\"🧑\\u200d🎨\", { version: \"ac732df83cea7fff18b8472768c88ad041fa750ff7682a21affe81863cbe77e4\", input: { prompt: \"What is the meaning of life?\", }, }); \\nreference docs\\nwaitForEvent\\nThis task allows you to wait for an event to be sent. To read about how events work, check out the Events documentation.\\nconst event = await io.waitForEvent( \"🥂\", { name: \"user.created\", schema: z.object({ id: z.string(), createdAt: z.coerce.date(), isAdmin: z.boolean(), }), filter: { isAdmin: [true], // Only wait for events where isAdmin is true }, }, { timeoutInSeconds: 60 * 60, // Wait for up to an hour } ); \\nThe event object returned from this task is the full event object that was sent, including id, name, payload, context, and more.\\nreference docs\\nbackgroundFetch\\nThis task allows you to perform a fetch request in the background, and then resume the execution of your job after the request has completed.\\nconst body = io.backgroundFetch<MyResponseData>(\"🕸️\", \"https://example.com/api/endpoint\", { method: \"POST\", headers: { \"Content-Type\": \"application/json\", Authorization: redactString`Bearer ${auth.apiKey}`, }, body: JSON.stringify({ foo: \"bar\" }), }); \\nThis is useful for when an API is slow to respond and might not finish before your serverless function times out. We created this task to power our OpenAI integration, which can sometimes take more than a minute to respond:\\n// This uses backgroundFetch under the hood await io.openai.chat.completions.backgroundCreate(\"💬\", { model: \"gpt-3.5-turbo\", messages: [ { role: \"user\", content: \"Create a good programming joke about background jobs\", }, ], }); \\nreference docs\\nbackgroundPoll\\nThis task is similar to backgroundFetch, but instead of waiting for a single request to complete, it will poll a URL until it returns a certain value.\\nconst result = await io.backgroundPoll<{ foo: string }>(\"🔃\", { url: \"https://example.com/api/endpoint\", interval: 10, // every 10 seconds timeout: 300, // stop polling after 5 minutes responseFilter: { // stop polling once this filter matches status: [200], body: { status: [\"SUCCESS\"], }, }, }); \\nlogger\\nThe logger object allows you to log messages to the Trigger.dev console. This is useful for debugging your jobs, or just to see what’s going on inside your job.\\nawait io.logger.info(\"This is an info message\"); \\nYou can optionally pass a context object to the logger, which will be displayed in the console:\\nawait io.logger.info(\"This is an info message\", { foo: \"bar\", }); \\nWe support the following log levels:\\nio.logger.debug()\\nio.logger.info()\\nio.logger.warn()\\nio.logger.error()\\nreference docs\\nstore\\nThe store object exposes several namespaced Key-Value Stores you can access inside of your Jobs. This is useful for storing small amounts of serializable data for later retrieval:\\n// store some data await io.store.job.set(\"💾\", \"disk-A\", \"Doom 1.2 Demo #3\"); // get it again later const value = await io.store.job.get<string>(\"read-💾\", \"disk-A\"); \\nIf you want to access the store from outside a run (e.g. just from your backend) you should use client.store instead.\\nThe following namespaces are at your disposal:\\nstore.env to access and store data within the Environment\\nstore.job to access and store data within the Job\\nstore.run to access and store data within the Run\\nreference docs\\nrandom\\nUse this task to generate a random number that stays stable during run retries/resumes:\\nconst randomNumber = await io.random(\"🎲\", { min: 1, max: 100, }); \\nreference docs\\nsendEvent\\nThis task allows you to send an event from inside your job run.\\nIf you want to send an event from outside a run (e.g. just from your backend) you should use client.sendEvent() instead.\\nawait io.sendEvent(\"🚚\", { id: \"e_1234567890\", name: \"new.user\", payload: { userId: \"u_1234567890\", }, }); \\nreference docs\\nsendEvents\\nThis task allows you to send multiple events from inside your job run.\\nIf you want to send multiple events from outside a run (e.g. just from your backend) you should use client.sendEvents() instead.\\nawait io.sendEvents(\"🚚🚚\", [ { id: \"e_12345\", name: \"new.user\", payload: { userId: \"u_12345\", }, }, { id: \"e_67890\", name: \"new.user\", payload: { userId: \"u_67890\", }, }, ]); \\nreference docs\\ngetEvent\\nThis task allows you to get an event by ID from inside your job run.\\nIf you want to get an event from outside a run (e.g. just from your backend) you should use client.getEvent() instead.\\nconst event = await io.getEvent(\"📥\", \"e_1234567890\"); \\nreference docs\\ncancelEvent\\nIf you send an event that has a delivery date in the future, you can use this task to cancel it.\\nawait io.sendEvent( \"🚚\", { id: \"e_1234567890\", name: \"new.user\", payload: { userId: \"u_1234567890\", }, }, { deliverAt: new Date(Date.now() + 1000 * 60 * 60 * 24), // deliver in 24 hours } ); // Later on, if you want to cancel the event: await io.cancelEvent(\"🚫\", \"e_1234567890\"); \\ncreateStatus\\nComing soon', metadata={'source': 'https://trigger.dev/docs/documentation/guides/task-library'}),\n",
       " Document(page_content='The Job subscribes to new GitHub issues and if the issues haven’t been dealt with after 24 hours a Slack reminder is sent and they’re assigned to someone.\\nYou can view the source code here.\\nWe cover how to:\\nget around serverless timeouts using Trigger.dev.\\nuse the CLI to get setup.\\nrun the Job locally.\\ncreate a GitHub onIssueOpened Trigger.\\ntest the Job.\\nuse Integrations inside the Job, including using API Keys (GitHub) and OAuth (Slack).\\nwrite the main logic of the Job.\\nuse rerun to quickly iterate.', metadata={'source': 'https://trigger.dev/docs/documentation/guides/video-walkthrough'}),\n",
       " Document(page_content='General Limits\\nThe following limits apply to the Trigger.dev Cloud service and users of the self-hosted version of Trigger.dev.\\nHobbyTeamSelf-hosted / Enterprise\\nTeam Members\\tUp to 2\\tUp to 5\\tCustom\\t\\nProjects\\t1\\tUp to 5\\tCustom\\t\\nJobs per Project\\tUp to 10\\tUp to 50\\tCustom\\t\\nRuns (per Month)\\t5,000\\tUp to 1m\\tCustom\\t\\nRun Log retention\\t24 hours\\t7 days\\tCustom\\t\\nConnected Integrations\\tUp to 50\\tUp to 1000\\tCustom\\t\\nTask Output Size\\t3MB\\t3MB\\t3MB\\t\\nTasks per Run\\tUp to 250\\tUp to 1000\\tCustom\\t\\nConcurrent Run Executions\\tUp to 10\\tUp to 100\\tCustom\\t\\nMaximum Task Duration\\t< 2m\\t< 2m\\t< Deployment Grace Period\\t\\nMaximum Run Execution Duration\\tup to 15m\\tup to 2 hrs\\tCustom\\t\\nYielded Executions per Run\\tUp to 100\\tUp to 100\\tCustom\\t\\nTasks per Run\\nFor any individual Job Run, the number of Tasks that can be executed is limited to 250 for Hobby and 1000 for Team plans. This limit is enforced to prevent runaway Jobs from consuming excessive resources.\\nWhat is a Task?\\nTasks are the fundamental building blocks on which the Trigger.dev service is constructed. You can create and run a task using io.runTask():\\nclient.defineJob({ id: \"task-example\", name: \"Task Example\", version: \"1.0.0\", trigger: eventTrigger({ name: \"task.example\" }), run: async (payload, io, ctx) => { const response = await io.runTask(\"task-1\", async (task) => { // Do some work here return { foo: \"bar\" }; }); }, }); \\nTasks power the following features as well:\\nio.wait()\\nio.sendEvent()\\nio.backgroundFetch()\\nio.logger\\nOur integration clients are also built on top of Tasks, so any time you call an integration client method, you are creating a Task. e.g.:\\nclient.defineJob({ id: \"send-resend-email\", name: \"Send Resend Email\", version: \"0.1.0\", trigger: eventTrigger({ name: \"send.email\", }), integrations: { resend, }, run: async (payload, io, ctx) => { // This creates a Task await io.resend.sendEmail(\"send-email\", { to: payload.to, subject: payload.subject, text: payload.text, from: \"Trigger.dev <hello@email.trigger.dev>\", }); }, }); \\nAnything that shows up as an item on the Run Log is a Task:\\nConcurrent Run Executions\\nA Run Execution is a single HTTP request from the Trigger.dev server to your endpoint to execute a run. The number of concurrent Run Executions is limited to 10 for Hobby and Team plans.\\nThis does not include runs that are waiting for a io.wait() to complete, so you could in theory have 1000s of “In Progress” jobs at a given time with no current run executions.\\nGoing over this limit does not abort or cancel runs, but it will prevent new run executions until the number of concurrent executions drops below the limit.\\nYou can limit the execution concurrency of a specific job like so:\\nclient.defineJob({ id: `test-job-1`, name: `Test Job 1`, version: \"1.0.0\", trigger: eventTrigger({ name: \"test\", }), concurrencyLimit: 5, // Limit this job to 5 concurrent executions }); \\nAlternatively, you can limit a group of jobs concurrency limit by defining a concurrency limit and passing it to the defineJobs() method:\\nconst concurrencyLimit = client.defineConcurrencyLimit({ id: `test-shared`, limit: 5, // Limit all jobs in this group to 5 concurrent executions }); client.defineJob({ id: `test-job-1`, name: `Test Job 1`, version: \"1.0.0\", trigger: eventTrigger({ name: \"test\", }), concurrencyLimit, }); client.defineJob({ id: `test-job-2`, name: `Test Job 2`, version: \"1.0.0\", trigger: eventTrigger({ name: \"test\", }), concurrencyLimit, }); \\nThe two jobs above will share the same concurrency limit, so between them they can only have 5 concurrent executions.\\nMaximum Task Duration\\nThe Maximum Task Duration is the maximum amount of time a single Task can run for. This limit is partly enforced by the Trigger.dev server, but also by the execution runtime of your deployed serverless function.\\nFor example, if you’re deploying to Vercel and using their Node.js Serverless functions, the maximum execution time is anywhere from 1 second to 5 minutes. If you have a single task that can run for longer than your maximum function execution time, it will never complete.\\nWe will retry tasks that never complete due to a timeout, but if the task continues to not complete, it will be marked as cancelled and the run will be timed out with an output like the following:\\n{ \"message\": \"Function timeout detected in 10s without any task creation. This is unexpected behavior and could lead to an infinite execution error because the run will never finish. This is likely caused by task \\\\\"initial-long-task\\\\\" execution exceeding the function timeout\" } \\nSee our Next.js section on Deployment for more information on how to configure your function timeout.\\nAdditionally, the Trigger.dev enforces a soft-cap of 2 minutes. Tasks that take longer than 2 minutes will be allowed to complete but we cannot guarentee that they won’t be retried erroneously or cause your run execution to be locked for up to 4 hours. This is because of a current limitation of Graphile Worker and our deployment platform.\\nMaximum Total Run Execution Duration\\nThe Maximum Total Run Execution Duration is the maximum amount of time a single run can execute for. Runs are completed over 1 or more executions, depending on many factors like the number of tasks, the serverless function timeout, task errors and delays. The Trigger.dev measures the total time spent across all run executions and will cancel the run if it exceeds the limit.\\nHobby plans have a limit of 15 minutes, Team plans have a limit of 2 hours, and Enterprise plans can set a custom limit.\\nYielded Executions per Run\\nYou can manually yield a run execution using io.yield(), which will exit the current run execution and schedule a new run execution to continue the run. You do this at most 100 times per run.', metadata={'source': 'https://trigger.dev/docs/documentation/concepts/limits'}),\n",
       " Document(page_content='Trigger.dev is a platform, SDK and API for building and running Jobs in your codebase, triggered by various sources, but without having to worry about managing any complicated orchestration infrastructure.\\nIt can be used from any Node.js (support versions) or TypeScript backend application (including serverless applications and microservices).\\nWhat we take care of for you:\\nWe make it possible to run long-running Jobs on serverless platforms that have short timeouts (e.g. 30 seconds).\\nWe provide an SDK for building Jobs in your codebase, triggered by various sources such as events, scheduled events, and webhooks.\\nWe provide an orchestration platform for running Jobs in your codebase.\\nWe provide out-of-the-box Integrations with popular services such as Slack, OpenAI, GitHub and more, which vastly simplifies the process interacting with 3rd-party services. \\nWe handle OAuth for you\\nWe provide a nice UI for viewing and debugging your Jobs.\\nWhat you take care of:\\nYou write your Jobs in your codebase.\\nYou get a Trigger.dev API Key and add it to your codebase.\\nYou deploy your codebase.\\nHow it works\\nTo get an idea of how Trigger.dev works, let’s take a look at a simple Job that sends a Slack message when a GitHub issue is labelled as critical:\\nimport { Job } from \"@trigger.dev/sdk\"; import { Github, events } from \"@trigger.dev/github\"; import { Slack } from \"@trigger.dev/slack\"; //GitHub integration with API Key (it supports OAuth too) const github = new Github({ id: \"github\", token: process.env.GITHUB_API_KEY!, }); //Slack integration with OAuth const slack = new Slack({ id: \"slack\", }); client.defineJob({ id: \"critical-issue-alert\", name: \"Critical Issue Alert\", version: \"0.1.0\", //When a GitHub issue is modified on the triggerdotdev/trigger.dev repo trigger: github.triggers.repo({ event: events.onIssue, owner: \"triggerdotdev\", repo: \"trigger.dev\", }), //include any integrations you want to use integrations: { slack, }, //this function gets executed when the trigger fires run: async (payload, io, ctx) => { await io.logger.info(`Action was ${payload.action}`); if (payload.action === \"labeled\" && payload.label?.name === \"critical\") { //use the Slack integration to post a message await io.slack.postMessage(\"post message\", { channel: \"C04GWUTDC3W\", text: `Issue ${payload.issue.number}: ${payload.issue.title} is critical!`, }); } }, }); \\nThis code lives in a file inside your project repo.\\nIt is listening for the issueEvent GitHub webhook, and when it receives one, we will take care of calling the run function supplied to the Job constructor with the webhook payload. This gives you the following advantages over traditional webhooks:\\nWe will automatically register the webhook with GitHub for you, and verify the payload signature.\\nWe provide a nicely typed event payload to your run function, so you don’t have to setup webhook payload types.\\nIf your server isn’t running, we will wait until it’s back online before attempting to run the Job.\\nIt is very easy to test your Job locally using our Test Run feature.\\nAs you can see the above Job also makes a call to our Slack postMessage function, which provides the following advantages over using the raw Slack API:\\nWe automatically handle the OAuth flow for you, so you don’t have to worry about setting up a Slack app and dealing with credentials in your code (see our Authentication guide for more details).\\nWe will automatically retry the request if the Slack API returns an error.\\nWe provide a nicely typed response object from your postMessage function, so you don’t have to setup Slack API types.\\nWhy use Trigger.dev?\\nApart from the reasons mentioned above, there are a few other reasons why you might want to use Trigger.dev:\\nYou want to access your database or other internal services from your Jobs, without having to expose them to the internet.\\nYou want to colocate your Jobs with your code, so you can deploy them together in one atomic unit.\\nYou want to build event-driven architectures without having to manage any complicated orchestration infrastructure.\\nYou want to add in delays or retries to your Jobs, without having to worry about managing a queue.\\nArchitecture\\nBelow is a simplified architecture diagram of how Trigger.dev works:\\nAs you can see above, we communicate between your code and the Trigger.dev platform. This allows us to send events to your code, and receive tasks from your code.\\nLimitations\\nThere are a few limitations that are important to understand.\\nIn the latest version the following are not supported:\\nLong-running servers\\nCurrently Trigger.dev is optimized for deployment to serverless functions, but not for long-running servers.\\nThis limitation will be removed in the future by adding an alternative mode so Jobs works well on localhost and long-running servers.\\nCompute intensive tasks\\nBecause Trigger.dev is optimized for serverless functions, it is not well suited for compute intensive jobs as each individual task is limited to the Maximum Run Chunk Execution Duration', metadata={'source': 'https://trigger.dev/docs/documentation/concepts/what-is-triggerdotdev'}),\n",
       " Document(page_content='User’s are members of one or more Organizations. Each Organization can have many Projects.\\nJobs are scoped to a specific Project. This means they can only be run in the context of a Project. Jobs are associated with a Project by using a Project API key.\\nCurrently all members of an Organization have access to all Projects. More granular access control will be added in the future.', metadata={'source': 'https://trigger.dev/docs/documentation/concepts/projects'}),\n",
       " Document(page_content='export const client = new TriggerClient({ id: \"my-webapp\", apiKey: process.env.TRIGGER_API_KEY!, });', metadata={'source': 'https://trigger.dev/docs/documentation/concepts/client-adaptors'}),\n",
       " Document(page_content='Runs can exceed the maximum timeout on serverless platforms and can survive server restarts.\\nHow does this work?\\nWhen a Run is created, it is given a unique ID. This ID is used to identify the Run.\\nTasks have a cacheKey which is a string and is the first parameter. This should be stable and unique inside that run function.\\nWhen a Task is completed, its output is stored.\\nIf a Run exceeds the timeout, or your server restarts, the Run will be “replayed”.\\nThe second+ time it is run, Tasks that have already successfully completed will immediately return their first output. The code inside them won’t re-run.\\nHow to use cache keys\\nLike we mentioned above, we use Task cache keys to determine which Tasks have already been executed. They are defined by you inside your run function, for example when you call io.slack.postMessage:\\nawait io.slack.postMessage(\"⭐️ New Star\", { channel: \"C04GWUTDC3W\", text: `@${starredBy} just starred ${repoName}!`, }); \\nIn this example, the cache key is the string \"⭐️ New Star\". This means that if the Job is interrupted and then resumed, the slack.postMessage Task will be skipped because it has already been executed.\\nIf you make multiple calls to slack.postMessage, you should use different cache keys for each call. For example:\\nawait io.slack.postMessage(\"⭐️ New Star\", { channel: \"C04GWUTDC3W\", text: `@${starredBy} just starred ${repoName}!`, }); await io.slack.postMessage(\"🚨 Critical Issue\", { channel: \"C04GWUTDC3W\", text: `@${assignee} just opened a critical issue in ${repoName}!`, }); \\nIf you are calling a Task multiple times with the same key, it will only be executed once. For example, if you call slack.postMessage with the key \"⭐️ New Star\" twice, it will only be executed once.\\nSee our Task concept guide for more information about tasks and how they are crucial to the resumability of your Jobs.\\nHow to use cache keys with loops\\nIf you are using a loop, you should use the loop index as the key. For example:\\nfor (let i = 0; i < 10; i++) { await ctx.waitFor(`Wait ${i}`, { seconds: 30 }); await slack.postMessage(`⭐️ New Star ${i}`, { channelName: \"github-stars\", text: `@${starredBy} just starred ${repoName}!`, }); } \\nHow does this work?\\nHow to use cache keys\\nHow to use cache keys with loops', metadata={'source': 'https://trigger.dev/docs/documentation/concepts/resumability'}),\n",
       " Document(page_content='//... the code from above that created the caldotcom endpoint client.defineJob({ id: \"http-caldotcom\", name: \"HTTP Cal.com\", version: \"1.0.0\", enabled: true, //this create a Trigger using the caldotcom endpoint trigger: caldotcom.onRequest(), run: async (request, io, ctx) => { //note that when using HTTP endpoints, the first parameter is the request //you need to get the body, usually it will be json so you do: const body = await request.json(); await io.logger.info(`Body`, body); }, });', metadata={'source': 'https://trigger.dev/docs/documentation/concepts/http-endpoints'}),\n",
       " Document(page_content='client.defineJob({ id: \"delay-job\", name: \"Delay Job\", version: \"0.0.1\", trigger: eventTrigger({ name: \"example.event\", }), run: async (payload, io, ctx) => { await io.logger.info(\"Hi\"); // the second parameter is the number of seconds to wait await io.wait(\"wait 30 days\", 30 * 24 * 60 * 60); await io.logger.info(\"Sorry for the slow reply!\"); }, });', metadata={'source': 'https://trigger.dev/docs/documentation/concepts/delays'}),\n",
       " Document(page_content='import { Github } from \"@trigger.dev/github\"; import { Slack } from \"@trigger.dev/slack\"; //1. create GitHub client using a token const github = new Github({ id: \"github\", token: process.env.GITHUB_TOKEN!, }); //2. create Slack client using OAuth const slack = new Slack({ id: \"slack\", }); client.defineJob({ id: \"alert-on-new-github-issues\", name: \"Alert on new GitHub issues\", version: \"0.1.1\", integrations: { //3. include the slack integration slack, }, //4. use the github integration to listen for new issues trigger: github.triggers.repo({ event: events.onIssueOpened, owner: \"triggerdotdev\", repo: \"trigger.dev\", }), run: async (payload, io, ctx) => { //5. send a message to slack const response = await io.slack.postMessage(\"Slack alert\", { text: `New Issue opened: ${payload.issue.html_url}`, channel: \"C04GWBTDQ7W\", }); return response; }, });', metadata={'source': 'https://trigger.dev/docs/documentation/concepts/integrations'}),\n",
       " Document(page_content='What is it?\\nIt allows you to easily add Integrations for your users, such as:\\nAdd contacts to their Hubspot.\\nSync data to their Airtable.\\nPost videos to their YouTube.\\nAdd issues to their Linear.\\nHow does it work?\\nCreate an Integration in the Dashboard.\\nUse our provided React components in your web app to authenticate your users.\\nTrigger a Job as one of your users.\\nInside the Run function the relevant Integrations will be authenticated with your user’s credentials.\\nProfit.\\nWhat is it?\\nHow does it work?', metadata={'source': 'https://trigger.dev/docs/documentation/concepts/connect'}),\n",
       " Document(page_content='export const client = new TriggerClient({ id: \"nextjs-example\", //this environment variable should be set to your DEV API Key locally, //and your PROD API Key in production apiKey: process.env.TRIGGER_API_KEY!, });', metadata={'source': 'https://trigger.dev/docs/documentation/concepts/environments-endpoints'}),\n",
       " Document(page_content='Introduction\\nIf you’re setting up your project for the first time, we recommend using the CLI by following the Quickstart guide.\\ninit Command\\nThe init command incorporates Trigger.dev into your project. It performs the following functions:\\nAdds Trigger.dev to the project.\\nCreates a new route.\\nGenerates an example file.\\nDuring execution, this command requires some configuration parameters. Detailed information on these parameters can be found in the next section.\\nRun this init command in a terminal window to setup your project with Trigger.dev\\nCLI steps explained\\ndev Command\\nOnce you’re running your project locally, you can then execute the dev CLI command to run Trigger.dev locally. You should run this command every time you want to use Trigger.dev locally.\\nIn a new terminal window or tab run:\\n\\nupdate Command\\nThe update command will update all Trigger.dev packages to the latest version.\\nwhoami Command\\nThe whoami command will print out information about your current Trigger.dev project and environment, based on the API key found in your .env or .env.local file\\nsend-event Command\\nThe send-event command will send an event to your Trigger.dev project. This is useful for testing your Trigger.dev project locally.\\nIntroduction\\ninit Command\\nCLI steps explained\\ndev Command\\nupdate Command\\nwhoami Command\\nsend-event Command', metadata={'source': 'https://trigger.dev/docs/documentation/guides/cli'}),\n",
       " Document(page_content=\"There are two ways to run your Jobs:\\nTriggering a test Run\\nTriggering your Job for real\\n1. Triggering a test Run\\nYou can perform a Run with any payload you want or use one of our examples on the test page in our dashboard.\\nHow to do a test Run from the dashboard.\\n2. Triggering your Job for real\\nTriggering your Job for real depends on the type of trigger you have attached to your Job.\\nThere are three types of Trigger:\\nScheduled Trigger\\nScheduled Triggers are triggered automatically by our system on the schedule you define. Once you have deployed your Job, it will run automatically.\\nEvent Trigger\\nEvent Triggers are triggered by an event that you send from elsewhere in your code.\\nThere are two way to send an event that will trigger eventTrigger():\\nUse client.sendEvent() from anywhere in your codebase.\\nUse io.sendEvent() from inside a Job's run() function.\\nWebhook Trigger\\nWebhook Triggers are triggered when you perform the action that the webhook subscribes to. For example, if you have a webhook Trigger that subscribes to the pull_request event on GitHub, then it will be triggered whenever a pull request is opened, closed, or updated on the repo you configured.\", metadata={'source': 'https://trigger.dev/docs/documentation/guides/running-jobs'}),\n",
       " Document(page_content='Given the following custom event payload:\\n{ \"uid\": \"jexAgaeJFJsrGfans1pxqm\", \"type\": \"15 Min Meeting\", \"price\": 0, \"title\": \"15 Min Meeting between Eric Allam and John Doe\", \"length\": 15, \"status\": \"ACCEPTED\", \"endTime\": \"2023-01-25T16:00:00Z\", \"bookingId\": 198052, \"organizer\": { \"id\": 32794, \"name\": \"Eric Allam\", \"email\": \"eric@trigger.dev\", \"language\": { \"locale\": \"en\" }, \"timeZone\": \"Europe/London\" } } \\nThe following event filter would match the event:\\n{ \"type\": [\"15 Min Meeting\"], \"status\": [\"ACCEPTED\", \"REJECTED\"], \"organizer\": { \"name\": [\"Eric Allam\"] } } \\nFor an event pattern to match an event, the event must contain all the field names listed in the event pattern. The field names must also appear in the event with the same nesting structure.\\nThe value of each field name in the event pattern must be an array of strings, numbers, or booleans. The event pattern matches the event if the value of the field name in the event is equal to any of the values in the array.\\nEffectively, each array is an OR condition, and the entire event pattern is an AND condition.\\nSo the above event filter will match because status == \"ACCEPTED\", and it would also match if status == \"REJECTED\".\\nString Filters\\nString filters are used to match string values within the event payload. You can filter events based on exact string matches, case-insensitive matches, starts-with, ends-with, and more.\\nExamples:\\nname: [\"John\"]: Triggers the job if the name field in the event payload is exactly “John”.\\nname: [{ $startsWith: \"Jo\" }]: Triggers the job if the name field starts with “Jo”.\\nname: [{ $endsWith: \"hn\" }]: Triggers the job if the name field ends with “hn”.\\nname: [{ $ignoreCaseEquals: \"john\" }] : Triggers the job if the name field is equal to “john” regardless of case.\\nExamples\\nclient.defineJob({ id: \"notify-hr-new-hires-specific-hobbies\", name: \"Notify HR of New Hires with Specific Hobbies\", version: \"1.0.0\", trigger: eventTrigger({ name: \"employee.hired\", schema: z.object({ name: z.string(), email: z.string(), hobbies: z.array(z.string()), }), filter: { name: [{ $startsWith: \"Jo\" }], // Only trigger for employees whose name starts with \"Jo\" }, }), run: async (payload, io, ctx) => {}, }); \\nclient.defineJob({ id: \"notify-hr-new-hires-specific-hobbies\", name: \"Notify HR of New Hires with Specific Hobbies\", version: \"1.0.0\", trigger: eventTrigger({ name: \"employee.hired\", schema: z.object({ name: z.string(), email: z.string(), hobbies: z.array(z.string()), }), filter: { name: [\"John\"], // Only trigger for employees whose name is \"John\" }, }), run: async (payload, io, ctx) => {}, }); \\nBoolean Filters\\nBoolean filters are used to filter events based on boolean values within the event payload.\\nExamples:\\npaidPlan: [true]: Triggers the job if the PaidPlan field in the event payload is true.\\nisAdmin: [false]: Triggers the job if the isAdmin field in the event payload is false.\\nExamples\\nclient.defineJob({ id: \"notify-admins-paid-users\", name: \"Notify Admins of New Paid Users\", version: \"1.0.0\", trigger: eventTrigger({ name: \"user.created\", schema: z.object({ name: z.string(), email: z.string(), paidPlan: z.boolean(), isAdmin: z.boolean(), }), filter: { paidPlan: [true], isAdmin: [true], // Only trigger for paid users who are also admins }, }), run: async (payload, io, ctx) => {}, }); \\nclient.defineJob({ id: \"send-premium-content\", name: \"Send Premium Content to Subscribed Users\", version: \"1.0.0\", trigger: eventTrigger({ name: \"content.available\", schema: z.object({ userId: z.string(), subscriptionStatus: z.boolean(), }), filter: { subscriptionStatus: [true], // Only trigger for users with an active subscription }, }), run: async (payload, io, ctx) => {}, }); \\nNumber Filters\\nNumber filters are used to filter events based on numeric values within the event payload. It can also be used to perform numeric comparisons on values within the event payload.\\nExamples:\\nage [18]: Triggers the job if the age field in the event payload is equal to 18.\\nscore: [{ $gt: 90 }] : Triggers the job if the score field is greater than 90.\\nscore: [{ $gte: 90 }] : Triggers the job if the score field is greater than or equal to 90.\\nscore: [{ $lt: 90 }] : Triggers the job if the score field is less than 90.\\nscore: [{ $lte: 90 }] : Triggers the job if the score field is less than or equal to 90.\\nage: [{ $gt: 20 }, { $lt: 40 }]: Triggers the job if the age field is greater than 20 and less than 40.\\nscore: [{ $between: [90, 110] }]: Triggers the job if the score field is between 90 and 110.\\nExamples\\nclient.defineJob({ id: \"send-discount-high-scores\", name: \"Send Discount to High Scoring Customers\", version: \"1.0.0\", trigger: eventTrigger({ name: \"customer.score.updated\", schema: z.object({ customerId: z.string(), score: z.number(), }), filter: { score: [{ $gt: 90 }], // Only trigger for customers with a score greater than 90 }, }), run: async (payload, io, ctx) => {}, }); \\nclient.defineJob({ id: \"notify-upcoming-birthdays\", name: \"Notify Users of Upcoming Birthdays\", version: \"1.0.0\", trigger: eventTrigger({ name: \"user.birthday\", schema: z.object({ userId: z.string(), age: z.number(), }), filter: { age: [25], // Only trigger for users who are turning 25 }, }), run: async (payload, io, ctx) => {}, }); \\nclient.defineJob({ id: \"send-discount-code\", name: \"Send Discount Code\", version: \"1.0.0\", trigger: eventTrigger({ name: \"user.purchase\", schema: z.object({ userId: z.string(), age: z.number(), }), filter: { age: [{ $gt: 18 }, { $lt: 30 }], // Only trigger for users aged between 18 and 30 }, }), run: async (payload, io, ctx) => {}, }); \\nclient.defineJob({ id: \"update-user-level\", name: \"Update User Level\", version: \"1.0.0\", trigger: eventTrigger({ name: \"user.scoreUpdated\", schema: z.object({ userId: z.string(), score: z.number(), }), filter: { score: [{ $between: [50, 100] }], // Only trigger for scores between 50 and 100 (inclusive) }, }), run: async (payload, io, ctx) => {}, }); \\nArray Filters\\nArray filters are used to filter events based on the content of arrays within the event payload.\\nExamples:\\nhobbies: [{ $includes: \"reading\" }]: Triggers the job if the hobbies array includes the value “reading”.\\nExamples\\nclient.defineJob({ id: \"recommend-books-avid-readers\", name: \"Recommend Books to Avid Readers\", version: \"1.0.0\", trigger: eventTrigger({ name: \"user.preferences.updated\", schema: z.object({ userId: z.string(), hobbies: z.array(z.string()), }), filter: { hobbies: [{ $includes: \"reading\" }], // Only trigger for users whose hobbies include \"reading\" }, }), run: async (payload, io, ctx) => { //run function }, }); \\nExistence Filters\\nExistence filters are used to filter events based on the presence or absence of certain keys within the event payload.\\nExamples:\\nname: [{ $exists: true }] : Triggers the job if the name field exists in the event payload.\\nfoo: [{ $exists: false }] : Triggers the job if the foo field does not exist in the event payload.\\nExamples\\nclient.defineJob({ id: \"send-welcome-email\", name: \"Send Welcome Email\", version: \"1.0.0\", trigger: eventTrigger({ name: \"user.created\", schema: z.object({ userId: z.string(), name: z.string(), email: z.string(), }), filter: { name: [{ $exists: true }], // Only trigger for events where the \\'name\\' field exists }, }), run: async (payload, io, ctx) => {}, }); \\nclient.defineJob({ id: \"notify-admin-missing-field\", name: \"Notify Admins of Missing Field\", version: \"1.0.0\", trigger: eventTrigger({ name: \"user.updated\", schema: z.object({ userId: z.string(), isAdmin: z.boolean(), }), filter: { foo: [{ $exists: false }], // Only trigger if the \\'foo\\' field does not exist }, }), run: async (payload, io, ctx) => {}, }); \\nCombining Filters\\nFilters can be combined to create more complex conditions for triggering jobs.\\nExamples:\\nname: [\"Alice\"], age: [30] : Triggers the job if the name is “Alice” and the age is 30.\\nname: [\"Alice\"], age: [{ $gt: 20 }, { $lt: 40 }] : Triggers the job if the name is “Alice” and the age is between 20 and 40.\\nname: [\"Alice\", \"Bob\"] : Triggers the job if the name is either “Alice” or “Bob”.\\nname: [\"Alice\", \"Bob\"], age: [30] : Triggers the job if the name is either “Alice” or “Bob” and the age is 30.\\nExamples\\nclient.defineJob({ id: \"notify-vip-event\", name: \"Notify VIP Customers\", version: \"1.0.0\", trigger: eventTrigger({ name: \"user.registration\", schema: z.object({ userId: z.string(), name: z.string(), age: z.number(), totalPurchases: z.number(), }), filter: { age: [{ $gt: 25 }, { $lt: 60 }], // Trigger for users aged between 25 and 60 totalPurchases: [{ $gte: 3 }], // Trigger for users with 3 or more total purchases }, }), run: async (payload, io, ctx) => {}, }); \\nclient.defineJob({ id: \"process-orders\", name: \"Process Orders\", version: \"1.0.0\", trigger: eventTrigger({ name: \"order.placed\", schema: z.object({ orderId: z.string(), customerId: z.string(), region: z.string(), totalAmount: z.number(), }), filter: { region: [\"US\", \"Canada\"], // Trigger for orders from US or Canada totalAmount: [{ $gt: 100 }, { $lt: 1000 }], // Trigger for orders with a total amount between 100 and 1000 }, }), run: async (payload, io, ctx) => {}, }); \\nUsing with Stripe triggers\\nHere is an example of a Stripe Trigger with an Event Filter:\\nclient.defineJob({ id: \"stripe-on-subscription-created\", name: \"Stripe On Subscription Created\", version: \"0.1.0\", trigger: stripe.onCustomerSubscriptionCreated({ filter: { currency: [\"usd\"], }, }), run: async (payload, io, ctx) => { await io.logger.info(\"ctx\", { ctx }); }, }); \\nThe job is triggered by the onCustomerSubscriptionCreated event from Stripe. It is configured to trigger only when certain conditions are met. In this case, the job is triggered when a subscription is created with the following condition:\\nCurrency: Only subscriptions with the currency “USD” will trigger this job.\\nUsing with Supabase triggers\\nHere is an example of a Supabase Trigger with an Event Filter:\\nclient.defineJob({ id: \"supabase-management-example-objects-storage\", name: \"Supabase Management Example Object Storage\", version: \"0.1.0\", trigger: triggers.onInserted({ schema: \"storage\", table: \"objects\", filter: { record: { bucket_id: [\"example_bucket\"], // Only trigger for objects in the \"example_bucket\" bucket name: [ { $endsWith: \".png\", // Only trigger for objects with a name ending in \".png\" }, ], path_tokens: [ { $includes: \"images\", // Only trigger for objects with a path that includes the token \"images\" }, ], }, }, }), integrations: { openai, supabase, }, run: async (payload, io, ctx) => {}, }); \\nThis Listens for insert events in the “objects” table of the “storage” schema.\\nConditions for Triggering: \\nObject belongs to “example_bucket”.\\nObject’s name ends with “.png”.\\nObject’s path includes the token “images”.', metadata={'source': 'https://trigger.dev/docs/documentation/guides/event-filter'}),\n",
       " Document(page_content='Disabling Jobs\\nTo prevent a Job from processing new Runs, you can disable it by setting the enabled option:\\nclient.defineJob({ id: \"example-job\", name: \"Example Job\", version: \"0.1.0\", trigger: eventTrigger({ name: \"example.event\" }), enabled: false, run: async (payload, io, ctx) => { // your Job code here }, }); \\nIf you omit the enabled option, it will default to true.\\nThe Job will only be disabled in environments that have seen the enabled = false value. So the Job will remain enabled in production until the code with the enabled = false is deployed to production.\\nOnce a Job is disabled no new Runs will be created for that Job, and it will still be visible in the Dashboard as disabled:\\nIn-progress Runs\\nIn-progress Runs will be allowed to finish, even Runs that are currently delayed from a call to io.wait. If you’d like to completely stop in-progress Runs, you have two options:\\nSet the enabled option to false and then throw an error at the top of your Job run function.\\nclient.defineJob({ id: \"example-job\", name: \"Example Job\", version: \"0.1.0\", trigger: eventTrigger({ name: \"example.event\" }), enabled: false, run: async (payload, io, ctx) => { throw new Error(\"Job disabled\"); }, }); \\nDelete the Job from your codebase. This will disable the Job as well but also stop in progress Runs.\\nDisabling in production with env vars\\nYou can easily disable Jobs in production using env vars so you don’t have to deploy new code to disable a Job.\\nclient.defineJob({ id: \"example-job\", name: \"Example Job\", version: \"0.1.0\", trigger: eventTrigger({ name: \"example.event\" }), enabled: process.env.TRIGGER_JOBS_DISABLED !== \"true\", run: async (payload, io, ctx) => { // your Job code here }, }); \\nThen you can disable the Job in production by setting the TRIGGER_JOBS_DISABLED env var to \"true\". And removing the env var will re-enable the Job.\\nDeleting Jobs\\nOnce you have disabled a Job in all environments, you can delete it from the dashboard by navigating to the Job list page and clicking the “triple-dot” menu next to the Job you want to delete:\\nThis will bring up a dialog confirming that you want to delete the Job and all of its history:\\nDisabling Jobs\\nIn-progress Runs\\nDisabling in production with env vars\\nDeleting Jobs', metadata={'source': 'https://trigger.dev/docs/documentation/guides/jobs/managing'}),\n",
       " Document(page_content='Trigger.dev is open source and we welcome contributions from the community.\\nGitHub Repo\\nOur Open Source GitHub repo\\nView and create issues\\nYou can view open issues and create new ones\\nCreate an Integration\\nCreate an Integration for your own use or as a public package.\\nJoin the community\\nThe place to meet other users, the team and to get product updates.', metadata={'source': 'https://trigger.dev/docs/documentation/guides/contributing'}),\n",
       " Document(page_content='Intro\\nZod is a fantastic utility package by @colinhacks that allows for defining runtime schema validation and type-safety.\\nWe use it extensively internally at Trigger.dev.\\nBut there are a few places where we ask you to provide us with a Zod schema, for example when defining your own events:\\nclient.defineJob({ id: \"new-user\", name: \"New user\", version: \"0.1.0\", //the eventTrigger uses zod to define the schema trigger: eventTrigger({ name: \"user.created\", schema: z.object({ name: z.string(), email: z.string(), paidPlan: z.boolean(), }), }), //this function is run when the custom event is received run: async (payload, io, ctx) => { //do stuff }, }); \\nSo it will help to know a little about Zod and how to use it. We definitely recommend the well written Zod README but we’ve included a short primer below.\\nBasic Usage\\nThere are three main steps to using Zod:\\nDefine a schema\\nimport { z } from \"zod\"; const mySchema = z.object({ name: z.string(), email: z.string(), paidPlan: z.boolean(), }); \\nInfer TypeScript types from the schema\\ntype MySchema = z.infer<typeof mySchema>; \\nValidate data against the schema\\nconst data: unknown = { name: \"Eric\", email: \"eric@trigger.dev\", paidPlan: true, }; const result = mySchema.parse(data); \\nWhen using Zod with Trigger.dev, you’ll only really need to do the first step, and by passing it to us (through the schema property of the customEvent function), we’ll do the second and third steps for you.\\nDefining Schemas\\nPrimitives\\nZod schemas are a way to define the shape of an object. They can be as simple as a single type, or as complex as a nested object.\\n// Primitives z.string(); z.number(); z.boolean(); z.date(); z.undefined(); z.null(); \\nAny schema can be marked as optional, which means the schema can be undefined or null:\\nSchemas can also be marked optional by providing a default value:\\nconst optionalString = z.string().default(\"default value\"); const value = optionalString.parse(undefined); // value === \"default value\" \\nIf you need to allow a value to be null, you can use nullable():\\nconst nullableString = z.string().nullable(); const value = nullableString.parse(null); \\nYou can also use Zod to coerce primites into other types. For example, you can coerce a string into a number:\\nconst numberString = z.coerce.number(); const value = numberString.parse(\"123\"); // value === 123 \\nThe following primitives are supported:\\nz.coerce.string(); z.coerce.number(); z.coerce.boolean(); z.coerce.bigint(); z.coerce.date(); \\nCoercing dates are especially useful when you are receiving a string from an API and want to convert it to a JavaScript Date object:\\nconst date = z.coerce.date(); const value = date.parse(\"2021-01-01T00:00:00.000Z\"); // value === Date object \\nObjects\\nObject schemas are the most common type of schema. They allow you to define the shape of an object, and the types of each property.\\nconst mySchema = z.object({ name: z.string(), email: z.string(), paidPlan: z.boolean(), }); \\nAll properties are required by default, although you can make them all optional using partial():\\nconst mySchema = z .object({ name: z.string(), email: z.string(), paidPlan: z.boolean(), }) .partial(); \\nYou can also make individual properties optional:\\nconst mySchema = z.object({ name: z.string(), email: z.string(), paidPlan: z.boolean().optional(), }); \\nBy default an object schema will strip out any extra properties that are not defined in the schema. You can disable this behavior using passthrough():\\nconst mySchema = z.object({ name: z.string(), }); mySchema.parse({ name: \"Eric\", email: \"eric@trigger.dev\" }); // { name: \"Eric\" } mySchema.passthrough().parse({ name: \"Eric\", email: \"eric@trigger.dev\" }); // { name: \"Eric\", email: \"eric@trigger.dev\" } \\nOr you can use strict() to make the schema throw an error if there are any extra properties:\\nconst mySchema = z .object({ name: z.string(), }) .strict(); mySchema.parse({ name: \"Eric\", email: \"eric@trigger.dev\" }); // throws error \\nZod includes a few useful object schema utilities to help with reusing schemas, extends() and merge():\\nconst baseSchema = z.object({ name: z.string(), }); const extendedSchema = baseSchema.extend({ email: z.string(), }); const mergedSchema = baseSchema.merge( z.object({ email: z.string(), }) ); \\nYou can also use pick() and omit() to create a new schema that only includes or excludes certain properties:\\nconst mySchema = z.object({ name: z.string(), email: z.string(), paidPlan: z.boolean(), }); const pickedSchema = mySchema.pick({ name: true, email: true }); const omittedSchema = mySchema.omit({ paidPlan: true }); \\nArrays\\nYou can specify the schema of an array using z.array():\\nz.array(z.string()); // string[] z.array(z.number()); // number[] z.array(z.object({ name: z.string() })); // Array<{ name: string }> \\nYou can also specify the type of array that has a fixed number of elements using z.tuple():\\nz.tuple([z.string(), z.number(), z.boolean()]); // [string, number, boolean] \\nUnions\\nYou can specify a union of schemas using z.union():\\nz.union([z.string(), z.number(), z.boolean()]); // string | number | boolean \\nDiscriminating unions are also supported, and especially useful when paired with type narrowing:\\nconst mySchema = z.discriminatingUnion(\"type\", [ z.object({ type: z.literal(\"a\"), data: z.string(), }), z.object({ type: z.literal(\"b\"), data: z.number(), }), ]); const value = mySchema.parse({ type: \"a\", data: \"hello\" }); if (type.a) { // value is { type: \"a\", data: string } } else if (type.b) { // value is { type: \"b\", data: number } } \\nRecords\\nYou can specify a record of schemas using z.record(), useful for when you have a map of values but don’t care about the keys:\\nz.record(z.string()); // Record<string, string> z.record(z.number()); // Record<string, number> \\nJSON type\\nIf you want to accept any valid JSON value, you can use the following schema:\\nconst literalSchema = z.union([z.string(), z.number(), z.boolean(), z.null()]); type Literal = z.infer<typeof literalSchema>; type Json = Literal | { [key: string]: Json } | Json[]; const jsonSchema: z.ZodType<Json> = z.lazy(() => z.union([literalSchema, z.array(jsonSchema), z.record(jsonSchema)]) ); jsonSchema.parse(data); \\nHat tip to ggoodman for this one.\\nResources\\nMatt Pocock @mattpocock has a great free Zod Tutorial up on Total Typescript that covers the basics of Zod.\\nEasily generate Zod schemas from JSON, JSON Schemas, or TypeScript types.', metadata={'source': 'https://trigger.dev/docs/documentation/guides/zod'}),\n",
       " Document(page_content='Community & Support\\nGet in touch with us!\\nConcepts\\nJobs, Runs & Tasks\\nTriggers\\nGuides\\nFrameworks\\nManual setup\\nUsing the Dashboard\\nUsing Integrations\\nReact hooks\\nDeployment\\nSelf hosting\\nCommunity & Support\\nOverview\\nContact us if you have any questions, or would like help with any issues.\\nWe are active in the community and will respond to all messages.\\nPlease email us at help@trigger.dev or choose one of the options below:', metadata={'source': 'https://trigger.dev/docs/documentation/get-help'}),\n",
       " Document(page_content='What does this mean?\\nInstead of using the Trigger.dev Cloud service which we host for you, you can host the Trigger.dev platform yourself.\\nLocal development\\nWhen you’re working on your web app, you can run Trigger.dev locally alongside your web app.\\nSee our GitHub docker project for instructions.\\nYou will need to setup tunneling if you want to receive webhooks from external services.\\nDeployment\\nWe provide an official Trigger.dev docker image you can use to easily self-host on your preferred platform. We also provide guides for some popular platforms:\\nWhat does this mean?\\nLocal development\\nDeployment', metadata={'source': 'https://trigger.dev/docs/documentation/guides/self-hosting'}),\n",
       " Document(page_content='Only what you choose to send. The main body of your Job code runs on your infrastructure. For example when you do a database query, that never touches us. We receive data that triggers the start of a Job, any data you pass to one of our API Integrations, and any data you choose to log using our logging function. We store this to display on the Runs page of your dashboard.\\nYou can use Trigger.dev with a variety of frameworks. You can check out the full list here.\\nYes. Jobs are created in your code locally.\\nYes. There’s no limit to the complexity of Jobs you can create. Jobs are created in code so you can write conditional, looping, branching or time delayed logic.\\nA run is a single execution of a Job. This can be in either development, staging or production. View full details.\\nYes. You create Jobs directly in your own code so it’s version controlled with everything else.\\nA simple Job doing a couple of API calls from different services will take about 5 minutes to create.\\nView our Integrations page to see the Integrations we currently support. If we don’t have an Integration you need, you can request it or create it yourself.\\nYes, Trigger.dev is open source. We are strong supporters of open source software, and our first product, jsonhero.io, has a thriving open source community. Trigger.dev follows in that tradition.\\nNo. Trigger.dev is designed for developers who want to create Jobs directly in code, without using a UI builder like Zapier. This allows developers to stay in their familiar development environment and customise their Jobs with code.\\nDevelopers will need to create Jobs. However, anyone on the team can monitor running Jobs in the Trigger.dev dashboard.', metadata={'source': 'https://trigger.dev/docs/documentation/faq'}),\n",
       " Document(page_content='Quick Starts\\nQuick Starts: Introduction\\nConcepts\\nJobs, Runs & Tasks\\nTriggers\\nGuides\\nFrameworks\\nManual setup\\nUsing the Dashboard\\nUsing Integrations\\nReact hooks\\nDeployment\\nSelf hosting\\nCommunity & Support\\nOverview\\nQuick Starts: Introduction\\nTrigger.dev lets you create long-running jobs in serverless environments. Support for long-running servers is coming soon.\\nSelect a framework to get started…\\nOr quickly setup Trigger.dev with…\\nSelect a framework to get started…\\nOr quickly setup Trigger.dev with…', metadata={'source': 'https://trigger.dev/docs/documentation/quickstarts/introduction'}),\n",
       " Document(page_content='Installing Required Packages\\nTo begin, install the necessary packages in your Next.js project directory. You can choose one of the following package managers:\\n\\nObtaining the Development API Key\\nTo locate your development API key, login to the Trigger.dev dashboard and select the Project you want to connect to. Then click on the Environments & API Keys tab in the left menu. You can copy your development API Key from the field at the top of this page. (Your development key will start with tr_dev_).\\nAdding Environment Variables\\nCreate a .env.local file at the root of your project and include your Trigger API key and URL like this:\\nTRIGGER_API_KEY=ENTER_YOUR_DEVELOPMENT_API_KEY_HERE TRIGGER_API_URL=https://api.trigger.dev # this is only necessary if you are self-hosting \\nReplace ENTER_YOUR_DEVELOPMENT_API_KEY_HERE with the actual API key obtained from the previous step.\\nConfiguring the Trigger Client\\nCreate a file at <root>/src/trigger.ts or <root>/trigger.ts depending on whether you’re using the src directory or not. <root> represents the root directory of your project.\\nNext, add the following code to the file which creates and exports a new TriggerClient:\\n// trigger.ts (for TypeScript) or trigger.js (for JavaScript) import { TriggerClient } from \"@trigger.dev/sdk\"; export const client = new TriggerClient({ id: \"my-app\", apiKey: process.env.TRIGGER_API_KEY, apiUrl: process.env.TRIGGER_API_URL, }); \\nReplace “my-app” with an appropriate identifier for your project.\\nCreating the API Route\\nTo establish an API route for interacting with Trigger.dev, follow these steps based on your project’s file type and structure\\nApp Directory\\nPages Directory\\nCreate a new file named route.(ts/js) within the app/api/trigger/ directory.\\nAdd the following code to route.(ts/js):\\nimport { createAppRoute } from \"@trigger.dev/nextjs\"; import { client } from \"@/trigger\"; import \"@/Jobs\"; export const { POST, dynamic } = createAppRoute(client); \\nCreating the Example Job\\nCreate a folder named Jobs alongside your app or pages directory\\nInside the Jobs folder, add two files named example.(ts/js) and index.(ts/js).\\nAdditonal Job Definitions\\nYou can define more job definitions by creating additional files in the Jobs folder and exporting them in index file.\\nFor example, in index.(ts/js), you can export other job files like this:\\n// import all your job files here export * from \"./examples\"; export * from \"./other-job-file\"; \\nAdding Configuration to package.json\\nInside the package.json file, add the following configuration under the root object:\\n\"trigger.dev\": { \"endpointId\": \"my-app\" } \\nYour package.json file might look something like this:\\n{ \"name\": \"my-app\", \"version\": \"1.0.0\", \"dependencies\": { // ... other dependencies }, \"trigger.dev\": { \"endpointId\": \"my-app\" } } \\nReplace “my-app” with the appropriate identifier you used during the step for creating the Trigger Client.\\nRunning\\nRun your Next.js app\\nRun your Next.js app locally, like you normally would. For example:\\nRun the CLI ‘dev’ command\\nIn a separate terminal window or tab run:', metadata={'source': 'https://trigger.dev/docs/documentation/guides/manual'}),\n",
       " Document(page_content='We currently support five types of Triggers: Manual Invoke, Events, Scheduled, HTTP, and Integration Webhooks. You can use any of these to start a Job Run.', metadata={'source': 'https://trigger.dev/docs/documentation/concepts/triggers/introduction'}),\n",
       " Document(page_content='A Task is a cached unit of work in a Job Run that are logged to the Trigger.dev UI.\\nWhy do you need tasks?\\nTasks are a key building block of how Trigger.dev works, and failing to use them will result in unpredictable results. Tasks allow bits of work inside a Job Run to be cached and the results of those tasks to be reused.\\nThis is very important because for a Job Run to be resumable (e.g. after a serverless function timeout, or because of a call to io.wait()), we need to call the Job.run function multiple times. If we didn’t cache the results of Tasks, then we would be repeating work on each run.\\nclient.defineJob({ id: \"new-user\", name: \"Run when a new user signs up\", version: \"1.0.0\", trigger: eventTrigger({ name: \"new.user\", schema: z.object({ userId: z.string(), }), }), integrations: { resend, }, run: async (payload, io, ctx) => { // This code will run twice. Once when the run first starts, and once after the wait const user = await prisma.user.findUniqueOrThrow({ where: { id: payload.userId }, select: { email: true, name: true }, }); // This code will run once, because the resend integration creates a task with the \"welcome-email\" cacheKey await io.resend.sendEmail(\"welcome-email\", { to: user.email, from: \"jane@acme.inc\", subject: \"Welcome!\", html: welcomeEmail(user.name), }); // This code will run once, because io.wait creates a task with the \"wait\" cacheKey await io.wait(\"wait\", 60 * 60 * 3); // wait for 3 hours // This code will run once, because we\\'re manually creating a task with the \"my-task\" cacheKey const response = await io.runTask( \"my-task\", async () => { return await longRunningCode(payload.userId); }, { name: \"My Task\" } ); return response; }, }); \\nAs well as powering the resumable nature of Trigger.dev, Tasks also provide:\\nRetryable – If a Task fails, it can be retried. You can configure how (or if) a Task is retried. Full details in the io.runTask() SDK reference.\\nLogging – Tasks are logged, so you can see what happened in a Run. Find out more about viewing runs.\\nTask Cache Keys\\nThe first param of all Tasks is a cacheKey. This is a unique identifier for the Task inside that Run. It is used for storing the cached result of a task. It is also used to identify the Task in the Viewing Runs Dashboard.\\nCreating Tasks\\nThere are 3 ways of using tasks in your code:\\nUsing the io.runTask() function\\nUsing one of our Integration packages and calling the io.integration.runTask() wrapper\\nUsing one of our Integration packages and calling a task wrapper function, such as io.slack.postMessage()\\nUsing io.runTask()\\nThe io.runTask() function allows you to run a Task manually. It takes a cacheKey and a function to run. The function will only be run if the Task is not already cached.\\nconst response = await io.runTask(\"my-task\", async (task) => { return await longRunningCode(payload.userId); }); \\nThe callback function is passed a task object, which can be useful for providing an idempotency key to an external service. For example, Stripe:\\nconst stripe = new Stripe(process.env.STRIPE_SECRET_KEY, { apiVersion: \"2020-08-27\", }); await io.runTask(\"create-customer\", async (task) => { await stripe.customers.create( { email: \"eric@trigger.dev\", }, { idempotencyKey: task.idempotencyKey, } ); }); \\nrunTask also takes an optional 3rd argument, which allows you to customize how the Task is displayed and run. For example, you can supply a name and some properties to be displayed in the Viewing Runs Dashboard:\\nconst response = await io.runTask( \"my-task\", async (task) => { return await longRunningCode(payload.userId); }, { name: \"My Task\", properties: [ { label: \"User ID\", value: payload.userId, }, ], icon: \"user\", } ); \\nSee the io.runTask() SDK reference for more information.\\nUsing io.integration.runTask()\\nAll of our Integration packages expose a runTask() function. The main differences between this and io.runTask() are:\\nAdds an additional callback parameter which provides the underlying authenticated integration client\\nAutomatically sets the icon property on the Task.\\nConfigures sensible defaults for retries and error handling.\\nAn example here demonstrates using the GitHub integration’s runTask function to create a project card when a new user signs up:\\nimport { Github } from \"@trigger.dev/github\"; const github = new Github({ id: \"github\", }); client.defineJob({ id: \"create-project-card\", name: \"Create Project Card\", version: \"1.0.0\", trigger: eventTrigger({ name: \"new.user\", }), integrations: { github, }, run: async (payload, io, ctx) => { await io.github.runTask( \"create-card\", async (client, task) => { // client is an authenticated GitHub client (https://github.com/octokit/octokit.js) return client.rest.projects.createCard({ column_id: process.env.GITHUB_PROJECT_COLUMN_ID, note: `New User ${payload.user.name} signed up!`, }); }, { name: \"Create card\" } ); }, }); \\nUsing an Integration Task Wrapper Function\\nOur Integration packages also expose a number of task wrapper functions. These are functions that wrap a common task for that integration. For example, the Slack integration exposes a postMessage() function:\\nimport { Slack } from \"@trigger.dev/slack\"; const slack = new Slack({ id: \"slack\", }); client.defineJob({ id: \"send-welcome-message\", name: \"Send welcome message\", version: \"1.0.0\", trigger: eventTrigger({ name: \"new.user\", }), integrations: { slack, }, run: async (payload, io, ctx) => { await io.slack.postMessage(\"send-message\", { channel: process.env.SLACK_CHANNEL_ID, text: `New user ${payload.user.name} signed up!`, }); }, }); \\nAll task wrapper functions take a cacheKey as the first argument, because they are Tasks under the hood. Think of them as a convenience wrapper around io.runTask().\\nWe strive to document all of the task wrapper functions in our Integration packages. For example, checkout our GitHub integration task docs.\\nSubtasks\\nYou can break up a task into multiple subtasks. This is useful for breaking up a long-running task into smaller chunks, while consolidating the logging into a single task in the dashboard with children.\\nconst response = await io.runTask(\"parent-task\", async (task) => { await io.runTask(\"child-1\", async () => { // do something }); await io.runTask(\"child-2\", async () => { // do something }); }); \\nTask cacheKey’s are automatically scoped to the parent task. So for example, you can reuse a cacheKey inside a parent task and it will not conflict with another top-level task.\\nconst response = await io.runTask(\"parent-task\", async (task) => { await io.runTask(\"child-1\", async () => { // do something }); await io.runTask(\"child-2\", async () => { // do something }); }); // This will not conflict with the child-1 task above const response = await io.runTask(\"child-1\", async (task) => { // do something }); \\nSubtasks allow you to DRY up any repeating task code into a single function. For example, if you have a common task that sends a welcome email, you can extract that into a function:\\nconst sendWelcomeEmail = async (cacheKey: string, io: IO, resend: Resend, userId: string) => { return await io.runTask(cacheKey, async () => { const user = await io.runTask(\"fetch-user\", async () => { return prisma.user.findUniqueOrThrow({ where: { id: userId }, select: { email: true, name: true }, }); }); await io.resend.sendEmail(\"📧\", { to: user.email, from: \"eric@trigger.dev\", subject: \"Welcome!\", html: welcomeEmail(user.name), }); }); }; client.defineJob({ id: \"new-user\", name: \"Run when a new user signs up\", version: \"1.0.0\", trigger: eventTrigger({ name: \"new.user\", schema: z.object({ userId: z.string(), }), }), integrations: { resend, }, run: async (payload, io, ctx) => { await sendWelcomeEmail(\"\\U0001fae1\", io, io.resend, payload.userId); }, }); \\nLimitations\\nA single task has an upper-bound on it’s execution duration, which must be less than the serverless function execution timeout of your deployed platform. For more information see our Limits docs\\nReferences', metadata={'source': 'https://trigger.dev/docs/documentation/concepts/tasks'}),\n",
       " Document(page_content='Send an event and any Jobs that subscribe to that event will get triggered.\\nName and Schemas\\nName\\nEvent triggers have a name. They will only get triggered when an event with that name are sent.\\nSchema\\nEvent triggers take a Zod schema. This is used to validate the data that is sent with the event. If the data does not match the schema, the Job will not run.\\nIt also means that inside your run function the payload will be typed correctly. We use Zod for our schemas – it’s a fantastic library that allows you to define schemas in a very simple way.\\nYou can always start out by using z.any() as your schema, and then later on you can add more strict validation. See our Zod guide for more information.\\nExample\\nclient.defineJob({ id: \"new-user-slack\", name: \"New user slack message\", version: \"0.1.0\", trigger: eventTrigger({ name: \"user.created\", schema: z.object({ name: z.string(), email: z.string(), paidPlan: z.boolean(), }), filter: { //only run the Job if the user is paying paidPlan: [true], }, }), integrations: { slack, }, //this function is run when the custom event is received run: async (payload, io, ctx) => { //send a message to the #new-users Slack channel with user details const response = await io.slack.postMessage(\"send-to-slack\", { channel: \"CUKJXJZ3Z\", text: `New user: ${payload.name} (${payload.email}) signed up. ${ payload.paidPlan ? \"They are paying\" : \"They are on the free plan\" }.`, }); return response.message; }, }); \\nSending events\\nThere are two ways of sending an event that will Trigger a Job.\\n1. From your own code\\nYou can use client.sendEvent() to send an event from your own code. View the SDK reference.\\n//somewhere in your code, can even be on another server await client.sendEvent({ name: \"user.created\", payload: { name: \"John Doe\", email: \"john@doe.com\", paidPlan: true }, }); \\n2. From another Job\\nYou can use io.sendEvent() to send events from inside a Job run, to trigger another. View the SDK reference.\\nclient.defineJob({ id: \"event-1\", name: \"Run when the foo.bar event happens\", version: \"0.0.1\", trigger: eventTrigger({ name: \"foo.bar\", schema: z.object({ url: z.string(), }), }), run: async (payload, io, ctx) => { //send an event using `io` await io.sendEvent(\"send event\", { name: \"user.created\", payload: { name: \"Rick Astley\", email: \"rick.astley@gmail.com\" }, }); }, }); \\nEvent filters\\nThey are declarative pattern-matching rules, modeled after AWS EventBridge patterns.\\nHere’s a more detailed explanation of Event filters\\nGiven the following custom event payload:\\n{ \"uid\": \"jexAgaeJFJsrGfans1pxqm\", \"type\": \"15 Min Meeting\", \"price\": 0, \"title\": \"15 Min Meeting between Eric Allam and John Doe\", \"length\": 15, \"status\": \"ACCEPTED\", \"endTime\": \"2023-01-25T16:00:00Z\", \"bookingId\": 198052, \"organizer\": { \"id\": 32794, \"name\": \"Eric Allam\", \"email\": \"eric@trigger.dev\", \"language\": { \"locale\": \"en\" }, \"timeZone\": \"Europe/London\" } } \\nThe following event filter would match the event:\\n{ \"type\": [\"15 Min Meeting\"], \"status\": [\"ACCEPTED\", \"REJECTED\"], \"organizer\": { \"name\": [\"Eric Allam\"] } } \\nFor an event pattern to match an event, the event must contain all the field names listed in the event pattern. The field names must also appear in the event with the same nesting structure.\\nThe value of each field name in the event pattern must be an array of strings, numbers, or booleans. The event pattern matches the event if the value of the field name in the event is equal to any of the values in the array.\\nEffectively, each array is an OR condition, and the entire event pattern is an AND condition.\\nSo the above event filter will match because status == \"ACCEPTED\", and it would also match if status == \"REJECTED\".', metadata={'source': 'https://trigger.dev/docs/documentation/concepts/triggers/events'}),\n",
       " Document(page_content='This quick start guide will get you setup with Trigger.dev and Supabase inside a new Next.js project.\\nPreparing Supabase & Next.js\\nCreate an account with Supabase or login if you have one already:\\nCreate a new Supabase account\\nLogin to Supabase\\nOnce on the dashboard, click the ‘New project’ button to create a new project in Supabase.\\nFrom inside your new project, click the ‘SQL Editor’ button in the side menu and then click the “User Management Starter” from the Quickstarts menu:\\nOver in a terminal window, create a new Next.js app with Supabase support:\\nnpx create-next-app@latest -e with-supabase my-supabase-app cd my-supabase-app \\nNext, rename the .env.local.example file to .env.local and add your Supabase URL and public key:\\nNEXT_PUBLIC_SUPABASE_URL=your-project-url NEXT_PUBLIC_SUPABASE_ANON_KEY=your-anon-key \\nYou can find your API keys in Supabase by clicking the cog (⚙️) icon in the left menu and selecting ‘API’ from the Project Settings menu:\\nFinally, in the terminal, go ahead and Generate the Typescript types now as we’ll be using them shortly:\\nnpx supabase gen types typescript --project-id <your project id> --schema public --schema auth > supabase-types.ts \\nSetup Trigger.dev\\nNow it’s time to create a Trigger.dev account. You can either:\\nUse the Trigger.dev Cloud.\\nOr self-host the service.\\nOnce you’ve created an account, follow the steps in the app to:\\nComplete your account details.\\nCreate your first Organization and Project.\\nNext, initialize Trigger.dev in the project by running this command:\\nnpx @trigger.dev/cli@latest init \\nEdit the middleware.ts file and add the following code to exclude the Trigger.dev endpoint from the Supabase auth middleware:\\n// Match all routes other than /api/trigger export const config = { matcher: [\"/((?!api/trigger).*)\"], }; \\nAdd the Supabase Integration\\nAdd the Supabase package to your project by running this command:\\nnpm add @trigger.dev/supabase \\nEdit the jobs/examples.ts file and replace with the following code:\\nimport { client } from \"@/trigger\"; import { Database } from \"@/supabase-types\"; import { SupabaseManagement } from \"@trigger.dev/supabase\"; // Use OAuth to authenticate with Supabase Management API const supabaseManagement = new SupabaseManagement({ id: \"supabase-management\", }); // Use the types we generated earlier const db = supabaseManagement.db<Database>(process.env.NEXT_PUBLIC_SUPABASE_URL!); client.defineJob({ id: \"supabase-test-job\", name: \"My Supabase Test Job\", version: \"1.0.0\", trigger: db.onInserted({ schema: \"auth\", table: \"users\", }), run: async (payload, io, ctx) => {}, }); \\nAuthenticate to the Supabase Management API\\nThe Supabase Triggers use the Supabase Management API to register the triggers in your Supabase projects.\\nYou can authenticate using a Personal Access Token or via the new Supabase Management API OAuth implementation, which we are using in this example.\\nLogin to Trigger.dev and navigate to the project “Integrations” page. Select the “Supabase Management” integration and configure it like so:\\nAuthorize access to your Supabase project and then you’ll be ready to run the Job.\\nRun and test the Job\\nNow you are ready to run the Next.js app and test the Job. Run the following command to start the Next.js app:\\nAnd then in a separate terminal, run the following command to start the Trigger.dev agent:\\nHead back to your Supabase Dashboard -> Auth, and create a new user (keep “Auto Confirm User?” checked)\\nThen navigate over to your Trigger.dev project dashboard and you should see the job running.\\nWhat’s next?\\nCheckout our fully-functioning Supabase Onboarding Email example to learn how to build an email drip campaign in 62 lines of code.', metadata={'source': 'https://trigger.dev/docs/documentation/quickstarts/supabase'}),\n",
       " Document(page_content='The Runs list\\nThe main page for a Job is a paginated table of Runs. The newest Runs are at the top.\\nRun page\\nYou can view the details of the Trigger, all the Tasks and the return value of the Run by clicking the elements in the timeline of the left hand side.\\nSome Tasks have subtasks which are collapsed by default but can be opened by clicking them.\\nThe Runs list\\nRun page', metadata={'source': 'https://trigger.dev/docs/documentation/guides/viewing-runs'}),\n",
       " Document(page_content='Initial setup\\nView our Quick start guide to get setup. Or manual setup if you’d prefer.\\nWriting Jobs\\nView our guide for writing Jobs.\\nDeployment\\nView our deployment guide to learn how to deploy your Jobs.\\nServerless function timeouts\\nIf you are deploying your Next.js app to Vercel, you may need to configure a larger max function duration. By default, Vercel has a max function duration of 10 seconds. As outlined in our Limits docs, the max function duration effects the maximum Task duration.\\nSo if you have any tasks that may take longer than 10 seconds to run (or close to 10 seconds), you should increase the max function duration to a higher value (only available to paid Vercel plans).\\nSee the Vercel docs for more information on increasing the maxDuration for your Vercel Serverless Functions.\\nMiddleware\\nNext.js Middleware allows you to run code before a request is completed, and if you are using it currently in your Next.js project (or you add it later), you might need to guard against altering requests to the /api/trigger endpoint, which needs to be exposed to the Trigger.dev installation (either your self-hosted one or the Trigger.dev Cloud).\\nTo make sure you aren’t matching the /api/trigger route in your middleware, check your config.matcher export:\\nimport { NextResponse } from \"next/server\"; import type { NextRequest } from \"next/server\"; // This function can be marked `async` if using `await` inside export function middleware(request: NextRequest) { return NextResponse.redirect(new URL(\"/home\", request.url)); } // See \"Matching Paths\" below to learn more export const config = { matcher: \"/about/:path*\", }; \\nThe above matcher doesn’t match /api/trigger so there won’t be an issue here. But the following middleware.ts file will cause conflicts with Trigger.dev\\nexport function middleware(request: NextRequest) { return NextResponse.redirect(new URL(\"/home\", request.url)); } export const config = { matcher: [\"/((?!.*\\\\\\\\..*|_next).*)\", \"/\", \"/(api|trpc)(.*)\"], }; \\nAs you can see, the last matcher \"/(api|trpc)(.*)\" matches anything starting with api, which matches /api/trigger. You can add a negative-lookahead pattern to the matcher to exclude the /api/trigger route:\\nexport const config = { matcher: [\"/((?!.*\\\\\\\\..*|_next).*)\", \"/\", \"/(api((?!/trigger))|trpc)(.*)\"], }; \\nThe above will match anything starting with /api except for /api/trigger.\\nIf you want to match all routes except for /api/trigger, you can use the following matcher:\\n// Match all routes other than /api/trigger export const config = { matcher: [\"/((?!api/trigger).*)\"], }; \\nFor more information about middleware matchers, head over to the Next.js middleware documentation.\\nClerk.com Auth Middleware\\nIf you are using Clerk.com for your Next.js authentication, make sure to update your middleware file to add /api/trigger to the publicRoutes, like so:\\nimport { authMiddleware } from \"@clerk/nextjs\"; // Adding the /api/trigger route to the public routes so clerk doesn\\'t return a 401 // /api/trigger will handle it\\'s own authentication export default authMiddleware({ publicRoutes: [\"/api/trigger\"], }); export const config = { matcher: [\"/((?!.*\\\\\\\\..*|_next).*)\", \"/\", \"/(api|trpc)(.*)\"], }; \\nKnown issues\\nModule parse failed: Identifier \\'NextResponse\\' has already been declared . This error is caused by version 13.4.4. For further information, please visit', metadata={'source': 'https://trigger.dev/docs/documentation/guides/platforms/nextjs'}),\n",
       " Document(page_content='import { Job } from \"@trigger.dev/sdk\"; import { Github, events } from \"@trigger.dev/github\"; //GitHub integration with API Key (it supports OAuth too) const github = new Github({ id: \"github\", token: process.env.GITHUB_API_KEY!, }); client.defineJob({ id: \"critical-issue-alert\", name: \"Critical Issue Alert\", version: \"0.1.0\", //When a GitHub issue is modified on the triggerdotdev/trigger.dev repo trigger: github.triggers.repo({ event: events.onIssue, owner: \"triggerdotdev\", repo: \"trigger.dev\", }), //include any integrations you want to use integrations: { slack, }, //this function gets executed when the webhook is received run: async (payload, io, ctx) => { await io.logger.info(`Action was ${payload.action}`); }, });', metadata={'source': 'https://trigger.dev/docs/documentation/concepts/triggers/webhooks'}),\n",
       " Document(page_content='Your options when Testing\\nIdentifying test runs\\nTests have a label in the top-right of the Run page and a tick in the Test column of the Runs list.\\nYour options when Testing\\nIdentifying test runs', metadata={'source': 'https://trigger.dev/docs/documentation/guides/testing-jobs'}),\n",
       " Document(page_content='import { Job, intervalTrigger } from \"@trigger.dev/sdk\"; client.defineJob({ id: \"scheduled-job-1\", name: \"Scheduled Job 1\", version: \"0.1.1\", trigger: intervalTrigger({ seconds: 60, }), run: async (payload, io, ctx) => { await io.logger.info(\"Received the scheduled event\", { payload, }); return { foo: \"bar\" }; }, });', metadata={'source': 'https://trigger.dev/docs/documentation/concepts/triggers/scheduled'}),\n",
       " Document(page_content='Installing Required Packages\\nTo begin, install the necessary packages in your NestJS project directory. You can choose one of the following package managers:\\n\\nObtaining the Development API Key\\nTo locate your development API key, login to the Trigger.dev dashboard and select the Project you want to connect to. Then click on the Environments & API Keys tab in the left menu. You can copy your development API Key from the field at the top of this page. (Your development key will start with tr_dev_).\\nAdding Environment Variables\\nCreate a .env file at the root of your project and include your Trigger API key and URL like this:\\nTRIGGER_API_KEY=ENTER_YOUR_DEVELOPMENT_API_KEY_HERE TRIGGER_API_URL=https://api.trigger.dev # this line is only necessary if you are self-hosting Trigger \\nReplace ENTER_YOUR_DEVELOPMENT_API_KEY_HERE with the actual API key obtained from the previous step.\\nAdding TriggerDev Module\\nOpen your app.module.ts, and add the following inside your imports:\\nimport { TriggerDevModule } from \"@trigger.dev/nestjs\"; import { Module } from \"@nestjs/common\"; //you need to load the environment variables from .env, this is one way to do it import \"dotenv/config\"; @Module({ imports: [ TriggerDevModule.register({ id: \"my-app\", apiKey: process.env.TRIGGER_API_KEY, apiUrl: process.env.TRIGGER_API_URL, }), // if you use NestJS Config, you can do like this: // TriggerDevModule.registerAsync({ // useFactory: (configService: ConfigService) => ({ // id: \\'my-app\\', // apiKey: configService.get<string>(\"TRIGGER_API_KEY\"), // apiUrl: configService.get<string>(\"TRIGGER_API_URL\"), // }), // inject: [ConfigService], // }), ], }) export class AppModule { //... } \\nReplace \"my-app\" with an appropriate identifier for your project. The apiKey and apiUrl are obtained from the environment variables you set earlier.\\nBy following these steps, you\\'ll configure the Trigger Client to work with your project.\\nCreating the Example Job\\nWhen you add TriggerDevModule to your project, you will can have access to the TriggerClient instance by using the @InjectTriggerDevClient() decorator in the constructor.\\nNow, let\\'s create an example job to test the integration.\\nCreate a controller named job.controller.ts alongside your app.module.ts\\nInside that controller, add the following code:\\n\\nAdding Configuration to package.json\\nInside the package.json file, add the following configuration under the root object:\\n\"trigger.dev\": { \"endpointId\": \"my-app\" } \\nYour package.json file might look something like this:\\n{ \"name\": \"my-app\", \"version\": \"1.0.0\", \"dependencies\": { // ... other dependencies }, \"trigger.dev\": { \"endpointId\": \"my-app\" } } \\nReplace \"my-app\" with the appropriate identifier you used during the step for creating the Trigger Client.\\nRunning\\nRun your NestJS app\\nRun your NestJS app locally, like you normally would. For example:\\nRun the CLI \\'dev\\' command\\nIn a separate terminal window or tab run:', metadata={'source': 'https://trigger.dev/docs/documentation/guides/manual/nestjs'}),\n",
       " Document(page_content='Concepts\\nJobs, Runs & Tasks\\nTriggers\\nGuides\\nFrameworks\\nManual setup\\nUsing the Dashboard\\nUsing Integrations\\nReact hooks\\nDeployment\\nSelf hosting\\nCommunity & Support\\nOverview\\nAstro\\nHow to get setup and deploy Jobs for your Astro project\\nInitial setup\\nWriting Jobs\\nDeployment', metadata={'source': 'https://trigger.dev/docs/documentation/guides/platforms/astro'}),\n",
       " Document(page_content='Initial setup\\nView our Quick start guide to get setup.\\nWriting Jobs\\nView our guide for writing Jobs.\\nDeployment\\nView our deployment guide to learn how to deploy your Jobs.\\nTroubleshooting\\n\\'TriggerProvider\\' not found\\nWhen running the Remix app, you may see an error like this:\\nimport { TriggerProvider } from \"@trigger.dev/react\"; ^^^^^^^^^^^^^^^ SyntaxError: Named export \\'TriggerProvider\\' not found. The requested module \\'@trigger.dev/react\\' is a CommonJS module, which may not support all module.exports as named exports. CommonJS modules can always be imported via the default export, for example using: import pkg from \\'@trigger.dev/react\\'; const { TriggerProvider } = pkg; \\nTo fix this, edit your remix.config.js file and add the @trigger.dev/react package to your list of serverDependenciesToBundle:\\nexport default { // ... other config serverDependenciesToBundle: [\"@trigger.dev/react\"], }; \\n[ERROR] Node builtin \"buffer\"\\nWhen running the Remix app, you may see an error like this:\\n✘ [ERROR] Node builtin \"buffer\" (imported by \"node_modules/@trigger.dev/core/dist/index.js\") must be polyfilled for the browser. You can enable this polyfill in your Remix config, e.g. `browserNodeBuiltinsPolyfill: { modules: { buffer: true } }` [plugin browser-node-builtins-polyfill-plugin] \\nTo fix this, edit your remix.config.js file and add the browserNodeBuiltinsPolyfill config:\\nexport default { browserNodeBuiltinsPolyfill: { modules: { buffer: \"empty\", }, }, };', metadata={'source': 'https://trigger.dev/docs/documentation/guides/platforms/remix'}),\n",
       " Document(page_content='Installing Required Packages\\nTo begin, install the necessary packages in your Sveltekit project directory. You can choose one of the following package managers:\\n\\nObtaining the Development API Key\\nTo locate your development API key, login to the Trigger.dev dashboard and select the Project you want to connect to. Then click on the Environments & API Keys tab in the left menu. You can copy your development API Key from the field at the top of this page. (Your development key will start with tr_dev_).\\nAdding Environment Variables\\nCreate a .env file at the root of your project and include your Trigger API key and URL like this:\\nTRIGGER_API_KEY=ENTER_YOUR_DEVELOPMENT_API_KEY_HERE TRIGGER_API_URL=https://api.trigger.dev # this is only necessary if you are self-hosting \\nReplace ENTER_YOUR_DEVELOPMENT_API_KEY_HERE with the actual API key obtained from the previous step.\\nSyncing Environment Variable types (TypeScript)\\nYou will have type errors for your environment variables unless you run this command:\\nConfiguring the Trigger Client\\nCreate a file at <root>/src/trigger.ts or <root>/trigger.ts depending on whether you\\'re using the src directory or not. <root> represents the root directory of your project.\\nNext, add the following code to the file which creates and exports a new TriggerClient:\\n// trigger.ts (for TypeScript) or trigger.js (for JavaScript) import { TriggerClient } from \"@trigger.dev/sdk\"; import { TRIGGER_API_KEY, TRIGGER_API_URL } from \"$env/static/private\"; export const client = new TriggerClient({ id: \"my-app\", apiKey: TRIGGER_API_KEY, apiUrl: TRIGGER_API_URL, }); \\nReplace \"my-app\" with an appropriate identifier for your project.\\nCreating the API Route\\nTo establish an API route for interacting with Trigger.dev, follow these steps based on your project\\'s file type and structure\\nCreate a new file named +server.(ts/js) within the src/routes/api/trigger directory, and add the following code:\\nimport { createSvelteRoute } from \"@trigger.dev/sveltekit\"; import { client } from \"../../../trigger\"; //import all jobs import \"../../../jobs\"; // Create the Svelte route handler using the createSvelteRoute function const svelteRoute = createSvelteRoute(client); // Define your API route handler export const POST = svelteRoute.POST; \\nCreating the Example Job\\nCreate a folder named jobs alongside your src directory\\nInside the jobs folder, add two files named example.(ts/js) and index.(ts/js).\\nAdditonal Job Definitions\\nYou can define more job definitions by creating additional files in the jobs folder and exporting them in the src/jobs/index file.\\nFor example, in index.(ts/js), you can export other job files like this:\\n// export all your job files here export * from \"./example\"; export * from \"./other-job-file\"; \\nAdding Configuration to package.json\\nInside the package.json file, add the following configuration under the root object:\\n\"trigger.dev\": { \"endpointId\": \"my-app\" } \\nYour package.json file might look something like this:\\n{ \"name\": \"my-app\", \"version\": \"1.0.0\", \"dependencies\": { // ... other dependencies }, \"trigger.dev\": { \"endpointId\": \"my-app\" } } \\nReplace \"my-app\" with the appropriate identifier you used during the step for creating the Trigger Client.\\nRunning\\nRun your Sveltekit app\\nRun your Sveltekit app locally. You need to use the --host flag to allow the Trigger.dev CLI to connect to your app.\\nFor example:\\nRun the CLI \\'dev\\' command\\nIn a separate terminal window or tab run:', metadata={'source': 'https://trigger.dev/docs/documentation/guides/platforms/sveltekit'}),\n",
       " Document(page_content='Everytime a Job is triggered, a Run is created with a payload of data.\\nAnatomy of a Run\\nA Run is a record of the execution of a Job. It is created from run() function of a Job.\\nclient.defineJob({ id: \"event-1\", name: \"Run when the foo.bar event happens\", version: \"0.0.1\", trigger: eventTrigger({ name: \"foo.bar\", schema: z.object({ url: z.string(), }), }), // 1. Run function with params run: async (payload, io, ctx) => { // 2. Regular code and Tasks // 3. Optionally return data from run execution return { status: \"success\" }; }, }); \\nThe run() function is called with some useful parameters. More on that in a second.\\nInside the run function you can write regular code and use Tasks.\\nYou can return data, which will then be retrievable with getRun or the React hooks.\\nResumability\\nRuns can exceed the maximum timeout on serverless platforms. If a Run exceeds this limit, it will be re-run. When it is re-run, any completed Tasks return their original output and they aren’t re-run. Read more about Resumability.\\nRun function parameters\\npayload\\nThe payload is the data that triggered the Job. It is the same data that was sent to the Trigger that triggered the Job.\\nFor Webhooks the payload is the data from the webhook.\\nFor Events the payload is the data from the event, in the example above that’s { url: \"https://...\" }.\\nFor Scheduled the payload is an object with the timestamp and the last timestamp (previous run).\\nio\\nThe io object gives you access to Integrations and other useful functions. View the full reference for io.\\nA few things you can do with io:\\nUse Integrations.\\nAdd delays (that can be longer than your server timeout).\\nLog messages to the Run log.\\nPerform background fetch requests (that can be longer than your server timeout).\\nSend events to Trigger other Jobs.\\nCreate a Task manually by wrapping code in io.runTask.\\ncontext\\nThe context object gives you access to information about the current Run, Job, Environment, Organization and Event. View the full reference for context.\\nRun Statuses\\nPending\\nThe run has been created but has not started yet. This is the initial status of a run.\\nQueued\\nThe run is waiting to be executed. Runs can be queued because of Run Execution Concurrency Limits\\nWaiting on Connections\\nIf a run depends on a hosted integration, it will be in this status until the integration is ready.\\nExecuting\\nThe run is currently executing. This means that the run function is running.\\nWaiting\\nThe run is waiting, either because of a call to io.wait() or because a task failed and will be retried at some point in the future. Runs in this state don’t count towards concurrency limits.\\nFailed\\nThe run failed. This can happen if the run function throws an error or if a task fails and the run is not configured to retry.\\nCompleted\\nThe run completed successfully. This means that the run function finished executing and all tasks completed successfully.\\nCancelled\\nThe run was cancelled. This can happen if the run is cancelled manually.\\nTimed Out\\nThe run timed out. This can happen if the run exceeds the maximum run duration, or if we receive a serverless function execution timed out response when hitting your endpoint repeatedly with no new task creation.\\nInvalid Payload\\nThe run failed because the payload was invalid.\\nUnresolved Auth\\nThe run failed because the auth data could not be resolved when using a custom Auth Resolver.\\nReferences', metadata={'source': 'https://trigger.dev/docs/documentation/concepts/runs'}),\n",
       " Document(page_content='//Job definition – uses the client client.defineJob({ // 1. Metadata id: \"event-1\", name: \"Run when the foo.bar event happens\", version: \"0.0.1\", // 2. Trigger trigger: eventTrigger({ name: \"foo.bar\", }), // 3. Run function run: async (payload, io, ctx) => { // do something }, });', metadata={'source': 'https://trigger.dev/docs/documentation/concepts/jobs'}),\n",
       " Document(page_content='Sometimes you want to subscribe to changes from an API, and we don’t have an Integration for it yet. That’s when you can use defineHttpEndpoint to receive webhooks, verify them, and create an HTTP Trigger.\\nYou should read the HTTP endpoint documentation to understand how to create an HTTP endpoint.\\nExamples\\nCal.com\\nCal.com has webhooks that closely follow conventions, and so are easy to use.\\n//create an HTTP endpoint const caldotcom = client.defineHttpEndpoint({ id: \"cal.com\", source: \"cal.com\", icon: \"caldotcom\", verify: async (request) => { //this helper function makes verifying most webhooks easy return await verifyRequestSignature({ request, headerName: \"X-Cal-Signature-256\", secret: process.env.CALDOTCOM_SECRET!, algorithm: \"sha256\", }); }, }); client.defineJob({ id: \"http-caldotcom\", name: \"HTTP Cal.com\", version: \"1.0.0\", enabled: true, //create a Trigger from the HTTP endpoint above. The filter is optional. trigger: caldotcom.onRequest({ filter: { body: { triggerEvent: [\"BOOKING_CANCELLED\"] } } }), run: async (request, io, ctx) => { //note that when using HTTP endpoints, the first parameter is the request //you need to get the body, usually it will be json so you do: const body = await request.json(); await io.logger.info(`Body`, body); }, }); \\nWhatsApp\\nWhatsApp is more unusual. When you enter the webhook URL and secret into the Meta developer dashboard, they send a GET request that requires a specific Response.\\nWe can handle this though, by using the respondWith option. This allows us to define a function that will be called immediately when a Request is received. The filter ensures we only receive GET requests with a specific query parameter. Then in the handler we response appropriately if the hub.verify_token matches our secret.\\nconst whatsApp = client.defineHttpEndpoint({ id: \"whatsapp\", source: \"whatsapp.com\", icon: \"whatsapp\", //only needed for strange APIs like WhatsApp which don\\'t setup the webhook until you pass the test respondWith: { //we don\\'t want to trigger Runs for the verification `GET` request skipTriggeringRuns: true, //we only want to respond to `GET` requests with a specific query parameter filter: { method: [\"GET\"], query: { \"hub.mode\": [{ $startsWith: \"sub\" }], }, }, handler: async (request, verify) => { const searchParams = new URL(request.url).searchParams; if (searchParams.get(\"hub.verify_token\") !== process.env.WHATSAPP_WEBHOOK_SECRET) { return new Response(\"Unauthorized\", { status: 401 }); } return new Response(searchParams.get(\"hub.challenge\") ?? \"OK\", { status: 200 }); }, }, verify: async (request) => { //verification of the actual webhooks is easy return await verifyRequestSignature({ request, headerName: \"x-hub-signature-256\", secret: process.env.WHATSAPP_APP_SECRET!, algorithm: \"sha256\", }); }, }); client.defineJob({ id: \"http-whatsapp\", name: \"HTTP WhatsApp\", version: \"1.1.0\", enabled: true, //create a Trigger from the HTTP endpoint above. You can provide an optional filter. trigger: whatsApp.onRequest(), run: async (request, io, ctx) => { //note that when using HTTP endpoints, the first parameter is the request //you need to get the body, usually it will be json so you do: const body = await request.json(); await io.logger.info(`Body`, body); }, });', metadata={'source': 'https://trigger.dev/docs/documentation/concepts/triggers/http'}),\n",
       " Document(page_content='//1. create a DynamicTrigger const dynamicOnIssueOpenedTrigger = client.defineDynamicTrigger({ id: \"github-issue-opened\", event: events.onIssueOpened, source: github.sources.repo, }); //2. create a Job that is attached to the dynamic trigger client.defineJob({ id: \"listen-for-dynamic-trigger\", name: \"Listen for dynamic trigger\", version: \"0.1.1\", trigger: dynamicOnIssueOpenedTrigger, integrations: { slack, }, run: async (payload, io, ctx) => { await io.slack.postMessage(\"Slack 📝\", { text: `New Issue opened on repo: ${payload.issue.html_url}. \\\\n\\\\n${JSON.stringify(ctx)}`, channel: \"C04GWUTDC3W\", }); }, }); //3. Register the DynamicTrigger anywhere in your app async function registerRepo(owner: string, repo: string) { //the first param (key) should be unique await dynamicOnIssueOpenedTrigger.register(`${owner}-${repo}`, { owner, repo, }); } //4. Register inside other Jobs client.defineJob({ id: \"new-repo\", name: \"New repo\", version: \"0.1.1\", //5. when a new repo is created in your org trigger: github.triggers.org({ event: events.onNewRepository, org: \"triggerdotdev\", }), run: async (payload, io, ctx) => { //6. Register the dynamic trigger so you get notified when an issue is opened await dynamicOnIssueOpenedTrigger.register(`${owner}-${repo}`, { owner, repo, }); }, });', metadata={'source': 'https://trigger.dev/docs/documentation/concepts/triggers/dynamic'}),\n",
       " Document(page_content='Integrations allow you to quickly use APIs, including webhooks and Tasks.\\nAuthentication\\nThere are two ways to authenticate Integrations, OAuth and API Keys/Access Tokens.\\nUsing for Jobs & Tasks\\nYou must pass Integrations into your Job to run integration tasks. Passed in Integrations will be available on the io object in the run() function with the same name as the key. For example:\\nclient.defineJob({ //... other options integrations: { slack, gh: github, }, run: async (payload, io, ctx) => { //slack is available on io.slack io.slack.postMessage(...); //github is available on io.gh io.gh.addIssueLabels(...); } }); \\nBoth the postMessage and addIssueLabels functions above are implemented as an “Authenticated Task” that is defined inside the Integration, where the first argument is always the task key and the second argument is the parameters for the task. For example, here is how you would call the postMessage function above:\\nclient.defineJob({ // ... integrations: { slack, }, run: async (payload, io, ctx) => { const response = await io.slack.postMessage(\"✋\", { channel: \"C04GWUTDC3W\", text: \"My first Slack message\", }); }, }); \\nThe above code will create a Task that will use the authentication configured for the specified Slack integration.\\nIf the integration does not support a specific task function that you want to use, you can use the runTask function to get access to the authenticated client.\\nFor example, the Slack integration doesn’t expose an Authenticated Task function for deleting a message, but you can use the chat.delete function provided by the @slack/web-api package directly:\\nclient.defineJob({ // ... integrations: { slack, }, run: async (payload, io, ctx) => { const response = await io.slack.postMessage(\"✋\", { channel: \"C04GWUTDC3W\", text: \"My first Slack message\", }); await io.wait(\"⏰\", 24 * 60 * 60 * 1000); // wait 24 hours await io.slack.runTask(\"delete\", async (slack) => { return slack.chat.delete({ channel: \"C04GWUTDC3W\", ts: response.ts!, }); }); }, }); \\nYou can optionally pass a third parameter to runTask to specify the task parameters:\\nawait io.slack.runTask( \"delete\", async (slack) => { return slack.chat.delete({ channel: \"C04GWUTDC3W\", ts: response.ts!, }); }, { name: \"Delete message\", properties: [{ label: \"Message\", text: response.ts }], } ); \\nView the runTask reference for more information.\\nUsing outside of Jobs\\nIf you want to use integrations that support “local auth” only (e.g. API keys, like Stripe and Supabase) outside of a Job you can use the .native property on the integration to get direct access to the client. For example:\\nimport { Stripe } from \"@trigger.dev/stripe\"; const stripe = new Stripe({ id: \"stripe\", apiKey: process.env.STRIPE_SECRET_KEY!, }); async function createCustomer() { await stripe.native.customers.create({ // ... customer attributes go here }); } \\nUsing for Triggers\\nSome integrations provide triggers that you can use to start a Job. For example, the GitHub integration provides a push trigger that will start a Job when a new commit is pushed to a repository:\\nimport { Github, events } from \"@trigger.dev/github\"; const github = new Github({ id: \"github\", token: process.env.GITHUB_TOKEN!, }); client.defineJob({ id: \"github-integration-on-push\", name: \"GitHub Integration - On Push\", version: \"0.1.0\", trigger: github.triggers.repo({ event: events.onPush, owner: \"triggerdotdev\", repo: \"trigger.dev\", }), run: async (payload, io, ctx) => { // do something on push }, }); \\nBehind the scenes, our @trigger.dev/github integration will create a webhook on your repository that will call our API when a new push event is received. We will then start your Job with the payload from the push event.', metadata={'source': 'https://trigger.dev/docs/documentation/guides/using-integrations'}),\n",
       " Document(page_content=\"We're in the process of building support for the Fastify framework. You can follow along with progress or contribute via this GitHub issue.\", metadata={'source': 'https://trigger.dev/docs/documentation/guides/platforms/fastify'}),\n",
       " Document(page_content='Deployment: Introduction\\nA guide for how to deploy your Jobs\\nDeployment uses Environments & Endpoints to connect your Jobs to the Trigger.dev platform.\\nFirst time deploying to a new Environment\\nThe first time you deploy to a new environment you will need to setup the endpoint for that environment.\\nFirst time setup\\nThis only needs to be done once for each environment\\nSubsequent deployments (i.e. refreshing your Endpoint)\\nIf you add new Jobs, change the id of a Job, or change the Triggers of a Job then you will need to refresh your Endpoint.\\nThere are two ways to do this:\\nFirst time deploying to a new Environment\\nSubsequent deployments (i.e. refreshing your Endpoint)', metadata={'source': 'https://trigger.dev/docs/documentation/guides/deployment'}),\n",
       " Document(page_content='Jobs live inside a Project.\\nFor Jobs to actually run, they need to be connected to the Trigger.dev platform.\\nViewing Jobs\\nThe main page for a Project is the list of connected Jobs. You can see overview details for a each Job here, including details on the Trigger, Integrations and when it was last run.\\nHow to add your first Job\\nWe recommend you do the initial setup of a new Project by following our quick start guide. Once completed you should have an example Job in your dashboard.\\nAdding more Jobs locally\\nWhen you add a new Job to your codebase, you’ll need to connect it to Trigger.dev. This is achieved by refreshing the endpoint in your local Dev environment.\\nGo to the “Environments & API Keys” page for your Project \\nSelect the “DEV” row in the table of endpoints. \\nClick the “Refresh now” button \\nDeploying Jobs\\nSee our deployment guide.\\nDisabling Jobs\\nSometimes there’s a Job that you no longer want to be Triggered. You can achieve this by Disabling it. This will prevent the run() function being called for that Job.\\nArchiving Jobs', metadata={'source': 'https://trigger.dev/docs/documentation/guides/managing-jobs'}),\n",
       " Document(page_content=\"We're in the process of building support for the Fastify framework. You can follow along with progress or contribute via this GitHub issue.\", metadata={'source': 'https://trigger.dev/docs/documentation/guides/manual/fastify'}),\n",
       " Document(page_content='Note this is only relevant to you if you are running the platform locally.\\nHow do I do it?\\nThere are a few ways to do this, but we recommend using ngrok. It’s free and easy to use.\\nStart ngrok\\nInstall ngrok:\\nOpen a new terminal window/tab, you need to leave this running\\nCreate an http tunnel at port 3030:\\nGrab your forwarding address in the ngrok output:\\n2. Use the forwarding URL\\nUse the forwarding URL that ngrok gave you for the LOGIN_ORIGIN and API_ORIGIN environment variables for the docker container.\\nHow do I do it?\\nStart ngrok\\n2. Use the forwarding URL', metadata={'source': 'https://trigger.dev/docs/documentation/guides/tunneling-platform'}),\n",
       " Document(page_content='In the dashboard\\nGo to the “Environments & API Keys” page in your Trigger.dev dashboard \\nSelect your endpoint row in the table of endpoints. \\nThere is a webhook URL in the “Automatic Refresh” section. Copy this URL. \\nHow to use the webhook URL\\nVercel\\nYou can use this webhook URL in your Vercel dashboard, so when a deployment succeeds it automatically refreshes the endpoint.\\nFrom your Vercel team dashboard, select “Settings” \\nGo to the “Webhooks” page \\nSelect the “Deployment Succeeded” event, your Vercel Project, and paste in our webhook URL. Then “Create Webhook”. \\nYou’re done! Whenever a deploy succeeds, Vercel will tell us to refresh your endpoint. \\nGitHub Actions\\nYou can add a step to the GitHub Action that deploys your app.\\n.github/workflows/release.yml\\n- name: 🚀 Refresh Trigger.dev Jobs env: TRIGGER_ENDPOINT_HOOK: ${{ secrets.TRIGGER_ENDPOINT_HOOK }} run: | curl -X POST $TRIGGER_ENDPOINT_HOOK \\nYou will need to setup the TRIGGER_ENDPOINT_HOOK secret in your GitHub repo. Set it to the webhook URL from the endpoint panel.\\nDo an HTTP request yourself\\nAny GET or POST request to the webhook URL will cause a refresh of the Jobs for that endpoint. POST requests don’t need a body. There is no authentication required.\\nIn the dashboard\\nHow to use the webhook URL\\nVercel\\nGitHub Actions\\nDo an HTTP request yourself', metadata={'source': 'https://trigger.dev/docs/documentation/guides/deployment-automatic'}),\n",
       " Document(page_content='Requirements\\ndocker\\ntime\\nSet up your project\\nCreate a project dir and grab the latest .env template\\nmkdir trigger-docker && cd trigger-docker curl -L https://github.com/triggerdotdev/docker/raw/main/.env.example > .env \\nGenerate secrets with openssl rand -hex 16 for the following variables:\\n... MAGIC_LINK_SECRET=15600f1236e568d6c9c400a94e16a4ed SESSION_SECRET=8d92078940c89588fc8b6f5481f2c6e0 ENCRYPTION_KEY=1189c93e399856a2a9a1454496171b2e ... \\nSet NODE_ENV to production:\\n... NODE_ENV=production ... \\nCreate a Supabase DB\\nNo account yet? Click here to complete onboarding\\nCreate a new project\\nMake a note of your password, we’ll need it in a moment\\nWait until your project has finished setting up\\nNavigate to Project Settings -> Database and copy your database connection string\\nPaste it into your .env file, and append ?schema=triggerdotdev\\n... DATABASE_URL=postgresql://postgres:<PASSWORD>@db.<ID>.supabase.co:5432/postgres?schema=triggerdotdev ... \\nThe complete file should look something like this now:\\nLOGIN_ORIGIN=http://localhost:3030 APP_ORIGIN=http://localhost:3030 PORT=3030 REMIX_APP_PORT=3030 MAGIC_LINK_SECRET=15600f1236e568d6c9c400a94e16a4ed SESSION_SECRET=8d92078940c89588fc8b6f5481f2c6e0 ENCRYPTION_KEY=1189c93e399856a2a9a1454496171b2e DATABASE_URL=postgresql://postgres:<PASSWORD>@db.<ID>.supabase.co:5432/postgres?schema=triggerdotdev DIRECT_URL=${DATABASE_URL} NODE_ENV=production RUNTIME_PLATFORM=docker-compose \\nSelf-host Trigger.dev with docker compose\\nCreate a docker compose file in your project dir\\n--- version: \"3.8\" services: triggerdotdev: image: ghcr.io/triggerdotdev/trigger.dev:latest container_name: triggerdotdev restart: unless-stopped env_file: - .env ports: - 3030:3030 \\nStart your docker container\\nWait for the database setup to finish - this might take a while\\nYou should now be able to visit http://localhost:3030 and see this screen:\\nClick “Continue with Email”, enter your email address and hit submit\\nGrab the magic link from your terminal and proceed with account creation\\nIf everything went well, the triggerdotdev and graphile_worker schemas should now be populated. Check your Supabase DB dashboard to be sure:\\nCongratulations, you’re all set up and ready to go with Supabase and Docker! 🚀\\nBonus: Connection pooling\\nWhat is connection pooling?\\nWhen running on a single server, our app can be trusted to manage its own connections - it knows how many are open and can process its internal queue as needed.\\nchanges things. Here, many concurrent app instances are started, each unaware of the others’ connections. This can lead to a huge - potentially fatal - increase in connections straight to your DB.\\nExternal connection pools solve this by putting an additional layer between client and server, which essentially works like a message queue that gets processed as limits allow. The clients are blind to this and can keep sending their requests as before.\\nEnable connection pooling\\nCopy and paste the connection string from earlier to DIRECT_URL\\n... DIRECT_URL=postgresql://postgres:<PASSWORD>@db.<ID>.supabase.co:5432/postgres?schema=triggerdotdev ... \\nNavigate to Project Settings -> Database and copy your connection pool URL\\nPaste it into your .env file next to DATABASE_URL\\nAppend ?schema=triggerdotdev&pgbouncer=true and insert your password\\n... # notice the differing ports and additional query param DATABASE_URL=postgresql://postgres:<PASSWORD>@db.<ID>.supabase.co:6543/postgres?schema=triggerdotdev&pgbouncer=true DIRECT_URL=postgresql://postgres:<PASSWORD>@db.<ID>.supabase.co:5432/postgres?schema=triggerdotdev ... \\nAll done! You can now enjoy the benefits of connection pooling ⚡️\\nNext steps\\nRequirements\\nSet up your project\\nCreate a Supabase DB\\nSelf-host Trigger.dev with docker compose\\nBonus: Connection pooling\\nWhat is connection pooling?\\nEnable connection pooling\\nNext steps', metadata={'source': 'https://trigger.dev/docs/documentation/guides/self-hosting/supabase'}),\n",
       " Document(page_content='You can use this repository as a jumping off point for deploying a self-hosted version of Trigger.dev on Render using the Trigger.dev public docker image located at ghcr.io/triggerdotdev/trigger.dev:latest\\nFork the repository mentioned above and change the app name\\nYou should fork the repository before starting so you can make changes and commit them. For example, in render.yaml file you’ll need to change the name under services to a unique name for your app.\\nCreate a new web service\\nGo to the Render dashboard.\\nClick on “New” in the top right corner.\\nChoose “Web service”\\nClick on “Connect With GitHub” (if you have not already done so). You will need to authorize Render to access your GitHub repositories.\\nOnce connected, you will see a list of your repositories. Choose the repository that contains your project.\\nRender will detect the render.yaml file in your repository and auto fill the fields, add your app name now.\\nScrolldown, click on advanced. Here you will need to add the environment variables\\nGather your secret environment variables\\nThe process for generating and collecting the required environment variables is identical to the one described in the Fly.io guide. Follow the steps there to generate and collect the required environment variables.\\nSet the environment variables\\nThe render.yaml file declares several environment variables that your app needs to run. These include the ENCRYPTION_KEY, MAGIC_LINK_SECRET and SESSION_SECRET, and others. You need to manually set the values of these environment variables in the Render dashboard:\\nClick on “Add Environment Variable” to add a new environment variable.\\nEnter the key and value for each environment variable declared in your render.yaml file. The key should match the name of the environment variable in the render.yaml file. \\nDeploy\\nRender automatically deploys your service when you push to the connected GitHub repository. Alternatively, you can manually trigger a deployment from the Render dashboard by clicking “Deploy” in the “Deploys” tab.\\nInitialize your Next.js project\\nNext, you can easily bootstrap your Next.js project to use your self-hosted instance of Trigger.dev.\\nFirst, cd into your Next.js project, then run the @trigger.dev/cli init command to initialize your Next.js project:\\nnpx @trigger.dev/cli@latest init -t \"https://<your render app name>.onrender.com\" \\nWhen it asks for your development API key, head over to your self-hosted Trigger.dev dashboard and select the initial project you created when signing up, and head to the Environments & API Keys page to copy your dev SERVER API key:\\nStart your dev server\\nRun your Next.js project dev server with npm run dev and then in a new terminal window you will need to run the @trigger.dev/cli dev command to connect to your Trigger.dev instance and allow it to tunnel to your local Next.js server:\\nnpx @trigger.dev/cli@latest dev \\nAt this point you should be able to navigate to your Trigger.dev dashboard and view your registered jobs', metadata={'source': 'https://trigger.dev/docs/documentation/guides/self-hosting/render'}),\n",
       " Document(page_content='You can use this repository as a jumping off point for deploying a self-hosted version of Trigger.dev on Fly.io using the Trigger.dev public docker image located at ghcr.io/triggerdotdev/trigger.dev:latest\\nFork the repository mentioned above and change the app name\\nYou should fork the repository before starting so you can make changes and commit them. For example, you’ll need to change the app property in the fly.toml file to be something other than app = \"trigger-v2-fly-demo\".\\nInstall and configure the fly.io CLI\\nInstall the Fly CLI tool:\\nAuthenticate the CLI:\\nCreate the fly.io app and pg db\\nLaunch the app:\\nFollow the prompts by fly launch and make sure to answer them in the following way:\\n? Would you like to copy its configuration to the new app? Yes ? Choose an app name (leaving blank will default to \\'trigger-v2-fly-demo\\') <enter your preferred app name here or leave blank> ? Would you like to set up a Postgresql database now? Yes ? Select configuration: Development - Single node, 1x shared CPU, 256MB RAM, 1GB disk <- feel free to pick a beefier machine ? Would you like to set up an Upstash Redis database now? No ? Would you like to deploy now? No \\nGather your secret environment variables\\nRequired\\nMAGIC_LINK_SECRET, SESSION_SECRET and ENCRYPTION_KEY\\nAll of these secrets should be 16-byte random strings, which you can easily generate (and copy into your pasteboard) with the following command:\\nopenssl rand -hex 16 | pbcopy \\nLOGIN_ORIGIN and APP_ORIGIN\\nBoth of these secrets should be set to the base URL of your fly application. For example https://trigger-v2-fly-demo.fly.dev\\nDIRECT_URL\\nThis needs to match the value of DATABASE_URL which was printed to your terminal after the creation step above:\\nThe following secret was added to <app name>: DATABASE_URL=postgres://postgres:<PASSWORD>@<fly db name>.flycast:5432/<app name>?sslmode=disable \\nOptional\\nAUTH_GITHUB_CLIENT_ID and AUTH_GITHUB_CLIENT_SECRET\\nIf you plan on logging in with GitHub auth, you’ll need to create a GitHub OAuth app with the following configuration:\\nOnce you register the application you’ll need to click on the “Generate new client secret” button:\\nAnd then you can copy out the AUTH_GITHUB_CLIENT_ID and AUTH_GITHUB_CLIENT_SECRET:\\nRESEND_API_KEY, FROM_EMAIL and REPLY_TO_EMAIL\\nWe use Resend.com for email sending (including the magic-link signup/login system). They have a generous free tier of 100 emails a day that should be sufficient. Signup for Resend.com and enter the environment vars below\\nSet the secrets\\nCall the fly secrets set command to stage the secrets to be used on first deploy:\\nfly secrets set \\\\ ENCRYPTION_KEY=<random string> \\\\ MAGIC_LINK_SECRET=<random string> \\\\ SESSION_SECRET=<random string> \\\\ LOGIN_ORIGIN=\"https://<fly app name>.fly.dev\" \\\\ APP_ORIGIN=\"https://<fly app name>.fly.dev\" \\\\ DIRECT_URL=\"postgres://postgres:<PASSWORD>@<fly db name>.flycast:5432/<app name>?sslmode=disable\" \\\\ FROM_EMAIL=\"Acme Inc. <hello@yourdomain.com>\" \\\\ REPLY_TO_EMAIL=\"Acme Inc. <reply@yourdomain.com>\" \\\\ RESEND_API_KEY=<your API Key> \\\\ AUTH_GITHUB_CLIENT_ID=<your GitHub OAuth Client ID> \\\\ AUTH_GITHUB_CLIENT_SECRET=<your GitHUb OAuth Client Secret> Secrets are staged for the first deployment \\nDeploy\\nNow you can deploy to fly. Here we are setting the machine VM size to use 1 dedicated CPU core with 2 GB of memory, but you can run fly platform vm-sizes to see other options. The below app will cost about $30/month.\\nfly deploy --vm-size performance-1x \\nVisit and create an account\\nOnce deployed, you should be able to open https://<your fly app name>.fly.dev/ in your browser and create an account, either using GitHub or a magic email link.\\nInitialize your Next.js project\\nNext you can easily bootstrap your Next.js project to use your self-hosted instance of Trigger.dev.\\nFirst, cd into your Next.js project, then run the @trigger.dev/cli init command to initialize your Next.js project:\\nnpx @trigger.dev/cli@latest init -t \"https://<your fly app name>.fly.dev\" \\nWhen it asks for your development API key, head over to your self-hosted Trigger.dev dashboard and select the initial project you created when signing up, and head to the Environments & API Keys page to copy your dev API key:\\nStart your dev server\\nRun your Next.js project dev server with npm run dev and then in a new terminal window you will need to run the @trigger.dev/cli dev command to connect to your Trigger.dev instance and allow it to tunnel to your local Next.js server:\\nnpx @trigger.dev/cli@latest dev \\nAt this point you should be able to navigate to your Trigger.dev dashboard and view your registered jobs', metadata={'source': 'https://trigger.dev/docs/documentation/guides/self-hosting/flyio'}),\n",
       " Document(page_content='Prerequisites\\nYou have an understanding of Kubernetes\\nYou have Helm version v3.11.3 or greater installed\\nYou have kubectl installed and connected to your kubernetes cluster\\nBy deploying Trigger.dev on Kubernetes, you can take advantage of its features to ensure that the application is fault-tolerant, highly available, and scalable. To make the installation process easier and more streamlined, we have created a Helm chart that you can use to install Trigger.dev on Kubernetes.\\nHelm is a package manager for Kubernetes that simplifies the installation and management of Kubernetes applications. With our Helm chart, you can easily install Trigger.dev on Kubernetes, configure it to your liking, and scale it up or down as needed.\\nGet our Helm chart\\nAs our charts aren’t published for official use yet, you’ll need a copy of the helm-charts dir:\\ngit clone https://github.com/triggerdotdev/trigger.dev cd trigger.dev/helm-charts \\nAdd Helm values\\nCreate a my-values.yaml file to configure various installation settings, such as the docker image tags and environment variables. To explore all configurable parameters for your values file visit our readme.\\nSet image tags\\nBy default, the application will use the latest tag to retrieve the required Docker images, which may be appropriate for most cases. However, we recommend that you use a specific version of the Docker image to avoid unexpected changes to the application.\\ntrigger: name: trigger replicaCount: 2 image: repository: ghcr.io/triggerdotdev/trigger.dev tag: \"latest\" # <--- image tag pullPolicy: Always \\nConfigure environment variables\\nYou can configure environment variables for trigger in your Helm values file under the property trigger.env. See examples for some of these values here.\\nAt a bare minimum, Trigger.dev requires the following environment variables to be defined:\\nMAGIC_LINK_SECRET\\nSESSION_SECRET\\nENCRYPTION_KEY\\nDIRECT_URL\\nDATABASE_URL\\nWhen the above environment variables are not defined, the Helm chart will automatically generate values for you. It will persist them in a secret which is preserved between upgrades or uninstalls. It is however strongly recommended to define your own values!\\ntrigger: ... env: ENCRYPTION_KEY: \"b1ebe43a6a6e24b2aa8fa0707d3890e3\" MAGIC_LINK_SECRET: \"842727396bcee22da68518f959c5730b\" ... \\nRouting external traffic\\nBy default, Trigger.dev takes all traffic coming to your external load balancer’s IP address and routes them Trigger.dev’s services. Trigger.dev uses Nginx to route external traffic. You can install Nginx along with Trigger by setting ingress.enabled to true in the Helm values file. View all parameters for ingress.\\n... ingress: nginx: enabled: true # <-- if you would like to install nginx along with Trigger.dev \\nDatabase\\nWith this Helm chart, you spin up a PostgreSQL instance powered by Bitnami alongside other Trigger.dev services in your cluster. When persistence is enabled, the data will be stored as a Kubernetes Persistence Volume. View all parameters for postgres.\\npostgresql: enabled: true persistence: enabled: true \\nExample values\\ntrigger: name: trigger replicaCount: 2 image: repository: ghcr.io/triggerdotdev/trigger.dev tag: \"latest\" pullPolicy: Always env: ENCRYPTION_KEY: \"b1ebe43a6a6e24b2aa8fa0707d3890e3\" MAGIC_LINK_SECRET: \"842727396bcee22da68518f959c5730b\" ingress: nginx: enabled: true # <-- if you would like to install nginx along with Trigger.dev \\nInstall the Helm chart\\nThe following command will install our chart into the trigger namespace:\\n# with custom values read from my-values.yaml helm upgrade --install --atomic --namespace trigger --create-namespace trigger . --values my-values.yaml # with default values helm upgrade --install --atomic --namespace trigger --create-namespace trigger . # with inlined values helm upgrade --install --atomic --namespace trigger --create-namespace trigger . --set trigger.replicaCount=3 \\nTo watch the pods coming up, simply run this from another terminal:\\nkubectl --namespace trigger get pods -w \\nAccess Trigger.dev\\nOnce the deployment is ready, you should be able to access Trigger.dev on the IP address exposed via Ingress on your load balancer. If you are not sure what the IP address is run kubectl get ingress to view the external IP address exposing Trigger.dev.\\nLocal access\\nForward a local port to access the webapp directly from your device:\\nkubectl --namespace trigger port-forward svc/trigger 2024:3000 \\nLog in via email at http://localhost:2024 then check your logs for the magic link:\\nkubectl --namespace trigger logs deployments/trigger', metadata={'source': 'https://trigger.dev/docs/documentation/guides/self-hosting/kubernetes'}),\n",
       " Document(page_content='Writing Jobs - Step by Step', metadata={'source': 'https://trigger.dev/docs/documentation/changelog'}),\n",
       " Document(page_content='// Your first job // This Job will be triggered by an event, log a joke to the console, and then wait 5 seconds before logging the punchline client.defineJob({ // This is the unique identifier for your Job, it must be unique across all Jobs in your project id: \"example-job\", name: \"Example Job: a joke with a delay\", version: \"0.0.1\", // This is triggered by an event using eventTrigger. You can also trigger Jobs with webhooks, on schedules, and more: https://trigger.dev/docs/documentation/concepts/triggers/introduction trigger: eventTrigger({ name: \"example.event\", }), run: async (payload, io, ctx) => { // This logs a message to the console await io.logger.info(\"🧪 Example Job: a joke with a delay\"); await io.logger.info(\"How do you comfort a JavaScript bug?\"); // This waits for 5 seconds, the second parameter is the number of seconds to wait, you can add delays of up to a year await io.wait(\"Wait 5 seconds for the punchline...\", 5); await io.logger.info(\"You console it! 🤦\"); await io.logger.info( \"✨ Congratulations, You just ran your first successful Trigger.dev Job! ✨\" ); // To learn how to write much more complex (and probably funnier) Jobs, check out our docs: https://trigger.dev/docs/documentation/guides/create-a-job }, });', metadata={'source': 'https://trigger.dev/docs/documentation/quickstarts/nextjs'}),\n",
       " Document(page_content='// Your first job // This Job will be triggered by an event, log a joke to the console, and then wait 5 seconds before logging the punchline client.defineJob({ // This is the unique identifier for your Job, it must be unique across all Jobs in your project id: \"example-job\", name: \"Example Job: a joke with a delay\", version: \"0.0.1\", // This is triggered by an event using eventTrigger. You can also trigger Jobs with webhooks, on schedules, and more: https://trigger.dev/docs/documentation/concepts/triggers/introduction trigger: eventTrigger({ name: \"example.event\", }), run: async (payload, io, ctx) => { // This logs a message to the console await io.logger.info(\"🧪 Example Job: a joke with a delay\"); await io.logger.info(\"How do you comfort a JavaScript bug?\"); // This waits for 5 seconds, the second parameter is the number of seconds to wait, you can add delays of up to a year await io.wait(\"Wait 5 seconds for the punchline...\", 5); await io.logger.info(\"You console it! 🤦\"); await io.logger.info( \"✨ Congratulations, You just ran your first successful Trigger.dev Job! ✨\" ); // To learn how to write much more complex (and probably funnier) Jobs, check out our docs: https://trigger.dev/docs/documentation/guides/create-a-job }, });', metadata={'source': 'https://trigger.dev/docs/documentation/quickstarts/remix'}),\n",
       " Document(page_content='// Your first job // This Job will be triggered by an event, log a joke to the console, and then wait 5 seconds before logging the punchline client.defineJob({ // This is the unique identifier for your Job, it must be unique across all Jobs in your project id: \"example-job\", name: \"Example Job: a joke with a delay\", version: \"0.0.1\", // This is triggered by an event using eventTrigger. You can also trigger Jobs with webhooks, on schedules, and more: https://trigger.dev/docs/documentation/concepts/triggers/introduction trigger: eventTrigger({ name: \"example.event\", }), run: async (payload, io, ctx) => { // This logs a message to the console await io.logger.info(\"🧪 Example Job: a joke with a delay\"); await io.logger.info(\"How do you comfort a JavaScript bug?\"); // This waits for 5 seconds, the second parameter is the number of seconds to wait, you can add delays of up to a year await io.wait(\"Wait 5 seconds for the punchline...\", 5); await io.logger.info(\"You console it! 🤦\"); await io.logger.info( \"✨ Congratulations, You just ran your first successful Trigger.dev Job! ✨\" ); // To learn how to write much more complex (and probably funnier) Jobs, check out our docs: https://trigger.dev/docs/documentation/guides/create-a-job }, });', metadata={'source': 'https://trigger.dev/docs/documentation/quickstarts/astro'}),\n",
       " Document(page_content='Installing Required Packages\\nStart by installing the necessary packages in your Express.js project directory. You can use npm, pnpm, or yarn as your package manager.\\n\\nObtaining the Development Server API Key\\nTo locate your development Server API key, login to the Trigger.dev dashboard and select the Project you want to connect to. Then click on the Environments & API Keys tab in the left menu. You can copy your development Server API Key from the field at the top of this page. (Your development key will start with tr_dev_).\\nAdding Environment Variables\\nCreate a .env file at the root of your project and include your Trigger API key and URL like this:\\nTRIGGER_API_KEY=ENTER_YOUR_DEVELOPMENT_API_KEY_HERE TRIGGER_API_URL=https://api.trigger.dev # this is only necessary if you are self-hosting \\nReplace ENTER_YOUR_DEVELOPMENT_API_KEY_HERE with the actual API key obtained from the previous step.\\nConfiguring the Trigger Client\\nCreate a file for your Trigger client, in this case we create it at <root>/trigger.(ts/js)\\nimport { TriggerClient } from \"@trigger.dev/sdk\"; export const client = new TriggerClient({ id: \"my-app\", apiKey: process.env.TRIGGER_API_KEY!, apiUrl: process.env.TRIGGER_API_URL!, }); \\nReplace \"my-app\" with an appropriate identifier for your project.\\nAdding the API endpoint\\nThere are a few different options depending on how your Express project is configured.\\nApp middleware\\nEntire app for Trigger.dev (only relevant if it\\'s the only thing your project is for)\\nSelect the appropriate code example from below:\\nCreating the Example Job\\nCreate a Job file. In this case created <root>/jobs/example.(ts/js)\\nimport { eventTrigger } from \"@trigger.dev/sdk\"; import { client } from \"../trigger\"; // your first job client.defineJob({ id: \"example-job\", name: \"Example Job\", version: \"0.0.1\", trigger: eventTrigger({ name: \"example.event\", }), run: async (payload, io, ctx) => { await io.logger.info(\"Hello world!\", { payload }); return { message: \"Hello world!\", }; }, }); \\nAdding Configuration to package.json\\nInside the package.json file, add the following configuration under the root object:\\n\"trigger.dev\": { \"endpointId\": \"my-app\" } \\nReplace \"my-app\" with the appropriate identifier you used in the trigger.js configuration file.\\nRunning\\nRun your Express app\\nRun your Express app locally, like you normally would. For example:\\nRun the CLI \\'dev\\' command\\nIn a separate terminal window or tab run:', metadata={'source': 'https://trigger.dev/docs/documentation/quickstarts/express'}),\n",
       " Document(page_content='Installing Required Packages\\nTo begin, install the necessary packages in your NestJS project directory. You can choose one of the following package managers:\\n\\nObtaining the Development API Key\\nTo locate your development API key, login to the Trigger.dev dashboard and select the Project you want to connect to. Then click on the Environments & API Keys tab in the left menu. You can copy your development API Key from the field at the top of this page. (Your development key will start with tr_dev_).\\nAdding Environment Variables\\nCreate a .env file at the root of your project and include your Trigger API key and URL like this:\\nTRIGGER_API_KEY=ENTER_YOUR_DEVELOPMENT_API_KEY_HERE TRIGGER_API_URL=https://api.trigger.dev # this line is only necessary if you are self-hosting Trigger \\nReplace ENTER_YOUR_DEVELOPMENT_API_KEY_HERE with the actual API key obtained from the previous step.\\nAdding TriggerDev Module\\nOpen your app.module.ts, and add the following inside your imports:\\nimport { TriggerDevModule } from \"@trigger.dev/nestjs\"; import { Module } from \"@nestjs/common\"; //you need to load the environment variables from .env, this is one way to do it import \"dotenv/config\"; @Module({ imports: [ TriggerDevModule.register({ id: \"my-app\", apiKey: process.env.TRIGGER_API_KEY, apiUrl: process.env.TRIGGER_API_URL, }), // if you use NestJS Config, you can do like this: // TriggerDevModule.registerAsync({ // useFactory: (configService: ConfigService) => ({ // id: \\'my-app\\', // apiKey: configService.get<string>(\"TRIGGER_API_KEY\"), // apiUrl: configService.get<string>(\"TRIGGER_API_URL\"), // }), // inject: [ConfigService], // }), ], }) export class AppModule { //... } \\nReplace \"my-app\" with an appropriate identifier for your project. The apiKey and apiUrl are obtained from the environment variables you set earlier.\\nBy following these steps, you\\'ll configure the Trigger Client to work with your project.\\nCreating the Example Job\\nWhen you add TriggerDevModule to your project, you will can have access to the TriggerClient instance by using the @InjectTriggerDevClient() decorator in the constructor.\\nNow, let\\'s create an example job to test the integration.\\nCreate a controller named job.controller.ts alongside your app.module.ts\\nInside that controller, add the following code:\\n\\nAdding Configuration to package.json\\nInside the package.json file, add the following configuration under the root object:\\n\"trigger.dev\": { \"endpointId\": \"my-app\" } \\nYour package.json file might look something like this:\\n{ \"name\": \"my-app\", \"version\": \"1.0.0\", \"dependencies\": { // ... other dependencies }, \"trigger.dev\": { \"endpointId\": \"my-app\" } } \\nReplace \"my-app\" with the appropriate identifier you used during the step for creating the Trigger Client.\\nRunning\\nRun your NestJS app\\nRun your NestJS app locally, like you normally would. For example:\\nRun the CLI \\'dev\\' command\\nIn a separate terminal window or tab run:', metadata={'source': 'https://trigger.dev/docs/documentation/quickstarts/nestjs'}),\n",
       " Document(page_content=\"We're in the process of building support for the Nuxt framework. You can follow along with progress or contribute via this GitHub issue.\", metadata={'source': 'https://trigger.dev/docs/documentation/quickstarts/nuxt'}),\n",
       " Document(page_content=\"We're in the process of building support for the Redwood framework. You can follow along with progress or contribute via this GitHub issue.\", metadata={'source': 'https://trigger.dev/docs/documentation/quickstarts/redwood'}),\n",
       " Document(page_content='Installing Required Packages\\nTo begin, install the necessary packages in your Sveltekit project directory. You can choose one of the following package managers:\\n\\nObtaining the Development API Key\\nTo locate your development API key, login to the Trigger.dev dashboard and select the Project you want to connect to. Then click on the Environments & API Keys tab in the left menu. You can copy your development API Key from the field at the top of this page. (Your development key will start with tr_dev_).\\nAdding Environment Variables\\nCreate a .env file at the root of your project and include your Trigger API key and URL like this:\\nTRIGGER_API_KEY=ENTER_YOUR_DEVELOPMENT_API_KEY_HERE TRIGGER_API_URL=https://api.trigger.dev # this is only necessary if you are self-hosting \\nReplace ENTER_YOUR_DEVELOPMENT_API_KEY_HERE with the actual API key obtained from the previous step.\\nSyncing Environment Variable types (TypeScript)\\nYou will have type errors for your environment variables unless you run this command:\\nConfiguring the Trigger Client\\nCreate a file at <root>/src/trigger.ts or <root>/trigger.ts depending on whether you\\'re using the src directory or not. <root> represents the root directory of your project.\\nNext, add the following code to the file which creates and exports a new TriggerClient:\\n// trigger.ts (for TypeScript) or trigger.js (for JavaScript) import { TriggerClient } from \"@trigger.dev/sdk\"; import { TRIGGER_API_KEY, TRIGGER_API_URL } from \"$env/static/private\"; export const client = new TriggerClient({ id: \"my-app\", apiKey: TRIGGER_API_KEY, apiUrl: TRIGGER_API_URL, }); \\nReplace \"my-app\" with an appropriate identifier for your project.\\nCreating the API Route\\nTo establish an API route for interacting with Trigger.dev, follow these steps based on your project\\'s file type and structure\\nCreate a new file named +server.(ts/js) within the src/routes/api/trigger directory, and add the following code:\\nimport { createSvelteRoute } from \"@trigger.dev/sveltekit\"; import { client } from \"../../../trigger\"; //import all jobs import \"../../../jobs\"; // Create the Svelte route handler using the createSvelteRoute function const svelteRoute = createSvelteRoute(client); // Define your API route handler export const POST = svelteRoute.POST; \\nCreating the Example Job\\nCreate a folder named jobs alongside your src directory\\nInside the jobs folder, add two files named example.(ts/js) and index.(ts/js).\\nAdditonal Job Definitions\\nYou can define more job definitions by creating additional files in the jobs folder and exporting them in the src/jobs/index file.\\nFor example, in index.(ts/js), you can export other job files like this:\\n// export all your job files here export * from \"./example\"; export * from \"./other-job-file\"; \\nAdding Configuration to package.json\\nInside the package.json file, add the following configuration under the root object:\\n\"trigger.dev\": { \"endpointId\": \"my-app\" } \\nYour package.json file might look something like this:\\n{ \"name\": \"my-app\", \"version\": \"1.0.0\", \"dependencies\": { // ... other dependencies }, \"trigger.dev\": { \"endpointId\": \"my-app\" } } \\nReplace \"my-app\" with the appropriate identifier you used during the step for creating the Trigger Client.\\nRunning\\nRun your Sveltekit app\\nRun your Sveltekit app locally. You need to use the --host flag to allow the Trigger.dev CLI to connect to your app.\\nFor example:\\nRun the CLI \\'dev\\' command\\nIn a separate terminal window or tab run:', metadata={'source': 'https://trigger.dev/docs/documentation/quickstarts/sveltekit'}),\n",
       " Document(page_content=\"We're in the process of building support for the Fastify framework. You can follow along with progress or contribute via this GitHub issue.\", metadata={'source': 'https://trigger.dev/docs/documentation/quickstarts/fastify'}),\n",
       " Document(page_content='Installing Required Packages\\nStart by installing the necessary packages in your Express.js project directory. You can use npm, pnpm, or yarn as your package manager.\\n\\nObtaining the Development Server API Key\\nTo locate your development Server API key, login to the Trigger.dev dashboard and select the Project you want to connect to. Then click on the Environments & API Keys tab in the left menu. You can copy your development Server API Key from the field at the top of this page. (Your development key will start with tr_dev_).\\nAdding Environment Variables\\nCreate a .env file at the root of your project and include your Trigger API key and URL like this:\\nTRIGGER_API_KEY=ENTER_YOUR_DEVELOPMENT_API_KEY_HERE TRIGGER_API_URL=https://api.trigger.dev # this is only necessary if you are self-hosting \\nReplace ENTER_YOUR_DEVELOPMENT_API_KEY_HERE with the actual API key obtained from the previous step.\\nConfiguring the Trigger Client\\nCreate a file for your Trigger client, in this case we create it at <root>/trigger.(ts/js)\\nimport { TriggerClient } from \"@trigger.dev/sdk\"; export const client = new TriggerClient({ id: \"my-app\", apiKey: process.env.TRIGGER_API_KEY!, apiUrl: process.env.TRIGGER_API_URL!, }); \\nReplace \"my-app\" with an appropriate identifier for your project.\\nAdding the API endpoint\\nThere are a few different options depending on how your Express project is configured.\\nApp middleware\\nEntire app for Trigger.dev (only relevant if it\\'s the only thing your project is for)\\nSelect the appropriate code example from below:\\nCreating the Example Job\\nCreate a Job file. In this case created <root>/jobs/example.(ts/js)\\nimport { eventTrigger } from \"@trigger.dev/sdk\"; import { client } from \"../trigger\"; // your first job client.defineJob({ id: \"example-job\", name: \"Example Job\", version: \"0.0.1\", trigger: eventTrigger({ name: \"example.event\", }), run: async (payload, io, ctx) => { await io.logger.info(\"Hello world!\", { payload }); return { message: \"Hello world!\", }; }, }); \\nAdding Configuration to package.json\\nInside the package.json file, add the following configuration under the root object:\\n\"trigger.dev\": { \"endpointId\": \"my-app\" } \\nReplace \"my-app\" with the appropriate identifier you used in the trigger.js configuration file.\\nRunning\\nRun your Express app\\nRun your Express app locally, like you normally would. For example:\\nRun the CLI \\'dev\\' command\\nIn a separate terminal window or tab run:', metadata={'source': 'https://trigger.dev/docs/documentation/guides/manual/express'}),\n",
       " Document(page_content='Installing Required Packages\\nTo begin, install the necessary packages in your Remix project directory. You can choose one of the following package managers:\\nObtaining the Development Server API Key\\nTo locate your development Server API key, login to the Trigger.dev dashboard and select the Project you want to connect to. Then click on the Environments & API Keys tab in the left menu. You can copy your development Server API Key from the field at the top of this page. (Your development key will start with tr_dev_).\\nAdding Environment Variables\\nCreate a .env file at the root of your project and include your Trigger API key and URL like this:\\nTRIGGER_API_KEY=ENTER_YOUR_DEVELOPMENT_API_KEY_HERE TRIGGER_API_URL=https://api.trigger.dev # this is only necessary if you are self-hosting \\nReplace ENTER_YOUR_DEVELOPMENT_API_KEY_HERE with the actual API key obtained from the previous step.\\nConfiguring the Trigger Client\\nCreate a file at <root>/app/trigger.ts, where <root> represents the root directory of your project.\\nNext, add the following code to the file which creates and exports a new TriggerClient:\\n// trigger.ts (for TypeScript) or trigger.js (for JavaScript) import { TriggerClient } from \"@trigger.dev/sdk\"; export const client = new TriggerClient({ id: \"my-app\", apiKey: process.env.TRIGGER_API_KEY, apiUrl: process.env.TRIGGER_API_URL, }); \\nReplace \"my-app\" with an appropriate identifier for your project.\\nCreating the API Route\\nTo establish an API route for interacting with Trigger.dev, follow these steps based on your project\\'s file type and structure\\nCreate a new file named api.trigger.(ts/js) within the app/routes/ directory.\\nAdd the following code to app/routes/api.trigger.(ts/js):\\napp/routes/api.trigger.(ts/js)\\nimport { createRemixRoute } from \"@trigger.dev/remix\"; import { client } from \"~/trigger\"; // Remix will automatically strip files with side effects // So you need to *export* your Job definitions like this: export * from \"~/jobs/example.server\"; export const { action } = createRemixRoute(client); \\nCreating the Example Job\\nCreate a folder named jobs inside your app directory\\nInside the jobs folder, add a file named example.server.(ts/js).\\n.server files are guaranteed to be excluded from the client-side build, so you can safely import server-only dependencies here.\\nAdditional Job Definitions\\nYou can define more job definitions by creating additional files in the Jobs folder, exporting from the file and from the api.trigger route.\\nAdding Configuration to package.json\\nInside the package.json file, add the following configuration under the root object:\\n\"trigger.dev\": { \"endpointId\": \"my-app\" } \\nYour package.json file might look something like this:\\n{ \"name\": \"my-app\", \"version\": \"1.0.0\", \"dependencies\": { // ... other dependencies }, \"trigger.dev\": { \"endpointId\": \"my-app\" } } \\nReplace \"my-app\" with the appropriate project identifier you used during the step for creating the Trigger Client.\\nRunning\\nRun your Remix app\\nRun your Remix app locally, like you normally would. For example:\\nRun the CLI \\'dev\\' command\\nIn a separate terminal window or tab run:\\n\\nTroubleshooting\\n\\'TriggerProvider\\' not found\\nWhen running the Remix app, you may see an error like this:\\nimport { TriggerProvider } from \"@trigger.dev/react\"; ^^^^^^^^^^^^^^^ SyntaxError: Named export \\'TriggerProvider\\' not found. The requested module \\'@trigger.dev/react\\' is a CommonJS module, which may not support all module.exports as named exports. CommonJS modules can always be imported via the default export, for example using: import pkg from \\'@trigger.dev/react\\'; const { TriggerProvider } = pkg; \\nTo fix this, edit your remix.config.js file and add the @trigger.dev/react package to your list of serverDependenciesToBundle:\\nexport default { // ... other config serverDependenciesToBundle: [\"@trigger.dev/react\"], }; \\n[ERROR] Node builtin \"buffer\"\\nWhen running the Remix app, you may see an error like this:\\n✘ [ERROR] Node builtin \"buffer\" (imported by \"node_modules/@trigger.dev/core/dist/index.js\") must be polyfilled for the browser. You can enable this polyfill in your Remix config, e.g. `browserNodeBuiltinsPolyfill: { modules: { buffer: true } }` [plugin browser-node-builtins-polyfill-plugin] \\nTo fix this, edit your remix.config.js file and add the browserNodeBuiltinsPolyfill config:\\nexport default { browserNodeBuiltinsPolyfill: { modules: { buffer: \"empty\", }, }, };', metadata={'source': 'https://trigger.dev/docs/documentation/guides/manual/remix'}),\n",
       " Document(page_content=\"We're in the process of building support for the Redwood framework. You can follow along with progress or contribute via this GitHub issue.\", metadata={'source': 'https://trigger.dev/docs/documentation/guides/manual/redwood'}),\n",
       " Document(page_content='Install Required Packages\\nTo begin, install the necessary packages in your Astro project directory. You can choose one of the following package managers:\\nObtaining the Development API Key\\nTo locate your development API key, login to the Trigger.dev dashboard and select the Project you want to connect to. Then click on the Environments & API Keys tab in the left menu. You can copy your development API Key from the field at the top of this page. (Your development key will start with tr_dev_).\\nAdding Environment Variables\\nCreate a .env file at the root of your project and include your Trigger API key and URL like this:\\nTRIGGER_API_KEY=ENTER_YOUR_DEVELOPMENT_API_KEY_HERE TRIGGER_API_URL=https://api.trigger.dev # this is only necessary if you are self-hosting \\nReplace ENTER_YOUR_DEVELOPMENT_API_KEY_HERE with the actual API key obtained from the previous step.\\nConfiguring the Trigger Client\\nCreate a file at <root>/trigger.ts or <root>/src/trigger.ts, depending on if your project uses a src directory, where <root> represents the root directory of your project.\\nNext, add the following code to the file which creates and exports a new TriggerClient:\\nimport { TriggerClient } from \"@trigger.dev/sdk\"; export const client = new TriggerClient({ id: \"my-astro-app\", apiKey: import.meta.env.TRIGGER_API_KEY, apiUrl: import.meta.env.TRIGGER_API_URL, }); \\nReplace \"my-astro-app\" with an appropriate identifier for your project.\\nUpdate the astro.config file to enable SSR (Server Side Rendering)\\nYou need to enable SSR to use API endpoints (which are required by Trigger.dev).\\nimport { defineConfig } from \"astro/config\"; export default defineConfig({ //alternatively you can use \"hybrid\" instead of \"server\" output: \"server\", }); \\nTo learn more about SSR, head over to the Astro docs on SSR.\\nCreating an Example Job\\nCreate a folder named jobs alongside your pages directory\\nInside the jobs folder, add two files named example.ts and index.ts.\\nCreating the API Route\\nTo establish an API route for interacting with Trigger.dev, follow these steps based on your project\\'s file type and structure\\nCreate a new file named trigger.ts within the pages/api/ directory.\\nAdd the following code to trigger.ts:\\nimport { createAstroRoute } from \"@trigger.dev/astro\"; //you may need to update this path to point at your trigger.ts file import { client } from \"../../trigger\"; //import your jobs, this could be different depending on your project structure import \"../../jobs\"; export const prerender = false; export const { POST } = createAstroRoute(client); \\nAdding Configuration to package.json\\nInside the package.json file, add the following configuration under the root object:\\n\"trigger.dev\": { \"endpointId\": \"my-astro-app\" } \\nYour package.json file might look something like this:\\n{ \"name\": \"my-app\", \"version\": \"1.0.0\", \"dependencies\": { // ... other dependencies }, \"trigger.dev\": { \"endpointId\": \"my-astro-app\" } } \\nReplace \"my-astro-app\" with the appropriate identifier you used during the step for creating the TriggerClient.\\nAdditonal Job Definitions\\nYou can define more job definitions by creating additional files in the jobs folder and exporting them in index file.\\nFor example, in index.ts, you can export other job files like this:\\n// import all your job files here export * from \"./examples\"; export * from \"./other-job-file\"; \\nRunning\\nRun your Astro app\\nRun your Astro app locally, like you normally would. For example:\\nRun the CLI \\'dev\\' command\\nIn a separate terminal window or tab run:\\n\\nNext Steps\\nYou should now see your example job in the Trigger.dev dashboard. You can now create additional jobs and use the Trigger.dev dashboard to test them.', metadata={'source': 'https://trigger.dev/docs/documentation/guides/manual/astro'}),\n",
       " Document(page_content=\"We're in the process of building support for the Nuxt framework. You can follow along with progress or contribute via this GitHub issue.\", metadata={'source': 'https://trigger.dev/docs/documentation/guides/manual/nuxt'}),\n",
       " Document(page_content='Installing Required Packages\\nTo begin, install the necessary packages in your Sveltekit project directory. You can choose one of the following package managers:\\n\\nObtaining the Development API Key\\nTo locate your development API key, login to the Trigger.dev dashboard and select the Project you want to connect to. Then click on the Environments & API Keys tab in the left menu. You can copy your development API Key from the field at the top of this page. (Your development key will start with tr_dev_).\\nAdding Environment Variables\\nCreate a .env file at the root of your project and include your Trigger API key and URL like this:\\nTRIGGER_API_KEY=ENTER_YOUR_DEVELOPMENT_API_KEY_HERE TRIGGER_API_URL=https://api.trigger.dev # this is only necessary if you are self-hosting \\nReplace ENTER_YOUR_DEVELOPMENT_API_KEY_HERE with the actual API key obtained from the previous step.\\nSyncing Environment Variable types (TypeScript)\\nYou will have type errors for your environment variables unless you run this command:\\nConfiguring the Trigger Client\\nCreate a file at <root>/src/trigger.ts or <root>/trigger.ts depending on whether you\\'re using the src directory or not. <root> represents the root directory of your project.\\nNext, add the following code to the file which creates and exports a new TriggerClient:\\n// trigger.ts (for TypeScript) or trigger.js (for JavaScript) import { TriggerClient } from \"@trigger.dev/sdk\"; import { TRIGGER_API_KEY, TRIGGER_API_URL } from \"$env/static/private\"; export const client = new TriggerClient({ id: \"my-app\", apiKey: TRIGGER_API_KEY, apiUrl: TRIGGER_API_URL, }); \\nReplace \"my-app\" with an appropriate identifier for your project.\\nCreating the API Route\\nTo establish an API route for interacting with Trigger.dev, follow these steps based on your project\\'s file type and structure\\nCreate a new file named +server.(ts/js) within the src/routes/api/trigger directory, and add the following code:\\nimport { createSvelteRoute } from \"@trigger.dev/sveltekit\"; import { client } from \"../../../trigger\"; //import all jobs import \"../../../jobs\"; // Create the Svelte route handler using the createSvelteRoute function const svelteRoute = createSvelteRoute(client); // Define your API route handler export const POST = svelteRoute.POST; \\nCreating the Example Job\\nCreate a folder named jobs alongside your src directory\\nInside the jobs folder, add two files named example.(ts/js) and index.(ts/js).\\nAdditonal Job Definitions\\nYou can define more job definitions by creating additional files in the jobs folder and exporting them in the src/jobs/index file.\\nFor example, in index.(ts/js), you can export other job files like this:\\n// export all your job files here export * from \"./example\"; export * from \"./other-job-file\"; \\nAdding Configuration to package.json\\nInside the package.json file, add the following configuration under the root object:\\n\"trigger.dev\": { \"endpointId\": \"my-app\" } \\nYour package.json file might look something like this:\\n{ \"name\": \"my-app\", \"version\": \"1.0.0\", \"dependencies\": { // ... other dependencies }, \"trigger.dev\": { \"endpointId\": \"my-app\" } } \\nReplace \"my-app\" with the appropriate identifier you used during the step for creating the Trigger Client.\\nRunning\\nRun your Sveltekit app\\nRun your Sveltekit app locally. You need to use the --host flag to allow the Trigger.dev CLI to connect to your app.\\nFor example:\\nRun the CLI \\'dev\\' command\\nIn a separate terminal window or tab run:', metadata={'source': 'https://trigger.dev/docs/documentation/guides/manual/sveltekit'}),\n",
       " Document(page_content='Sometimes it makes sense to be able to invoke a Job manually, without having to specify an event, especially for cases where you want to get notified when the invoked Job Run is complete.\\nTo specify that a job is manually invokable, you can use the invokeTrigger() function when defining a job:\\nimport { invokeTrigger } from \"@trigger.dev/sdk\"; import { client } from \"@/trigger\"; export const exampleJob = client.defineJob({ id: \"example-job\", name: \"Example job\", version: \"1.0.1\", trigger: invokeTrigger(), run: async (payload, io, ctx) => { // do something with the payload }, }); \\nAnd then you can invoke the job using the Job.invoke() method:\\nimport { exampleJob } from \"./exampleJob\"; const jobRun = await exampleJob.invoke({ foo: \"bar\" }); \\nPayload Schema\\nYou can specify the type of the expected payload by passing a Zod schema to invokeTrigger():\\nimport { invokeTrigger } from \"@trigger.dev/sdk\"; import { client } from \"@/trigger\"; export const exampleJob = client.defineJob({ id: \"example-job\", name: \"Example job\", version: \"1.0.1\", trigger: invokeTrigger({ //the expected payload shape schema: z.object({ userId: z.string(), tier: z.enum([\"free\", \"pro\"]), }), }), run: async (payload, io, ctx) => { // payload is typed as { userId: string, tier: \"free\" | \"pro\" } }, }); \\nNow when you invoke the job, you will get a type error if the payload does not match the schema:\\nimport { exampleJob } from \"./exampleJob\"; // this will throw a type error because the payload does not match the schema const jobRun = await exampleJob.invoke({ foo: \"bar\" }); // this will work const jobRun = await exampleJob.invoke({ userId: \"123\", tier: \"free\" }); \\nInvoking a Job\\nAs you can see in the example above, invoking a job is as simple as calling the Job.invoke() method. This method returns a JobRun object that you can use to track the progress of the job run, especially in conjunction with our React hooks, like useRunDetails()\\nimport { exampleJob } from \"./exampleJob\"; // Somewhere in your backend code const jobRun = await exampleJob.invoke({ userId: \"123\", tier: \"free\" }); // Somewhere in your frontend code const { data: jobRunDetails } = useRunDetails(jobRun.id); \\nDeduplicate Invocations\\nYou can pass an optional idempotencyKey to the invoke() method to deduplicate invocations. This is useful when you want to make sure that a job is only invoked once for a given payload.\\nimport { exampleJob } from \"./exampleJob\"; // Somewhere in your backend code const jobRun = await exampleJob.invoke( { userId: \"123\", tier: \"free\" }, { idempotencyKey: \"abc123\" } ); // This will not invoke the job again, but return the existing job run const jobRun2 = await exampleJob.invoke( { userId: \"123\", tier: \"free\" }, { idempotencyKey: \"abc123\" } ); \\nCallback URL\\nYou can also pass an optional callbackUrl to the invoke() method to get notified when the job run is complete, either successfully or with an error.\\nimport { exampleJob } from \"./exampleJob\"; // Somewhere in your backend code const jobRun = await exampleJob.invoke( { userId: \"123\", tier: \"free\" }, { callbackUrl: `${process.env.VERCEL_URL}/api/trigger/runs` } ); \\nWhen the run is complete, we will issue a POST request to the URL with the RunNotification payload.\\nVerifying the callback\\nYou should make sure to verify the payload in your callback route to make sure that the request is coming from Trigger. You can do this by checking the X-Trigger-Signature-256 header, which contains a HMAC signature of the payload using your secret API Key.\\nimport { crypto } from \"node:crypto\" app.post(\"/api/trigger/runs\", (req, res) => { // Create digest with payload + hmac secret const hashPayload = req.rawBody; const hmac = crypto.createHmac(\"sha256\", process.env.TRIGGER_API_KEY); / const digest = Buffer.from( signatureAlgorithm + \"=\" + hmac.update(hashPayload).digest(\"hex\"), \"utf8\" ); // Get hash sent by the provider const providerSig = Buffer.from(req.get(\"X-Trigger-Signature-256\") || \"\", \"utf8\"); // Compare digest signature with signature sent by provider if (providerSig.length !== digest.length || !crypto.timingSafeEqual(digest, providerSig)) { res.status(401).send(\"Unauthorized\"); } else { // Webhook Authenticated // process and respond... res.json({ message: \"Success\" }); } }); \\nAdditional context\\nYou can pass an optional context object to the invoke() method, which will be available in the job run context. This is useful for passing additional information to the job run that doesn’t make sense in the payload.\\nimport { exampleJob } from \"./exampleJob\"; // Somewhere in your backend code const jobRun = await exampleJob.invoke( { userId: \"123\", tier: \"free\" }, { context: { traceId: \"trace_123\", correlationId: \"def456\", }, } ); \\nAnd then you can access the context in the job run:\\nexport const exampleJob = client.defineJob({ id: \"example-job\", name: \"Example job\", version: \"1.0.1\", trigger: invokeTrigger({ //the expected payload shape schema: z.object({ userId: z.string(), tier: z.enum([\"free\", \"pro\"]), }), }), run: async (payload, io, ctx) => { console.log(ctx.event.context); // { traceId: \"trace_123\", correlationId: \"def456\" } }, }); \\nInvoking a job from a job\\nYou can also invoke a job from another job:\\nimport { exampleJob } from \"./exampleJob\"; client.defineJob({ id: \"example-job2\", name: \"Example job 2\", version: \"1.0.1\", trigger: intervalTrigger({ seconds: 60, }), run: async (payload, io, ctx) => { const jobRun = await exampleJob.invoke(\"⚡\", { userId: \"123\", tier: \"free\" }); }, }); \\nNotice how the invoke() method takes a string as the first argument. This is because under the hood invoke() is automatically creating a Task and the \"⚡\" string is the cacheKey for the created task. You can easily see the run created via the Run Dashboard:\\nWait for completion\\nYou can also invoke a job and wait for it to complete before continuing execution of the current job:\\nimport { exampleJob } from \"./exampleJob\"; client.defineJob({ id: \"example-job2\", name: \"Example job 2\", version: \"1.0.1\", trigger: intervalTrigger({ seconds: 60, }), run: async (payload, io, ctx) => { const jobRun = await exampleJob.invokeAndWaitForCompletion(\"⚡\", { userId: \"123\", tier: \"free\", }); if (jobRun.ok) { // The job run finished successfully console.log(jobRun.output); } else { // The job run finished with an error console.log(`The job failed with status ${jobRun.status}`, jobRun.error); } }, }); \\nBy default, invokeAndWaitForCompletion() will wait for the job run to complete for up to 60 minutes. You can change this by passing a timeoutInSeconds option:\\nimport { exampleJob } from \"./exampleJob\"; client.defineJob({ id: \"example-job2\", name: \"Example job 2\", version: \"1.0.1\", trigger: intervalTrigger({ seconds: 60, }), run: async (payload, io, ctx) => { const jobRun = await exampleJob.invokeAndWaitForCompletion( \"⚡\", { userId: \"123\", tier: \"free\", }, 86_400 // 24 hours ); if (jobRun.ok) { // The job run finished successfully console.log(jobRun.output); } else { // The job run finished with an error console.log(`The job failed with status ${jobRun.status}`, jobRun.error); } }, }); \\nThe return value of invokeAndWaitForCompletion is a RunNotification object.\\nBatch invoke and wait for completion\\nYou can also batch invoke jobs and wait for them all to complete before continuing execution of the current job:\\nimport { exampleJob } from \"./exampleJob\"; client.defineJob({ id: \"example-job2\", name: \"Example job 2\", version: \"1.0.1\", trigger: intervalTrigger({ seconds: 60, }), run: async (payload, io, ctx) => { const runs = await exampleJob.batchInvokeAndWaitForCompletion(\"⚡\", [ { payload: { userId: \"123\", tier: \"free\", }, timeoutInSeconds: 86_400, // 24 hours }, { payload: { userId: \"abc\", tier: \"paid\", }, timeoutInSeconds: 86_400, // 24 hours }, ]); // runs is an array of RunNotification objects }, }); \\nYou can batch up to 25 invocations at once, and we will run them in parallel and wait for all of them to complete before continuing execution of the current job.\\nYou can in an optional options object to each invocation, if you want to pass context and accountId to each invocation:\\nimport { exampleJob } from \"./exampleJob\"; client.defineJob({ id: \"example-job2\", name: \"Example job 2\", version: \"1.0.1\", trigger: intervalTrigger({ seconds: 60, }), run: async (payload, io, ctx) => { const runs = await exampleJob.batchInvokeAndWaitForCompletion(\"⚡\", [ { payload: { userId: \"123\", tier: \"free\", }, timeoutInSeconds: 86_400, // 24 hours options: { context: { traceId: \"trace_123\", correlationId: \"def456\", }, accountId: \"abc123\", }, }, { payload: { userId: \"abc\", tier: \"paid\", }, timeoutInSeconds: 86_400, // 24 hours options: { context: { traceId: \"trace_abc\", correlationId: \"defabc\", }, accountId: \"abc123\", }, }, ]); // runs is an array of RunNotification objects }, });', metadata={'source': 'https://trigger.dev/docs/documentation/concepts/triggers/invoke'}),\n",
       " Document(page_content='On the Run page when the Run is in a success or failed state, you can rerun by clicking the Rerun button.\\nThere are two possible rerun options.\\n1. Run again\\nThis option is always available (when a run has finished).\\nIt creates a brand new run with the same inputs as the original – the same Trigger payload, settings, connections etc.\\n2. Retry job run\\nThis option is only available when the run has failed.\\nIt will continue the existing Run by retrying the Task that failed the Job.\\n1. Run again\\n2. Retry job run', metadata={'source': 'https://trigger.dev/docs/documentation/guides/rerunning'}),\n",
       " Document(page_content='Installing Required Packages\\nStart by installing the necessary packages in your Express.js project directory. You can use npm, pnpm, or yarn as your package manager.\\n\\nObtaining the Development Server API Key\\nTo locate your development Server API key, login to the Trigger.dev dashboard and select the Project you want to connect to. Then click on the Environments & API Keys tab in the left menu. You can copy your development Server API Key from the field at the top of this page. (Your development key will start with tr_dev_).\\nAdding Environment Variables\\nCreate a .env file at the root of your project and include your Trigger API key and URL like this:\\nTRIGGER_API_KEY=ENTER_YOUR_DEVELOPMENT_API_KEY_HERE TRIGGER_API_URL=https://api.trigger.dev # this is only necessary if you are self-hosting \\nReplace ENTER_YOUR_DEVELOPMENT_API_KEY_HERE with the actual API key obtained from the previous step.\\nConfiguring the Trigger Client\\nCreate a file for your Trigger client, in this case we create it at <root>/trigger.(ts/js)\\nimport { TriggerClient } from \"@trigger.dev/sdk\"; export const client = new TriggerClient({ id: \"my-app\", apiKey: process.env.TRIGGER_API_KEY!, apiUrl: process.env.TRIGGER_API_URL!, }); \\nReplace \"my-app\" with an appropriate identifier for your project.\\nAdding the API endpoint\\nThere are a few different options depending on how your Express project is configured.\\nApp middleware\\nEntire app for Trigger.dev (only relevant if it\\'s the only thing your project is for)\\nSelect the appropriate code example from below:\\nCreating the Example Job\\nCreate a Job file. In this case created <root>/jobs/example.(ts/js)\\nimport { eventTrigger } from \"@trigger.dev/sdk\"; import { client } from \"../trigger\"; // your first job client.defineJob({ id: \"example-job\", name: \"Example Job\", version: \"0.0.1\", trigger: eventTrigger({ name: \"example.event\", }), run: async (payload, io, ctx) => { await io.logger.info(\"Hello world!\", { payload }); return { message: \"Hello world!\", }; }, }); \\nAdding Configuration to package.json\\nInside the package.json file, add the following configuration under the root object:\\n\"trigger.dev\": { \"endpointId\": \"my-app\" } \\nReplace \"my-app\" with the appropriate identifier you used in the trigger.js configuration file.\\nRunning\\nRun your Express app\\nRun your Express app locally, like you normally would. For example:\\nRun the CLI \\'dev\\' command\\nIn a separate terminal window or tab run:', metadata={'source': 'https://trigger.dev/docs/documentation/guides/platforms/express'}),\n",
       " Document(page_content='In the previous guides we’ve covered how you can use our integrations with API Keys or OAuth, but in both cases those authentication credentials belong to you the developer.\\nIf you want to use our integrations using auth credentials of your users you can use an Auth Resolver which allows you to implement your own custom auth resolving using a third-party service like Clerk or Nango\\nIn this guide we’ll demonstrate how to use Clerk.com’s Social Connections to allow you to make requests with your user’s Slack credentials and the official Trigger.dev Slack integration\\n1. Install the Slack integration package\\n2. Create a Slack integration\\nimport { Slack } from \"@trigger.dev/slack\"; const byoSlack = new Slack({ id: \"byo-slack\", }); \\n3. Define an Auth Resolver\\nUsing your TriggerClient instance, define a new Auth Resolver for the slack integration:\\nimport { Slack } from \"@trigger.dev/slack\"; // Import your TriggerClient instance. This is merely an example of how you could do it import { client } from \"./trigger\"; const byoSlack = new Slack({ id: \"byo-slack\", }); client.defineAuthResolver(byoSlack, async (ctx) => { // this is where we\\'ll use the clerk backend SDK }); \\n4. Define a job\\nBefore we finish the Slack Auth Resolver, let’s create an example job that uses the Slack integration:\\nimport { z } from \"zod\"; client.defineJob({ id: \"post-a-message\", name: \"Post a Slack Message\", version: \"1.0.0\", trigger: eventTrigger({ name: \"post.message\", schema: z.object({ text: z.string(), channel: z.string(), }), }), integrations: { slack: byoSlack, }, run: async (payload, io, ctx) => { await io.slack.postMessage(\"💬\", { channel: payload.channel, text: payload.text, }); }, }); \\nAs you can see above, we’re passing the byoSlack integration into the Job and using it by calling io.slack.postMessage.\\n5. Install the Clerk backend SDK\\n6. Import and initialize the Clerk SDK\\nimport { Clerk } from \"@clerk/backend\"; // Clerk is not a class so the omission of `new Clerk` here is on purpose const clerk = Clerk({ apiKey: process.env.CLERK_API_KEY }); \\n7. Implement the Auth Resolver\\nNow we’ll implement the Auth Resolver to provide authentication credentials saved in Clerk.com for Job runs, depending on the account ID of the run.\\nclient.defineAuthResolver(slack, async (ctx) => { if (!ctx.account?.id) { return; } const tokens = await clerk.users.getUserOauthAccessToken(ctx.account.id, \"oauth_slack\"); if (tokens.length === 0) { throw new Error(`Could not find Slack auth for account ${ctx.account.id}`); } return { type: \"oauth\", token: tokens[0].token, }; }); \\nThe first parameter to the Auth Resolver callback is the run context (reference docs), which optionally contains an associated account (more on this below).\\nBonus: Multiple Slack integration clients\\nIf you want to also use Slack with your own authentication credentials, you can always create another slack integration with a different id.\\nconst ourSlack = new Slack({ id: \"our-slack\" }); client.defineJob({ id: \"post-a-message\", name: \"Post a Slack Message\", version: \"1.0.0\", trigger: eventTrigger({ name: \"post.message\", schema: z.object({ text: z.string(), channel: z.string(), }), }), integrations: { byoSlack: byoSlack, ourSlack: ourSlack, }, run: async (payload, io, ctx) => { await io.byoSlack.postMessage(\"💬\", { channel: payload.channel, text: payload.text, }); await io.ourSlack.postMessage(\"📢\", { channel: \"C01234567\", text: `We just sent the following message to ${ctx.account?.id}: ${payload.text}`, }); }, }); \\nHow to Trigger Job runs with an Account ID\\nNow that we have a working Clerk.com Auth Resolver for Slack we’re ready to start triggering jobs with an associated account ID. The way you do this is different depending on the Trigger type.\\nEvent Triggers\\nJobs that have Event Triggers can be run with an associated account by providing an accountId when calling sendEvent:\\n// This is an instance of `TriggerClient` await client.sendEvent( { name: \"post.created\", payload: { id: \"post_123\" }, }, { accountId: \"user_123\", } ); \\nThe accountId value is completely arbitrary and doesn’t map to anything inside Trigger.dev, but generally it should be a unique ID that can be used to lookup Auth credentials in your Auth Resolvers.\\nYou can also send events with an associated account ID from the run of another job:\\nclient.defineJob({ id: \"event-1\", name: \"Run when the foo.bar event happens\", version: \"0.0.1\", trigger: eventTrigger({ name: \"foo.bar\", }), run: async (payload, io, ctx) => { //send an event using `io` await io.sendEvent( \"🎫\", { name: \"post.created\", payload: { id: \"post_123\" }, }, { accountId: \"user_123\", } ); }, }); \\nWhen a run is triggered with an associated account ID, you’ll see the account ID in the run dashboard:\\nScheduled Triggers\\nRunning a job with an associated account ID that is triggered by a Scheduled Trigger works a bit differently than Event Triggers as you’ll need to convert your normal intervalTrigger or cronTrigger into using a Dynamic Schedule and then registering schedules with an associated account ID.\\n1. Convert a job to using a Dynamic Schedule\\nFirst let’s convert the following job from an intervalTrigger to a Dynamic Schedule:\\n// Before client.defineJob({ id: \"scheduled-job\", name: \"Scheduled Job\", version: \"1.0.0\", trigger: intervalTrigger({ seconds: 60, }), run: async (payload, io, ctx) => { await io.logger.info(\"This runs every 60 seconds\"); }, }); // After export const dynamicInterval = client.defineDynamicSchedule({ id: \"my-schedule\" }); client.defineJob({ id: \"scheduled-job\", name: \"Scheduled Job\", version: \"1.0.0\", trigger: dynamicInterval, run: async (payload, io, ctx) => { await io.logger.info(\"This runs dynamic schedules\"); }, }); \\nAs you can see above, we’ve dropped the specific interval when defining the trigger as that will now be specific when registering schedules.\\n2. Register a schedule\\nYou can now use the dynamicInterval instance to register a schedule, which will trigger the scheduled-job:\\nimport { dynamicInterval } from \"./dynamicSchedule\"; // Somewhere in your backend await dynamicInterval.register(\"schedule_123\", { type: \"interval\", options: { seconds: 60 }, accountId: \"user_123\", // associate runs triggered by this schedule with user_123 }); \\nAs you can see above, we’ve associated this registered schedule with an accountId, so any runs triggered by this schedule will be associated with \"user_123\"\\nThe first parameter above \"schedule_123\" is the Schedule ID and can be used to unregister the schedule at a later point:\\nimport { dynamicInterval } from \"./dynamicSchedule\"; // Somewhere in your backend await dynamicInterval.unregister(\"schedule_123\"); \\nYou can also use register/unregister inside another job run and it will automatically create a Task:\\nimport { dynamicInterval } from \"./dynamicSchedule\"; client.defineJob({ id: \"event-1\", name: \"Run when the foo.bar event happens\", version: \"0.0.1\", trigger: eventTrigger({ name: \"foo.bar\", }), run: async (payload, io, ctx) => { await dynamicInterval.register(\"schedule_123\", { type: \"interval\", options: { seconds: 60 }, accountId: \"user_123\", // associate runs triggered by this schedule with user_123 }); }, }); \\nWill produce the following run dashboard:\\nWebhook Triggers\\nRunning a job with an associated account ID that is triggered by a Webhook Trigger requires converting to the use of a Dynamic Trigger\\nDynamic Trigger’s work very similarly to Dynamic Schedules, but instead of registering schedules, you register triggers:\\nTesting jobs with Account ID\\nIf a job uses any integrations with an Auth Resolver that requires an account ID, you’ll need to provide an account ID when testing the job:\\nAuth Resolver reference\\nThe Auth Resolver callback has the following signature:\\ntype TriggerAuthResolver = ( ctx: TriggerContext, integration: TriggerIntegration ) => Promise<AuthResolverResult | undefined>; type AuthResolverResult = { type: \"apiKey\" | \"oauth\"; token: string; additionalFields?: Record<string, string>; }; \\nThe ctx parameter is the TriggerContext for the run and the integration parameter is the TriggerIntegration instance that the Auth Resolver is being called for. You can use the integration parameter to check the id of the integration to determine which integration the Auth Resolver is being called for:\\nclient.defineAuthResolver(slack, async (ctx, integration) => { if (integration.id === \"byo-slack\") { // do something } }); \\nYou can also return additionalFields in the Auth Resolver result which will be passed to the integration when making requests. This is useful if you need to provide additional fields to the integration that are not part of the standard integration options.\\nclient.defineAuthResolver(shopify, async (ctx, integration) => { return { type: \"apiKey\", token: \"my-api-key\", additionalFields: { shop: \"my-shop-name\", }, }; });', metadata={'source': 'https://trigger.dev/docs/documentation/guides/using-integrations-byo-auth'}),\n",
       " Document(page_content=\"We're in the process of building support for the Redwood framework. You can follow along with progress or contribute via this GitHub issue.\", metadata={'source': 'https://trigger.dev/docs/documentation/guides/platforms/redwood'}),\n",
       " Document(page_content=\"We're in the process of building support for the Nuxt framework. You can follow along with progress or contribute via this GitHub issue.\", metadata={'source': 'https://trigger.dev/docs/documentation/guides/platforms/nuxt'}),\n",
       " Document(page_content='client.defineJob({ id: \"assign-on-issue-opened\", name: \"Assign on Issue Opened\", version: \"0.1.0\", //1. If you want to use the Integration in the run function, add it here integrations: { github }, //2. If the Integration supports webhooks, you can use them here trigger: github.triggers.repo({ event: events.onIssueOpened, owner: \"triggerdotdev\", repo: \"trigger.dev\", }), run: async (payload, io, ctx) => { //3. Because we add `github` to Integrations, it comes through to `io.github` here const assignee = await io.github.addIssueAssignees(\"add assignee\", { owner: payload.repository.owner.login, repo: payload.repository.name, issueNumber: payload.issue.number, assignees: [\"matt-aitken\"], }); return assignee; }, });', metadata={'source': 'https://trigger.dev/docs/documentation/guides/using-integrations-apikeys'}),\n",
       " Document(page_content='You can display the live progress of a Job Run to your users, including the status of individual tasks and the final output of the Run.\\nSetting up your project for hooks\\nThis guide assumes that your project is already setup and you have a Job running. If not, you should follow the quick start guide first.\\nTwo ways to report Run progress\\nAutomatic progress – without writing additional code in your Job you can get updates on the overall run status and individual tasks inside the run.\\nExplicit status – add code to your Job that reports the status of the things you’re doing. This gives you full flexibility for displaying progress in your UI.', metadata={'source': 'https://trigger.dev/docs/documentation/guides/react-hooks'}),\n",
       " Document(page_content='client.defineJob({ id: \"star-slack-notification\", name: \"New Star Slack Notification\", version: \"0.1.0\", //1. If you want to use the Integration in the run function, add it here integrations: { slack }, //2. If the Integration supports webhooks, you can use them here trigger: github.triggers.repo({ event: events.onNewStar, owner: \"triggerdotdev\", repo: \"empty\", }), run: async (payload, io, ctx) => { //3. Because we add `slack` to Integrations, it comes through to `io.slack` here const response = await io.slack.postMessage(\"Slack star\", { text: `${payload.sender.login} starred ${payload.repository.full_name}.\\\\nTotal: ${payload.repository.stargazers_count}⭐️`, channel: \"C04GWUTDC3W\", }); }, });', metadata={'source': 'https://trigger.dev/docs/documentation/guides/using-integrations-oauth'}),\n",
       " Document(page_content='export const client = new TriggerClient({ id: \"nextjs-example\", //this environment variable should be set to your DEV API Key locally, //and your PROD API Key in production apiKey: process.env.TRIGGER_API_KEY!, });', metadata={'source': 'https://trigger.dev/docs/documentation/guides/deployment-setup'}),\n",
       " Document(page_content='Deployment\\nManual endpoint refreshing\\nConcepts\\nJobs, Runs & Tasks\\nTriggers\\nCommunity & Support\\nOverview\\nManual endpoint refreshing\\nYou can manually refresh Endpoints in your Trigger.dev dashboard', metadata={'source': 'https://trigger.dev/docs/documentation/guides/deployment-manual'}),\n",
       " Document(page_content='Writing Jobs - Step by Step', metadata={'source': 'https://trigger.dev/docs/documentation/roadmap'}),\n",
       " Document(page_content='//your job client.defineJob({ id: \"meme-generator\", name: \"Generate memes\", version: \"0.1.1\", trigger: eventTrigger({ name: \"generate-memes\", }), run: async (payload, io, ctx) => { //create a status \"generating-memes\" //you give it the starting state. Only label is required const generatingMemes = await io.createStatus(\"generating-memes\", { //the label is compulsory on this first call label: \"Generating memes\", //state is optional state: \"loading\", //data is an optional object. the values can be any type that is JSON serializable data: { progress: 0.1, }, }); //...do stuff, like generate memes //update the generatingMemes status. //anything set here will override the previous values, but you\\'ll be able to view the full history with hooks await generatingMemes.update(\"middle-generation\", { //label isn\\'t specified so will remain the same //state will be updated to \"success\" state: \"success\", //set data, this overrides the previous value data: { progress: 1, urls: [ \"https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExZnZoMndsdWh0MmhvY2kyaDF6YjZjZzg1ZGsxdnhhYm13a3Q1Y3lkbyZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/13HgwGsXF0aiGY/giphy.gif\", ], }, }); }, });', metadata={'source': 'https://trigger.dev/docs/documentation/guides/react-hooks-statuses'}),\n",
       " Document(page_content='Hono is a fast & lightweight web framework built on top of Web Standards, and we support using Hono with Trigger.dev on Cloudflare Workers, Bun, Deno, and Node.js.\\nInstalling Required Packages\\nTo begin, install the necessary packages in your Hono project:\\nObtain the Development Server API Key\\nTo locate your development Server API key, login to the Trigger.dev dashboard and select the Project you want to connect to. Then click on the Environments & API Keys tab in the left menu. You can copy your development Server API Key from the field at the top of this page. (Your development key will start with tr_dev_).\\nConfigure Environment Variables\\nAdd the following environment variables to your .env file (or .dev.vars file if you are using Cloudflare Workers):\\nTRIGGER_API_KEY=<development server api key> TRIGGER_API_URL=https://api.trigger.dev # change this if you are self-hosting \\nReplace <development server api key> with the actual API key obtained from the previous step.\\nEnable Node.js compatibility\\nIf you are using Cloudflare Workers, you’ll need to enable Node.js compatibility mode in your wrangler.toml file:\\ncompatibility_flags = [\"nodejs_compat\"] \\nAdd Middelware to Your Hono app\\nOur @trigger.dev/hono package provides two different ways of configuring the necessary middleware needed to connect your Hono app to Trigger.dev. addMiddleware which should be used for Cloudflare Workers, and createMiddleware which can be used with Bun, Deno, and Node.js.\\nCloudflare Workers\\nBecause environment variables in Cloudflare Workers aren’t available in the global scope, but are instead available only inside the fetch handler, we need to use the addMiddleware function to add the necessary middleware to your Hono app.\\nimport { Hono } from \"hono\"; import { addMiddleware } from \"@trigger.dev/hono\"; import { TriggerClient } from \"@trigger.dev/sdk\"; const app = new Hono<{ Bindings: { TRIGGER_API_KEY: string; TRIGGER_API_URL: string; }; }>(); addMiddleware(app, (env) => { const client = new TriggerClient({ id: \"hono-client\", apiKey: env.TRIGGER_API_KEY, apiUrl: env.TRIGGER_API_URL, }); return client; }); // Your other routes here export default app; \\nThe second argument to addMiddleware is a function that receives the environment variables (either from .dev.vars in development or from Cloudflare when deployed), and returns a TriggerClient instance. This function will be called once per request.\\nIf you want, you can extract our the function that creates the TriggerClient instance into a separate file, and import it into your index.ts file:\\nNow that you’ve created the TriggerClient and setup the middleware, you can add your first job:\\nAs you can see above in jobs.ts, we define our first job using the new Job constructor from @trigger.dev/sdk, and then in trigger-client.ts we attach the job to the TriggerClient instance.\\nBun\\nIf you are using Bun, you can use the createMiddleware function to create the necessary middleware to connect your Hono app to Trigger.dev and define your TriggerClient and jobs in the global scope:\\nimport { createMiddleware } from \"@trigger.dev/hono\"; import { TriggerClient, invokeTrigger } from \"@trigger.dev/sdk\"; import { Hono } from \"hono\"; const app = new Hono(); const client = new TriggerClient({ id: \"hono-client\", apiKey: Bun.env.TRIGGER_API_KEY, // Bun.env is available in the global scope apiUrl: Bun.env.TRIGGER_API_URL, // Bun.env is available in the global scope }); client.defineJob({ id: \"example-job\", name: \"Example Job\", version: \"0.0.1\", trigger: invokeTrigger(), run: async (payload, io, ctx) => { await io.logger.info(\"Hello world!\", { payload }); return { message: \"Hello world!\", }; }, }); app.use(\"/api/trigger\", createMiddleware(client)); // The rest of your routes here export default app; \\nDeno\\nImport trigger.dev packages with Deno using npm: specifiers:\\nimport { createMiddleware } from \"npm:@trigger.dev/hono@latest\"; import { TriggerClient, invokeTrigger } from \"npm:@trigger.dev/sdk@latest\"; import { Hono } from \"npm:hono\"; // Make sure to use the npm specifier for hono as well \\nTo load a .env file on startup, pass the deno run command a --env flag:\\ndeno run --env --allow-net --watch index.ts \\nYou can also load an environment variables file in code using the dotenv package:\\nimport { load } from \"https://deno.land/std@0.208.0/dotenv/mod.ts\"; const env = await load(); \\nIn which case you need to pass the --allow-env and --allow-read flags:\\ndeno run --allow-env --allow-net --allow-read --watch index.ts \\nNow we can create the TriggerClient, define our jobs, and create the middleware:\\nconst app = new Hono(); const client = new TriggerClient({ id: \"hono-client\", apiKey: env[\"TRIGGER_API_KEY\"], apiUrl: env[\"TRIGGER_API_URL\"], }); client.defineJob({ id: \"example-job\", name: \"Example Job\", version: \"0.0.1\", trigger: invokeTrigger(), run: async (payload, io, ctx) => { await io.logger.info(\"Hello world!\", { payload }); return { message: \"Hello world!\", }; }, }); app.use(\"/api/trigger\", createMiddleware(client)); // The rest of your routes here Deno.serve(app.fetch); \\nNode.js\\nNode.js works very similarly to Deno and Bun, in that you can define the TriggerClient and jobs in the global scope, and then create the middleware and add it to your Hono app:\\nimport \"dotenv/config\"; import { serve } from \"@hono/node-server\"; import { Hono } from \"hono\"; import { createMiddleware } from \"@trigger.dev/hono\"; import { TriggerClient, invokeTrigger } from \"@trigger.dev/sdk\"; const app = new Hono(); const client = new TriggerClient({ id: \"hono-client\", apiKey: process.env.TRIGGER_API_KEY!, apiUrl: process.env.TRIGGER_API_URL!, }); client.defineJob({ id: \"example-job\", name: \"Example Job\", version: \"0.0.1\", trigger: invokeTrigger(), run: async (payload, io, ctx) => { await io.logger.info(\"Hello world!\", { payload }); return { message: \"Hello world!\", }; }, }); app.use(\"/api/trigger\", createMiddleware(client)); // Your other routes here serve(app, (info) => { console.log(`Listening on port ${info.port}`); }); \\nRunning\\nCloudflare Workers\\nRun your Hono app locally, like you normally would. For example:\\nIn a separate terminal window or tab run:\\nBun\\nRun your Hono app locally, like you normally would. For example:\\nIn a separate terminal window or tab run:\\nDeno\\nRun your Hono app locally, like you normally would. For example:\\ndeno run --env --allow-net --watch index.ts \\nIn a separate terminal window or tab run:\\nNode.js\\nRun your Hono app locally, like you normally would. For example:\\nIn a separate terminal window or tab run:', metadata={'source': 'https://trigger.dev/docs/documentation/quickstarts/hono'}),\n",
       " Document(page_content='The data you can receive\\nInfo about an event you sent, including the Runs it triggered.\\nThe overall status of the Run (in progress, success and fail statuses).\\nMetadata like start and completed times.\\nThe Run output (what is returned or an error that failed the Job)\\nInformation about the Tasks that have completed/failed/are running.\\nThe hooks\\nuseEventDetails: get the details of a specific event\\nuseRunDetails: get the details of a specific Run\\nuseEventRunDetails: get the details of a Run triggered from a specific event\\nAll of these hooks will automatically refresh your components as the state of your Runs or events change.\\nuseEventDetails\\nThe useEventDetails hook will get the details of a specific event. You can use this to show the status of a specific event.\\nThis component will show the details of an event and the overall status of Runs that were triggered by the event:\\nimport { useEventDetails } from \"@trigger.dev/react\"; export default function EventDetails({ eventId }: { eventId: string }) { const { data, error } = useEventDetails(eventId); if (error) { return <div>Error: {error.message}</div>; } if (!data) { return <div>Loading...</div>; } return ( <div> <h1>{data.name}</h1> <p>Runs: {data.runs?.length}</p> <div> {data.runs?.map((run) => ( <div key={run.id}> <p> Run {run.id}: {run.status} </p> </div> ))} </div> </div> ); } \\nuseRunDetails\\nThe useRunDetails hook will get the details of a specific Run. You can use this to show the status of a specific Run.\\nThis component will show the details of a Run and the status of each task in the Run:\\nimport { useRunDetails } from \"@trigger.dev/react\"; export default function RunDetails({ runId }: { runId: string }) { const { data, error } = useRunDetails(runId); if (error) { return <div>Error: {error.message}</div>; } if (!data) { return <div>Loading...</div>; } return ( <div> <h1>Run {data.id}</h1> <p>Status: {data.status}</p> <div> {data.tasks?.map((task) => ( <div key={task.id}> <p> Task {task.id}: {task.status} </p> </div> ))} </div> </div> ); } \\nuseEventRunDetails\\nThe useEventRunDetails hook will get the details of a specific Run that was triggered by a specific event. You can use this to show the status of a specific Run.\\nThis component will show the details of a Run and the status of each task in the Run:\\nimport { useEventRunDetails } from \"@trigger.dev/react\"; export default function EventRunDetails({ eventId }: { eventId: string }) { const { data, error } = useEventRunDetails(eventId); if (error) { return <div>Error: {error.message}</div>; } if (!data) { return <div>Loading...</div>; } return ( <div> <h1>Run {data.id}</h1> <p>Status: {data.status}</p> <div> {data.tasks?.map((task) => ( <div key={task.id}> <p> Task {task.id}: {task.status} </p> </div> ))} </div> </div> ); } \\nThe data you can receive\\nThe hooks\\nuseEventDetails\\nuseRunDetails\\nuseEventRunDetails', metadata={'source': 'https://trigger.dev/docs/documentation/guides/react-hooks-automatic'})]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documentation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model_kwargs={\"llm\": \"gpt-4-0125-preview\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAI(client=<openai.resources.completions.Completions object at 0x160c1de80>, async_client=<openai.resources.completions.AsyncCompletions object at 0x160db4d30>, model_kwargs={'llm': 'gpt-4-0125-preview'}, openai_api_key='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', openai_proxy='')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorstoreIndexCreator().from_loaders([loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "2 validation errors for LLMChain\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShow me a code example of creating a job in trigger.dev that triggers using a Supabase webhook?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4-0125-preview\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m (result)\n",
      "File \u001b[0;32m~/rohil/OnFinance/venv/lib/python3.9/site-packages/langchain/indexes/vectorstore.py:43\u001b[0m, in \u001b[0;36mVectorStoreIndexWrapper.query\u001b[0;34m(self, question, llm, retriever_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m llm \u001b[38;5;241m=\u001b[39m llm \u001b[38;5;129;01mor\u001b[39;00m OpenAI(temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     42\u001b[0m retriever_kwargs \u001b[38;5;241m=\u001b[39m retriever_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m---> 43\u001b[0m chain \u001b[38;5;241m=\u001b[39m \u001b[43mRetrievalQA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_chain_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretriever\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectorstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_retriever\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mretriever_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chain\u001b[38;5;241m.\u001b[39mrun(question)\n",
      "File \u001b[0;32m~/rohil/OnFinance/venv/lib/python3.9/site-packages/langchain/chains/retrieval_qa/base.py:105\u001b[0m, in \u001b[0;36mBaseRetrievalQA.from_chain_type\u001b[0;34m(cls, llm, chain_type, chain_type_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load chain from chain type.\"\"\"\u001b[39;00m\n\u001b[1;32m    104\u001b[0m _chain_type_kwargs \u001b[38;5;241m=\u001b[39m chain_type_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m--> 105\u001b[0m combine_documents_chain \u001b[38;5;241m=\u001b[39m \u001b[43mload_qa_chain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchain_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchain_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_chain_type_kwargs\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(combine_documents_chain\u001b[38;5;241m=\u001b[39mcombine_documents_chain, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/rohil/OnFinance/venv/lib/python3.9/site-packages/langchain/chains/question_answering/__init__.py:249\u001b[0m, in \u001b[0;36mload_qa_chain\u001b[0;34m(llm, chain_type, verbose, callback_manager, **kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chain_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m loader_mapping:\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    246\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot unsupported chain type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchain_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    247\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloader_mapping\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    248\u001b[0m     )\n\u001b[0;32m--> 249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloader_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mchain_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/rohil/OnFinance/venv/lib/python3.9/site-packages/langchain/chains/question_answering/__init__.py:73\u001b[0m, in \u001b[0;36m_load_stuff_chain\u001b[0;34m(llm, prompt, document_variable_name, verbose, callback_manager, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_stuff_chain\u001b[39m(\n\u001b[1;32m     64\u001b[0m     llm: BaseLanguageModel,\n\u001b[1;32m     65\u001b[0m     prompt: Optional[BasePromptTemplate] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m     71\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m StuffDocumentsChain:\n\u001b[1;32m     72\u001b[0m     _prompt \u001b[38;5;241m=\u001b[39m prompt \u001b[38;5;129;01mor\u001b[39;00m stuff_prompt\u001b[38;5;241m.\u001b[39mPROMPT_SELECTOR\u001b[38;5;241m.\u001b[39mget_prompt(llm)\n\u001b[0;32m---> 73\u001b[0m     llm_chain \u001b[38;5;241m=\u001b[39m \u001b[43mLLMChain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m# TODO: document prompt\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m StuffDocumentsChain(\n\u001b[1;32m     82\u001b[0m         llm_chain\u001b[38;5;241m=\u001b[39mllm_chain,\n\u001b[1;32m     83\u001b[0m         document_variable_name\u001b[38;5;241m=\u001b[39mdocument_variable_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     88\u001b[0m     )\n",
      "File \u001b[0;32m~/rohil/OnFinance/venv/lib/python3.9/site-packages/langchain_core/load/serializable.py:120\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/rohil/OnFinance/venv/lib/python3.9/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 2 validation errors for LLMChain\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)"
     ]
    }
   ],
   "source": [
    "query = \"Show me a code example of creating a job in trigger.dev that triggers using a Supabase webhook?\"\n",
    "\n",
    "result = index.query(question=query)\n",
    "\n",
    "print (result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
